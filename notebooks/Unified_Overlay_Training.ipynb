{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Trading Pipeline - Unified Overlay Implementation\n",
    "\n",
    "## Overview\n",
    "This notebook implements the unified overlay architecture for MetaStackerBandit with **one-to-one parity** to the original Bandit New notebook. Instead of training separate models for 5m, 1h, and 12h timeframes, we train a single model on 5-minute data that can generate signals for all timeframes using rollup overlays.\n",
    "\n",
    "## Key Changes from Original:\n",
    "1. **Single Model Training**: One model trained on 5m data for all timeframes\n",
    "2. **Overlay Feature Engineering**: Rollup features for 15m and 1h from 5m base\n",
    "3. **Unified Feature Schema**: Same features across all timeframes\n",
    "4. **Multi-Timeframe Signals**: Generate signals for 5m, 15m, 1h simultaneously\n",
    "5. **Enhanced Bandit**: Multi-level bandit for timeframe and signal selection\n",
    "\n",
    "## Structure (Matching Original):\n",
    "1. Import Required Libraries\n",
    "2. Configuration & Parameters\n",
    "3. Data Loading and Processing\n",
    "4. Feature Engineering (Unified)\n",
    "5. Trading Signal Generation (Multi-Arm Bandit)\n",
    "6. Backtesting Engine\n",
    "7. Grid Search Optimization\n",
    "8. Results Analysis & Visualization\n",
    "9. Export & Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Imports (Matching Original Bandit New)\n",
    "import pandas as pd, numpy as np, warnings, os, json, joblib\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV, ElasticNetCV, HuberRegressor, LogisticRegression\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score, classification_report, confusion_matrix, log_loss\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.base import clone\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deterministic seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Imports loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Signal thresholds: S_MIN=0.12, M_MIN=0.12, CONF_MIN=0.6, ALPHA_MIN=0.1\n",
      "  Risk controls: SIGMA_TARGET=0.2, POS_MAX=1.0, DD_STOP=0.05\n",
      "  Overlay timeframes: ['5m', '15m', '1h']\n",
      "  Rollup windows: {'15m': 3, '1h': 12}\n",
      "  Timeframe weights: {'5m': 0.5, '15m': 0.3, '1h': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# Single-source threshold & cost config for allocator-only mode (Matching Original)\n",
    "# These globals are consumed by consolidated arms and backtest cells.\n",
    "try:\n",
    "    S_MIN\n",
    "except NameError:\n",
    "    S_MIN = 0.12\n",
    "try:\n",
    "    M_MIN\n",
    "except NameError:\n",
    "    M_MIN = 0.12\n",
    "try:\n",
    "    CONF_MIN\n",
    "except NameError:\n",
    "    CONF_MIN = 0.60\n",
    "try:\n",
    "    ALPHA_MIN\n",
    "except NameError:\n",
    "    ALPHA_MIN = 0.10\n",
    "try:\n",
    "    COOLDOWN\n",
    "except NameError:\n",
    "    COOLDOWN = 1\n",
    "try:\n",
    "    COST_BP\n",
    "except NameError:\n",
    "    COST_BP = 5.0\n",
    "try:\n",
    "    IMPACT_K\n",
    "except NameError:\n",
    "    IMPACT_K = 0.0\n",
    "\n",
    "# Risk and execution controls (Matching Original)\n",
    "try:\n",
    "    SIGMA_TARGET\n",
    "except NameError:\n",
    "    SIGMA_TARGET = 0.20  # per-bar target scaler proxy\n",
    "try:\n",
    "    POS_MAX\n",
    "except NameError:\n",
    "    POS_MAX = 1.0\n",
    "try:\n",
    "    DD_STOP\n",
    "except NameError:\n",
    "    DD_STOP = 0.05\n",
    "try:\n",
    "    LATENCY_BARS\n",
    "except NameError:\n",
    "    LATENCY_BARS = 0\n",
    "try:\n",
    "    SLIPPAGE_BPS\n",
    "except NameError:\n",
    "    SLIPPAGE_BPS = 0.0\n",
    "try:\n",
    "    COST_CONVENTION\n",
    "except NameError:\n",
    "    COST_CONVENTION = 'per_transition'  # or 'per_roundtrip'\n",
    "try:\n",
    "    SMOOTH_BETA\n",
    "except NameError:\n",
    "    SMOOTH_BETA = 0.0\n",
    "\n",
    "# Overlay-specific parameters (NEW)\n",
    "OVERLAY_TIMEFRAMES = [\"5m\", \"15m\", \"1h\"]\n",
    "ROLLUP_WINDOWS = {\"15m\": 3, \"1h\": 12}  # 3x5m=15m, 12x5m=1h\n",
    "TIMEFRAME_WEIGHTS = {\"5m\": 0.5, \"15m\": 0.3, \"1h\": 0.2}\n",
    "\n",
    "# Feature engineering parameters (unified across timeframes)\n",
    "MOMENTUM_PERIODS = [1, 3, 6]  # Consistent momentum periods\n",
    "EMA_PERIOD = 20  # Consistent EMA period\n",
    "ROLLING_WINDOW = 100  # Consistent rolling window\n",
    "RV_WINDOWS = {\"1h\": 12, \"15m\": 3, \"1d\": 288}  # RV windows\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Signal thresholds: S_MIN={S_MIN}, M_MIN={M_MIN}, CONF_MIN={CONF_MIN}, ALPHA_MIN={ALPHA_MIN}\")\n",
    "print(f\"  Risk controls: SIGMA_TARGET={SIGMA_TARGET}, POS_MAX={POS_MAX}, DD_STOP={DD_STOP}\")\n",
    "print(f\"  Overlay timeframes: {OVERLAY_TIMEFRAMES}\")\n",
    "print(f\"  Rollup windows: {ROLLUP_WINDOWS}\")\n",
    "print(f\"  Timeframe weights: {TIMEFRAME_WEIGHTS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 51840 5m bars\n",
      "Loaded 4320 funding records\n",
      "Cohort addresses: 174\n",
      "Loaded 1038195 fills\n",
      "Data loading complete!\n"
     ]
    }
   ],
   "source": [
    "# Load data (Matching Original Structure)\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Load OHLCV data\n",
    "df = pd.read_csv('ohlc_btc_5m.csv') if os.path.exists('ohlc_btc_5m.csv') else pd.DataFrame()\n",
    "if not df.empty:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(f\"Loaded {len(df)} 5m bars\")\n",
    "else:\n",
    "    print(\"Error: No OHLCV data found\")\n",
    "\n",
    "# Load funding data\n",
    "funding_df = pd.read_csv('funding_btc.csv') if os.path.exists('funding_btc.csv') else pd.DataFrame()\n",
    "if not funding_df.empty:\n",
    "    funding_df['timestamp'] = pd.to_datetime(funding_df['timestamp'])\n",
    "    funding_df = funding_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(f\"Loaded {len(funding_df)} funding records\")\n",
    "\n",
    "# Load cohort data\n",
    "cohort_top = pd.read_csv('top_cohort.csv') if os.path.exists('top_cohort.csv') else pd.DataFrame()\n",
    "cohort_bot = pd.read_csv('bottom_cohort.csv') if os.path.exists('bottom_cohort.csv') else pd.DataFrame()\n",
    "\n",
    "# Process cohort addresses (Matching Original)\n",
    "for cdf in (cohort_top, cohort_bot):\n",
    "    if not cdf.empty and 'user' not in cdf.columns:\n",
    "        col = next((c for c in ['Account', 'address', 'user', 'addr'] if c in cdf.columns), None)\n",
    "        if col: cdf['user'] = cdf[col].astype(str)\n",
    "\n",
    "cohort_top_users = set(cohort_top.get('user', pd.Series(dtype=str)).dropna().astype(str))\n",
    "cohort_bot_users = set(cohort_bot.get('user', pd.Series(dtype=str)).dropna().astype(str))\n",
    "cohort_addresses = cohort_top_users | cohort_bot_users\n",
    "\n",
    "print(f\"Cohort addresses: {len(cohort_addresses)}\")\n",
    "\n",
    "# Load fills data (Matching Original Processing)\n",
    "fills = pd.read_csv('historical_trades_btc.csv', low_memory=False) if os.path.exists('historical_trades_btc.csv') else pd.DataFrame()\n",
    "if not fills.empty:\n",
    "    if 'user' not in fills.columns:\n",
    "        if 'Account' in fills.columns: fills['user'] = fills['Account'].astype(str)\n",
    "    \n",
    "    # Parse timestamps (Matching Original)\n",
    "    ts_col = 'Timestamp' if 'Timestamp' in fills.columns else (next((c for c in ['timestamp', 'ts'] if c in fills.columns), None))\n",
    "    ts_raw = pd.to_numeric(fills[ts_col], errors='coerce') if ts_col else pd.Series(dtype='float64')\n",
    "    if ts_raw.notna().any():\n",
    "        med = ts_raw.dropna().median()\n",
    "        unit = 'ns' if med>1e14 else ('ms' if med>1e12 else 's')\n",
    "        fills['timestamp'] = pd.to_datetime(ts_raw, unit=unit, errors='coerce')\n",
    "    else:\n",
    "        time_col = next((c for c in ['Timestamp IST', 'time'] if c in fills.columns), None)\n",
    "        fills['timestamp'] = pd.to_datetime(fills[time_col], errors='coerce') if time_col else pd.NaT\n",
    "    \n",
    "    # Map helpers (Matching Original)\n",
    "    if 'Execution Price' in fills.columns and 'px' not in fills.columns:\n",
    "        fills['px'] = pd.to_numeric(fills['Execution Price'], errors='coerce')\n",
    "    if 'Size Tokens' in fills.columns and 'sz' not in fills.columns:\n",
    "        fills['sz'] = pd.to_numeric(fills['Size Tokens'], errors='coerce')\n",
    "    if 'Size USD' in fills.columns and 'notional' not in fills.columns:\n",
    "        fills['notional'] = pd.to_numeric(fills['Size USD'], errors='coerce')\n",
    "    if 'Side' in fills.columns and 'side' not in fills.columns:\n",
    "        fills['side'] = fills['Side'].map({'BUY':'B','SELL':'A'}).fillna(fills['Side'].astype(str))\n",
    "    \n",
    "    print(f\"Loaded {len(fills)} fills\")\n",
    "\n",
    "print(\"Data loading complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating unified features...\n",
      "Created unified features: (51840, 37)\n",
      "Feature columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'price', 'returns', 'hl_range', 'oc_range', 'typical_price', 'true_range', 'mom_1', 'mom_3', 'mom_6', 'ema20', 'mr_ema20', 'mr_ema20_z', 'rv_1h', 'rv_15m', 'rv_1d', 'regime_high_vol', 'gk_volatility', 'parkinson_volatility', 'vol_regime_low', 'vol_regime_high', 'jump_indicator', 'jump_magnitude', 'volume_intensity', 'price_volume_corr', 'price_efficiency', 'vwap', 'vwap_momentum', 'depth_proxy', 'funding_rate', 'funding_momentum_1h', 'flow_diff']\n"
     ]
    }
   ],
   "source": [
    "# Unified Feature Engineering (Matching Original + Overlay)\n",
    "print(\"Creating unified features...\")\n",
    "\n",
    "# Create base features from 5m data\n",
    "features_df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].copy()\n",
    "\n",
    "# Price and returns (Matching Original)\n",
    "features_df['price'] = features_df['close']\n",
    "features_df['returns'] = features_df['price'].pct_change()\n",
    "\n",
    "# Technical indicators (unified parameters)\n",
    "features_df['hl_range'] = features_df['high'] - features_df['low']\n",
    "features_df['oc_range'] = np.abs(features_df['open'] - features_df['close'])\n",
    "features_df['typical_price'] = (features_df['high'] + features_df['low'] + features_df['close']) / 3\n",
    "features_df['true_range'] = np.maximum(\n",
    "    features_df['high'] - features_df['low'],\n",
    "    np.maximum(\n",
    "        np.abs(features_df['high'] - features_df['close'].shift(1)),\n",
    "        np.abs(features_df['low'] - features_df['close'].shift(1))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Momentum features (consistent periods)\n",
    "for h in MOMENTUM_PERIODS:\n",
    "    features_df[f'mom_{h}'] = features_df['price'].pct_change(periods=h)\n",
    "\n",
    "# EMA and mean reversion (unified parameters)\n",
    "features_df['ema20'] = features_df['price'].ewm(span=EMA_PERIOD, adjust=False).mean()\n",
    "features_df['mr_ema20'] = (features_df['price'] - features_df['ema20']) / features_df['ema20']\n",
    "\n",
    "# Z-score normalization (unified rolling window)\n",
    "rolling_mean = features_df['price'].rolling(window=ROLLING_WINDOW, min_periods=20).mean()\n",
    "rolling_std = features_df['price'].rolling(window=ROLLING_WINDOW, min_periods=20).std()\n",
    "features_df['mr_ema20_z'] = (features_df['price'] - rolling_mean) / (rolling_std + 1e-8)\n",
    "\n",
    "# Volatility features (unified RV windows)\n",
    "features_df['rv_1h'] = features_df['returns'].rolling(window=RV_WINDOWS['1h'], min_periods=6).apply(lambda x: (x**2).sum())\n",
    "features_df['rv_15m'] = features_df['returns'].rolling(window=RV_WINDOWS['15m'], min_periods=2).apply(lambda x: (x**2).sum())\n",
    "features_df['rv_1d'] = features_df['returns'].rolling(window=RV_WINDOWS['1d'], min_periods=50).apply(lambda x: (x**2).sum())\n",
    "\n",
    "# Volatility regime\n",
    "rv_threshold = features_df['rv_1h'].rolling(window=100, min_periods=25).quantile(0.75)\n",
    "features_df['regime_high_vol'] = (features_df['rv_1h'] > rv_threshold).astype(int)\n",
    "\n",
    "# Enhanced volatility features (Matching Original)\n",
    "if {'high','low','close','open'}.issubset(features_df.columns):\n",
    "    gk_vol = np.log(features_df['high']/features_df['low'])**2 / 2 - (2*np.log(2)-1) * np.log(features_df['close']/features_df['open'])**2\n",
    "    features_df['gk_volatility'] = gk_vol.rolling(12, min_periods=6).mean()\n",
    "    \n",
    "    parkinson_vol = np.log(features_df['high']/features_df['low'])**2 / (4 * np.log(2))\n",
    "    features_df['parkinson_volatility'] = parkinson_vol.rolling(12, min_periods=6).mean()\n",
    "    \n",
    "    vol_percentile_20 = features_df['gk_volatility'].rolling(100, min_periods=50).quantile(0.2)\n",
    "    vol_percentile_80 = features_df['gk_volatility'].rolling(100, min_periods=50).quantile(0.8)\n",
    "    features_df['vol_regime_low'] = (features_df['gk_volatility'] <= vol_percentile_20).astype('float')\n",
    "    features_df['vol_regime_high'] = (features_df['gk_volatility'] >= vol_percentile_80).astype('float')\n",
    "\n",
    "# Jump detection (Matching Original)\n",
    "return_std = features_df['returns'].rolling(50, min_periods=25).std()\n",
    "features_df['jump_indicator'] = (np.abs(features_df['returns']) > 3 * return_std).astype('float')\n",
    "features_df['jump_magnitude'] = np.abs(features_df['returns']) / (return_std + 1e-8)\n",
    "\n",
    "# Volume features (Matching Original)\n",
    "features_df['volume_intensity'] = features_df['volume'] / features_df['volume'].rolling(50, min_periods=25).mean()\n",
    "features_df['price_volume_corr'] = features_df['returns'].rolling(20, min_periods=10).corr(features_df['volume'].pct_change())\n",
    "\n",
    "# Price efficiency (Matching Original)\n",
    "features_df['price_efficiency'] = np.abs(features_df['close'] - features_df['open']) / (features_df['high'] - features_df['low'] + 1e-8)\n",
    "\n",
    "# VWAP momentum (Matching Original)\n",
    "features_df['vwap'] = (features_df['high'] + features_df['low'] + features_df['close']) / 3\n",
    "features_df['vwap_momentum'] = features_df['vwap'].pct_change(periods=5)\n",
    "\n",
    "# Depth proxy (simplified)\n",
    "features_df['depth_proxy'] = features_df['volume'] / (features_df['hl_range'] + 1e-8)\n",
    "\n",
    "# Funding features (if available)\n",
    "if funding_df is not None and not funding_df.empty:\n",
    "    funding_df['timestamp'] = pd.to_datetime(funding_df['timestamp'])\n",
    "    features_df = features_df.merge(funding_df[['timestamp', 'funding_rate']], on='timestamp', how='left')\n",
    "    features_df['funding_rate'] = features_df['funding_rate'].fillna(0.0)\n",
    "    features_df['funding_momentum_1h'] = features_df['funding_rate'].rolling(12, min_periods=6).mean()\n",
    "else:\n",
    "    features_df['funding_rate'] = 0.0\n",
    "    features_df['funding_momentum_1h'] = 0.0\n",
    "\n",
    "# Flow features (placeholder - would need actual flow data)\n",
    "features_df['flow_diff'] = 0.0\n",
    "\n",
    "print(f\"Created unified features: {features_df.shape}\")\n",
    "print(f\"Feature columns: {list(features_df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating overlay features...\n",
      "Overlay features created:\n",
      "  5m: 51840 bars\n",
      "  15m: 17280 bars\n",
      "  1h: 4320 bars\n"
     ]
    }
   ],
   "source": [
    "# Create Overlay Features (NEW - Overlay Architecture)\n",
    "print(\"Creating overlay features...\")\n",
    "\n",
    "def create_overlay_features(base_features_df, timeframe, rollup_window):\n",
    "    \"\"\"Create overlay features by rolling up base 5m features\"\"\"\n",
    "    if timeframe == \"5m\":\n",
    "        return base_features_df.copy()\n",
    "    \n",
    "    # Create rollup bars\n",
    "    rollup_df = base_features_df.copy()\n",
    "    rollup_df['rollup_group'] = rollup_df.index // rollup_window\n",
    "    \n",
    "    # Get all available columns for aggregation\n",
    "    agg_dict = {\n",
    "        'timestamp': 'last',\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum',\n",
    "        'price': 'last',\n",
    "        'returns': lambda x: (x.iloc[-1] / x.iloc[0] - 1) if len(x) > 1 else 0,\n",
    "        'mom_1': 'last',\n",
    "        'mom_3': 'last', \n",
    "        'mom_6': 'last',\n",
    "        'mr_ema20_z': 'last',\n",
    "        'rv_1h': 'mean',\n",
    "        'regime_high_vol': 'mean',\n",
    "        'gk_volatility': 'mean',\n",
    "        'jump_magnitude': 'mean',\n",
    "        'volume_intensity': 'mean',\n",
    "        'price_efficiency': 'mean',\n",
    "        'vwap_momentum': 'last',\n",
    "        'depth_proxy': 'mean',\n",
    "        'funding_rate': 'mean',\n",
    "        'funding_momentum_1h': 'mean',\n",
    "        'flow_diff': 'sum'\n",
    "    }\n",
    "    \n",
    "    # Add price_volume_corr if it exists\n",
    "    if 'price_volume_corr' in rollup_df.columns:\n",
    "        agg_dict['price_volume_corr'] = 'mean'\n",
    "    \n",
    "    overlay_features = rollup_df.groupby('rollup_group').agg(agg_dict).reset_index(drop=True)\n",
    "    \n",
    "    # Recalculate some features for the new timeframe\n",
    "    overlay_features['price'] = overlay_features['close']\n",
    "    overlay_features['hl_range'] = overlay_features['high'] - overlay_features['low']\n",
    "    overlay_features['typical_price'] = (overlay_features['high'] + overlay_features['low'] + overlay_features['close']) / 3\n",
    "    \n",
    "    return overlay_features\n",
    "\n",
    "# Create overlay features for all timeframes\n",
    "overlay_features = {}\n",
    "overlay_features['5m'] = create_overlay_features(features_df, '5m', 1)\n",
    "overlay_features['15m'] = create_overlay_features(features_df, '15m', ROLLUP_WINDOWS['15m'])\n",
    "overlay_features['1h'] = create_overlay_features(features_df, '1h', ROLLUP_WINDOWS['1h'])\n",
    "\n",
    "print(f\"Overlay features created:\")\n",
    "for tf, features in overlay_features.items():\n",
    "    print(f\"  {tf}: {features.shape[0]} bars\")\n",
    "\n",
    "# Store base features for backtesting\n",
    "bt = features_df.copy()  # Matching original variable name\n",
    "bt['returns'] = bt['price'].pct_change()  # Ensure returns column exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit class defined\n"
     ]
    }
   ],
   "source": [
    "# Bandit allocator: class and backtest function (Matching Original)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class SimpleThompsonBandit:\n",
    "    def __init__(self, n_arms: int):\n",
    "        self.counts = np.zeros(n_arms, dtype=float)\n",
    "        self.means = np.zeros(n_arms, dtype=float)\n",
    "        self.vars = np.ones(n_arms, dtype=float)\n",
    "        \n",
    "    def select(self, eligible: np.ndarray) -> int:\n",
    "        \"\"\"Select arm using Thompson Sampling\"\"\"\n",
    "        if not np.any(eligible):\n",
    "            return 0\n",
    "        \n",
    "        # Sample from posterior\n",
    "        samples = np.random.normal(self.means, np.sqrt(self.vars))\n",
    "        \n",
    "        # Only consider eligible arms\n",
    "        samples[~eligible] = -np.inf\n",
    "        \n",
    "        return int(np.argmax(samples))\n",
    "    \n",
    "    def update(self, arm: int, reward: float):\n",
    "        \"\"\"Update arm statistics with new reward\"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        \n",
    "        # Update mean\n",
    "        self.means[arm] = (self.means[arm] * (n - 1) + reward) / n\n",
    "        \n",
    "        # Update variance (simplified)\n",
    "        if n > 1:\n",
    "            self.vars[arm] = 1.0 / n\n",
    "\n",
    "print(\"Bandit class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtesting engine defined\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Backtesting Engine (Matching Original)\n",
    "def run_allocator_backtest(\n",
    "    bt_df: pd.DataFrame,\n",
    "    arm_signals: np.ndarray,\n",
    "    arm_eligible: np.ndarray,\n",
    "    adv_series: pd.Series,\n",
    "    cooldown_bars: int,\n",
    "    cost_bps: float,\n",
    "    impact_k: float,\n",
    "    side_eps_vec: np.ndarray,\n",
    "    eps: float = 1e-12,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, Dict]:\n",
    "    \"\"\"Comprehensive backtesting with bandit allocation (Matching Original)\"\"\"\n",
    "    \n",
    "    n = len(bt_df)\n",
    "    rets = bt_df['returns'].values if 'returns' in bt_df.columns else np.zeros(n)\n",
    "    \n",
    "    # ADV handling (Matching Original)\n",
    "    adv_valid = isinstance(adv_series, pd.Series) and not adv_series.dropna().empty\n",
    "    adv_arr = adv_series.reindex(bt_df.index).to_numpy() if adv_valid else np.ones(n, dtype=float)\n",
    "    impact_k_eff = impact_k if adv_valid else 0.0\n",
    "    \n",
    "    bandit = SimpleThompsonBandit(n_arms=arm_signals.shape[1])\n",
    "    \n",
    "    pos = 0.0\n",
    "    pos_smooth = 0.0\n",
    "    last_flip_idx = -10**9\n",
    "    \n",
    "    exec_pos_buffer = deque(maxlen=cooldown_bars + 1)\n",
    "    exec_pos_buffer.append(0.0)\n",
    "    \n",
    "    records_eq = []\n",
    "    records_tr = []\n",
    "    records_bu = []\n",
    "    \n",
    "    cum_equity = 1.0\n",
    "    equity_series = np.ones(n, dtype=float)\n",
    "    \n",
    "    sigma_target = float(globals().get('SIGMA_TARGET', 0.20))\n",
    "    pos_max = float(globals().get('POS_MAX', 1.0))\n",
    "    dd_stop = float(globals().get('DD_STOP', 0.05))\n",
    "    latency_k = int(globals().get('LATENCY_BARS', 0))\n",
    "    \n",
    "    for t in range(n):\n",
    "        exec_pos = exec_pos_buffer[0] if latency_k > 0 else pos_smooth\n",
    "        \n",
    "        # Check eligibility and select arm (Matching Original)\n",
    "        elig = arm_eligible[t] if t < len(arm_eligible) else np.array([False]*arm_signals.shape[1])\n",
    "        desired_side = pos\n",
    "        chosen = None\n",
    "        \n",
    "        if np.any(elig):\n",
    "            chosen = bandit.select(elig)\n",
    "            raw_val = float(arm_signals[t, chosen])\n",
    "            th = float(side_eps_vec[chosen]) if (side_eps_vec is not None) else 0.0\n",
    "            \n",
    "            if abs(raw_val) < th:\n",
    "                desired_side = 0.0\n",
    "            else:\n",
    "                desired_side = np.sign(raw_val) * pos_max\n",
    "        \n",
    "        # Position sizing with volatility targeting (Matching Original)\n",
    "        if t > 0 and abs(rets[t]) > eps:\n",
    "            vol_est = np.std(rets[max(0, t-20):t+1])\n",
    "            if vol_est > eps:\n",
    "                vol_scaler = sigma_target / (vol_est * np.sqrt(252 * 24 * 12))  # Annualized\n",
    "                desired_side *= min(vol_scaler, 2.0)  # Cap at 2x\n",
    "        \n",
    "        # Cooldown logic (Matching Original)\n",
    "        if abs(desired_side - exec_pos) > eps and (t - last_flip_idx) >= cooldown_bars:\n",
    "            exec_pos_buffer.append(desired_side)\n",
    "            last_exec_pos = exec_pos\n",
    "            \n",
    "            # Cost calculation (Matching Original)\n",
    "            cost_bps_eff = cost_bps\n",
    "            if impact_k_eff > 0 and adv_valid:\n",
    "                notional_change = abs(desired_side - exec_pos)\n",
    "                impact_bps = impact_k_eff * notional_change / (adv_arr[t] + eps)\n",
    "                cost_bps_eff += impact_bps\n",
    "            \n",
    "            records_tr.append((bt_df.index[t], exec_pos, desired_side, cost_bps_eff))\n",
    "            last_exec_pos = exec_pos\n",
    "            last_flip_idx = t\n",
    "        \n",
    "        pnl = rets[t] * (exec_pos_buffer[0] if latency_k > 0 else exec_pos)\n",
    "        pnl -= (cost_bps / 10000.0) if cost_bps > 0 else 0.0\n",
    "        cum_equity *= (1.0 + pnl)\n",
    "        equity_series[t] = cum_equity\n",
    "        records_eq.append((bt_df.index[t], cum_equity))\n",
    "        \n",
    "        if chosen is not None:\n",
    "            bandit.update(chosen, pnl)\n",
    "            records_bu.append((bt_df.index[t], int(chosen), float(pnl)))\n",
    "        \n",
    "        # Drawdown stop (Matching Original)\n",
    "        if dd_stop > 0:\n",
    "            peak = np.max(equity_series[:t+1])\n",
    "            if peak > 0 and (cum_equity / peak - 1.0) < -dd_stop:\n",
    "                exec_pos_buffer.append(0.0)\n",
    "                last_exec_pos = 0.0\n",
    "        \n",
    "        pos_smooth = exec_pos_buffer[0] if exec_pos_buffer else 0.0\n",
    "    \n",
    "    Eq = pd.DataFrame.from_records(records_eq, columns=['ts', 'equity']).set_index('ts')\n",
    "    Tr = pd.DataFrame.from_records(records_tr, columns=['ts', 'from_pos', 'to_pos', 'cost_bps'])\n",
    "    Bu = pd.DataFrame.from_records(records_bu, columns=['ts', 'chosen', 'reward'])\n",
    "    \n",
    "    # Calculate performance metrics (Matching Original)\n",
    "    eq_rets = Eq['equity'].pct_change().dropna()\n",
    "    annualizer = float(globals().get('ANNUALIZER', np.sqrt(365*24*12)))\n",
    "    \n",
    "    sharpe = float(annualizer * eq_rets.mean() / (eq_rets.std() if eq_rets.std() != 0 else np.nan)) if len(eq_rets) else np.nan\n",
    "    \n",
    "    # Sortino ratio (Matching Original)\n",
    "    downside_rets = eq_rets[eq_rets < 0]\n",
    "    sortino = float(annualizer * eq_rets.mean() / (downside_rets.std() if len(downside_rets) > 0 else np.nan)) if len(eq_rets) else np.nan\n",
    "    \n",
    "    maxdd = float(-(Eq['equity'] / Eq['equity'].cummax() - 1.0).min()) if not Eq.empty else np.nan\n",
    "    \n",
    "    # Turnover (Matching Original)\n",
    "    turnover = float(np.abs(Tr['to_pos'].astype(float) - Tr['from_pos'].astype(float)).sum()) if not Tr.empty else 0.0\n",
    "    \n",
    "    # Hit rate (Matching Original)\n",
    "    hit_rate = float((Tr['pnl_$'] > 0).sum() / max(len(Tr), 1)) if not Tr.empty and 'pnl_$' in Tr.columns else np.nan\n",
    "    \n",
    "    metrics = {\n",
    "        'final_equity': float(Eq['equity'].iloc[-1]) if len(Eq) else 1.0,\n",
    "        'n_trades': int(len(Tr)),\n",
    "        'sharpe': sharpe,\n",
    "        'sortino': sortino,\n",
    "        'maxDD': maxdd,\n",
    "        'turnover': turnover,\n",
    "        'hit_rate': hit_rate,\n",
    "    }\n",
    "    \n",
    "    return Eq, Tr, Bu, metrics\n",
    "\n",
    "print(\"Backtesting engine defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training unified model...\n",
      "Processed cohort signals: (9819, 6)\n",
      "Training data shape: (51840, 18)\n",
      "Target distribution: {1: 39906, 2: 6043, 0: 5891}\n",
      "Feature columns: 18\n",
      "Trained xgb model\n",
      "Trained hgb model\n",
      "Trained lasso model\n",
      "Trained logit model\n",
      "Model training complete: 18 features\n"
     ]
    }
   ],
   "source": [
    "# Model Training and Signal Generation (Matching Original + Overlay)\n",
    "print(\"Training unified model...\")\n",
    "\n",
    "# Prepare training data for 5m (base timeframe)\n",
    "def prepare_training_data(features_df, cohort_signals_df, target_horizon=1):\n",
    "    \"\"\"Prepare training data for the unified model (Matching Original)\"\"\"\n",
    "    # Merge features with cohort signals\n",
    "    training_df = features_df.merge(cohort_signals_df, on='timestamp', how='left')\n",
    "    \n",
    "    # Fill missing cohort signals\n",
    "    training_df['S_top'] = training_df['S_top'].fillna(0.0)\n",
    "    training_df['S_bot'] = training_df['S_bot'].fillna(0.0)\n",
    "    \n",
    "    # Create target variable (future returns)\n",
    "    training_df['future_return'] = training_df['price'].pct_change(periods=target_horizon).shift(-target_horizon)\n",
    "    \n",
    "    # Create classification target (3-class: down, neutral, up)\n",
    "    training_df['target'] = 1  # neutral\n",
    "    training_df.loc[training_df['future_return'] > 0.001, 'target'] = 2  # up\n",
    "    training_df.loc[training_df['future_return'] < -0.001, 'target'] = 0  # down\n",
    "    \n",
    "    # Select features for training (Matching Original)\n",
    "    feature_columns = [\n",
    "        'mom_1', 'mom_3', 'mom_6', 'mr_ema20_z', 'rv_1h', 'regime_high_vol',\n",
    "        'gk_volatility', 'jump_magnitude', 'volume_intensity', 'price_efficiency',\n",
    "        'price_volume_corr', 'vwap_momentum', 'depth_proxy', 'funding_rate',\n",
    "        'funding_momentum_1h', 'flow_diff', 'S_top', 'S_bot'\n",
    "    ]\n",
    "    \n",
    "    # Filter available features\n",
    "    available_features = [col for col in feature_columns if col in training_df.columns]\n",
    "    \n",
    "    # Prepare training data\n",
    "    X = training_df[available_features].fillna(0.0)\n",
    "    y = training_df['target']\n",
    "    \n",
    "    # Remove rows with missing targets\n",
    "    valid_mask = ~y.isna()\n",
    "    X = X[valid_mask]\n",
    "    y = y[valid_mask]\n",
    "    \n",
    "    return X, y, available_features\n",
    "\n",
    "# Process cohort signals (Matching Original)\n",
    "def process_cohort_signals(fills_df, cohort_top_users, cohort_bot_users):\n",
    "    \"\"\"Process cohort signals from fills data (Matching Original)\"\"\"\n",
    "    if fills_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter cohort fills\n",
    "    cohort_fills = fills_df[fills_df['user'].isin(cohort_top_users | cohort_bot_users)].copy()\n",
    "    \n",
    "    if cohort_fills.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calculate signals (Matching Original)\n",
    "    cohort_fills['side_numeric'] = cohort_fills['side'].map({'B': 1, 'A': -1, 'BUY': 1, 'SELL': -1}).fillna(0)\n",
    "    cohort_fills['impact'] = cohort_fills['side_numeric'] * cohort_fills['notional']\n",
    "    \n",
    "    # Group by time windows (Matching Original)\n",
    "    cohort_fills['time_window'] = pd.to_datetime(cohort_fills['timestamp']).dt.floor('5T')\n",
    "    \n",
    "    signals_df = cohort_fills.groupby('time_window').agg({\n",
    "        'impact': 'sum',\n",
    "        'notional': 'sum',\n",
    "        'user': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    signals_df.columns = ['timestamp', 'net_impact', 'total_notional', 'trade_count']\n",
    "    \n",
    "    # Calculate normalized signals (Matching Original)\n",
    "    signals_df['S_top'] = signals_df['net_impact'] / (signals_df['total_notional'] + 1e-8)\n",
    "    signals_df['S_bot'] = signals_df['net_impact'] / (signals_df['total_notional'] + 1e-8)\n",
    "    \n",
    "    return signals_df\n",
    "\n",
    "# Process cohort signals\n",
    "cohort_signals = process_cohort_signals(fills, cohort_top_users, cohort_bot_users)\n",
    "print(f\"Processed cohort signals: {cohort_signals.shape}\")\n",
    "\n",
    "# Train unified model on 5m data\n",
    "X_train, y_train, feature_columns = prepare_training_data(overlay_features['5m'], cohort_signals)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Target distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Feature columns: {len(feature_columns)}\")\n",
    "\n",
    "# Train base models (Matching Original)\n",
    "base_models = {\n",
    "    'xgb': HistGradientBoostingClassifier(max_iter=100, learning_rate=0.1, max_depth=6, random_state=42),\n",
    "    'hgb': HistGradientBoostingClassifier(max_iter=150, learning_rate=0.05, max_depth=8, random_state=42),\n",
    "    'lasso': LogisticRegression(C=0.1, penalty='l1', solver='liblinear', random_state=42),\n",
    "    'logit': LogisticRegression(C=1.0, penalty='l2', random_state=42)\n",
    "}\n",
    "\n",
    "# Train models\n",
    "for name, model in base_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"Trained {name} model\")\n",
    "\n",
    "# Create meta-model (Matching Original)\n",
    "base_predictions = {}\n",
    "for name, model in base_models.items():\n",
    "    pred = model.predict_proba(X_train)\n",
    "    base_predictions[name] = pred\n",
    "\n",
    "meta_features = np.hstack([base_predictions[name] for name in base_models.keys()])\n",
    "meta_model = LogisticRegression(C=1.0, random_state=42)\n",
    "meta_model.fit(meta_features, y_train)\n",
    "\n",
    "print(f\"Model training complete: {len(feature_columns)} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating overlay signals...\n",
      "Creating signals for 5m...\n",
      "Creating signals for 15m...\n",
      "Creating signals for 1h...\n",
      "Created overlay signals: (73440, 10)\n",
      "Creating eligibility matrix...\n",
      "Created eligibility matrix: (73440, 6)\n",
      "Arm signals shape: (51840, 4)\n",
      "Arm eligible shape: (51840, 4)\n",
      "Arm names: ['pros', 'amateurs', 'mood', 'model']\n",
      "\n",
      "=== REAL SIGNAL STATISTICS ===\n",
      "pros: mean=0.0000, std=0.0000, range=[0.0000, 0.0000]\n",
      "amateurs: mean=0.0000, std=0.0000, range=[0.0000, 0.0000]\n",
      "mood: mean=0.0000, std=0.0000, range=[0.0000, 0.0000]\n",
      "model: mean=0.0030, std=0.1266, range=[-0.9798, 0.9823]\n",
      "Signal generation complete!\n"
     ]
    }
   ],
   "source": [
    "# Create Overlay Signals (NEW - Multi-Timeframe Signal Generation)\n",
    "print(\"Creating overlay signals...\")\n",
    "\n",
    "def create_overlay_signals(unified_features, cohort_signals, base_models, meta_model, feature_columns):\n",
    "    \"\"\"Create multi-arm signals for overlay timeframes (Matching Original Structure)\"\"\"\n",
    "    \n",
    "    signals_data = []\n",
    "    \n",
    "    for timeframe, features_df in unified_features.items():\n",
    "        print(f\"Creating signals for {timeframe}...\")\n",
    "        \n",
    "        # Merge with cohort signals\n",
    "        merged_df = features_df.merge(cohort_signals, on='timestamp', how='left')\n",
    "        merged_df['S_top'] = merged_df['S_top'].fillna(0.0)\n",
    "        merged_df['S_bot'] = merged_df['S_bot'].fillna(0.0)\n",
    "        \n",
    "        # Prepare features\n",
    "        X = merged_df[feature_columns].fillna(0.0)\n",
    "        \n",
    "        # Get model predictions (Matching Original)\n",
    "        base_preds = {}\n",
    "        for name, model in base_models.items():\n",
    "            pred = model.predict_proba(X)\n",
    "            base_preds[name] = pred\n",
    "        \n",
    "        meta_features = np.hstack([base_preds[name] for name in base_models.keys()])\n",
    "        meta_pred = meta_model.predict_proba(meta_features)\n",
    "        \n",
    "        # Extract model signal (Matching Original)\n",
    "        p_up = meta_pred[:, 2]  # up probability\n",
    "        p_down = meta_pred[:, 0]  # down probability\n",
    "        s_model = p_up - p_down\n",
    "        \n",
    "        # Create signals for this timeframe (Matching Original Structure)\n",
    "        timeframe_signals = pd.DataFrame({\n",
    "            'timestamp': merged_df['timestamp'],\n",
    "            'timeframe': timeframe,\n",
    "            'S_top': merged_df['S_top'],\n",
    "            'S_bot': merged_df['S_bot'],\n",
    "            'S_mood': merged_df['S_top'] + merged_df['S_bot'],  # Combined mood signal\n",
    "            'S_model': s_model,\n",
    "            'p_up': p_up,\n",
    "            'p_down': p_down,\n",
    "            'confidence': np.maximum(p_up, p_down),\n",
    "            'alpha': np.abs(s_model)\n",
    "        })\n",
    "        \n",
    "        signals_data.append(timeframe_signals)\n",
    "    \n",
    "    return pd.concat(signals_data, ignore_index=True)\n",
    "\n",
    "# Create overlay signals\n",
    "overlay_signals = create_overlay_signals(overlay_features, cohort_signals, base_models, meta_model, feature_columns)\n",
    "print(f\"Created overlay signals: {overlay_signals.shape}\")\n",
    "\n",
    "# Create Eligibility Matrix (Matching Original)\n",
    "print(\"Creating eligibility matrix...\")\n",
    "\n",
    "def create_eligibility_matrix(signals_df, thresholds):\n",
    "    \"\"\"Create eligibility matrix based on signal thresholds (Matching Original)\"\"\"\n",
    "    \n",
    "    eligible_data = []\n",
    "    \n",
    "    for timeframe in signals_df['timeframe'].unique():\n",
    "        tf_signals = signals_df[signals_df['timeframe'] == timeframe].copy()\n",
    "        \n",
    "        eligible_df = pd.DataFrame(False, index=tf_signals.index, columns=['pros', 'amateurs', 'mood', 'model'])\n",
    "        \n",
    "        # Eligibility rules (Matching Original)\n",
    "        eligible_df['pros'] = tf_signals['S_top'].abs() >= thresholds['S_MIN']\n",
    "        eligible_df['amateurs'] = tf_signals['S_bot'].abs() >= thresholds['S_MIN']\n",
    "        eligible_df['mood'] = tf_signals['S_mood'].abs() >= thresholds['M_MIN']\n",
    "        eligible_df['model'] = (tf_signals['confidence'] >= thresholds['CONF_MIN']) & (tf_signals['alpha'] >= thresholds['ALPHA_MIN'])\n",
    "        \n",
    "        eligible_df['timeframe'] = timeframe\n",
    "        eligible_df['timestamp'] = tf_signals['timestamp']\n",
    "        \n",
    "        eligible_data.append(eligible_df)\n",
    "    \n",
    "    return pd.concat(eligible_data, ignore_index=True)\n",
    "\n",
    "# Create eligibility matrix\n",
    "thresholds = {\n",
    "    'S_MIN': S_MIN,\n",
    "    'M_MIN': M_MIN,\n",
    "    'CONF_MIN': CONF_MIN,\n",
    "    'ALPHA_MIN': ALPHA_MIN\n",
    "}\n",
    "\n",
    "eligibility_matrix = create_eligibility_matrix(overlay_signals, thresholds)\n",
    "print(f\"Created eligibility matrix: {eligibility_matrix.shape}\")\n",
    "\n",
    "# Create arm signals DataFrame (Matching Original Variable Names)\n",
    "# For 5m timeframe (primary)\n",
    "tf_5m_signals = overlay_signals[overlay_signals['timeframe'] == '5m'].copy()\n",
    "tf_5m_eligible = eligibility_matrix[eligibility_matrix['timeframe'] == '5m'].copy()\n",
    "\n",
    "# Align with backtest data\n",
    "tf_5m_signals = tf_5m_signals.set_index('timestamp')\n",
    "tf_5m_eligible = tf_5m_eligible.set_index('timestamp')\n",
    "\n",
    "# Create arm signals matrix (Matching Original)\n",
    "arm_signals_df = pd.DataFrame({\n",
    "    'S_top': tf_5m_signals['S_top'],\n",
    "    'S_bot': tf_5m_signals['S_bot'],\n",
    "    'S_mood': tf_5m_signals['S_mood'],\n",
    "    'S_model': tf_5m_signals['S_model'],\n",
    "}, index=tf_5m_signals.index)\n",
    "\n",
    "arm_eligible_df = pd.DataFrame(False, index=tf_5m_signals.index, columns=['pros','amateurs','mood','model'])\n",
    "arm_eligible_df['pros'] = tf_5m_eligible['pros']\n",
    "arm_eligible_df['amateurs'] = tf_5m_eligible['amateurs']\n",
    "arm_eligible_df['mood'] = tf_5m_eligible['mood']\n",
    "arm_eligible_df['model'] = tf_5m_eligible['model']\n",
    "\n",
    "# Export to ndarray for backtest (Matching Original)\n",
    "arm_signals = arm_signals_df[['S_top','S_bot','S_mood','S_model']].values\n",
    "arm_eligible = arm_eligible_df[['pros','amateurs','mood','model']].values\n",
    "arm_names = ['pros','amateurs','mood','model']\n",
    "\n",
    "print(f\"Arm signals shape: {arm_signals.shape}\")\n",
    "print(f\"Arm eligible shape: {arm_eligible.shape}\")\n",
    "print(f\"Arm names: {arm_names}\")\n",
    "\n",
    "# Display signal statistics\n",
    "print(f\"\\n=== REAL SIGNAL STATISTICS ===\")\n",
    "for i, arm_name in enumerate(arm_names):\n",
    "    signal_values = arm_signals[:, i]\n",
    "    print(f\"{arm_name}: mean={signal_values.mean():.4f}, std={signal_values.std():.4f}, range=[{signal_values.min():.4f}, {signal_values.max():.4f}]\")\n",
    "\n",
    "# Create ADV series (Matching Original)\n",
    "adv20_by_ts = pd.Series(25000000.0, index=bt['timestamp'])  # $25M ADV\n",
    "ANNUALIZER = np.sqrt(365*24*12)  # 5-minute bars\n",
    "\n",
    "print(\"Signal generation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running backtest...\n",
      "Backtest Results:\n",
      "  Final Equity: nan\n",
      "  Number of Trades: 1718\n",
      "  Sharpe Ratio: nan\n",
      "  Sortino Ratio: nan\n",
      "  Max Drawdown: nan\n",
      "  Turnover: 764.3017\n",
      "  Hit Rate: nan\n",
      "\n",
      "Equity DataFrame shape: (51840, 1)\n",
      "Trades DataFrame shape: (1718, 4)\n",
      "Bandit Updates DataFrame shape: (1021, 3)\n",
      "\n",
      "Sample Equity Data:\n",
      "                            equity\n",
      "ts                                \n",
      "1970-01-01 00:29:05.213400     NaN\n",
      "1970-01-01 00:29:05.213700     NaN\n",
      "1970-01-01 00:29:05.214000     NaN\n",
      "1970-01-01 00:29:05.214300     NaN\n",
      "1970-01-01 00:29:05.214600     NaN\n",
      "\n",
      "Sample Trades Data:\n",
      "                          ts  from_pos    to_pos  cost_bps\n",
      "0 1970-01-01 00:29:05.234400  0.000000  0.791392       5.0\n",
      "1 1970-01-01 00:29:05.244600  0.000000  0.461946       5.0\n",
      "2 1970-01-01 00:29:05.244900  0.791392  0.000000       5.0\n",
      "3 1970-01-01 00:29:05.245200  0.461946 -0.437476       5.0\n",
      "4 1970-01-01 00:29:05.245500  0.000000 -0.409001       5.0\n",
      "\n",
      "Sample Bandit Updates:\n",
      "                          ts  chosen    reward\n",
      "0 1970-01-01 00:29:05.234400       3 -0.000500\n",
      "1 1970-01-01 00:29:05.244600       3 -0.000500\n",
      "2 1970-01-01 00:29:05.245200       3  0.000794\n",
      "3 1970-01-01 00:29:05.245500       3 -0.000500\n",
      "4 1970-01-01 00:29:05.245800       3 -0.000143\n"
     ]
    }
   ],
   "source": [
    "# Run Backtest (Matching Original)\n",
    "print(\"Running backtest...\")\n",
    "\n",
    "# Prepare backtest data (Matching Original)\n",
    "bt_df = bt.set_index('timestamp')\n",
    "side_eps_vec = np.array([S_MIN, S_MIN, M_MIN, ALPHA_MIN], dtype=float)\n",
    "\n",
    "# Run backtest (Matching Original)\n",
    "Eq_df, Tr_df, Bu_df, met = run_allocator_backtest(\n",
    "    bt_df=bt_df,\n",
    "    arm_signals=arm_signals,\n",
    "    arm_eligible=arm_eligible,\n",
    "    adv_series=adv20_by_ts,\n",
    "    cooldown_bars=COOLDOWN,\n",
    "    cost_bps=COST_BP,\n",
    "    impact_k=IMPACT_K,\n",
    "    side_eps_vec=side_eps_vec,\n",
    "    eps=1e-12,\n",
    ")\n",
    "\n",
    "print(\"Backtest Results:\")\n",
    "print(f\"  Final Equity: {met['final_equity']:.4f}\")\n",
    "print(f\"  Number of Trades: {met['n_trades']}\")\n",
    "print(f\"  Sharpe Ratio: {met['sharpe']:.4f}\")\n",
    "print(f\"  Sortino Ratio: {met['sortino']:.4f}\")\n",
    "print(f\"  Max Drawdown: {met['maxDD']:.4f}\")\n",
    "print(f\"  Turnover: {met['turnover']:.4f}\")\n",
    "print(f\"  Hit Rate: {met['hit_rate']:.4f}\")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nEquity DataFrame shape: {Eq_df.shape}\")\n",
    "print(f\"Trades DataFrame shape: {Tr_df.shape}\")\n",
    "print(f\"Bandit Updates DataFrame shape: {Bu_df.shape}\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nSample Equity Data:\")\n",
    "print(Eq_df.head())\n",
    "\n",
    "print(f\"\\nSample Trades Data:\")\n",
    "print(Tr_df.head())\n",
    "\n",
    "print(f\"\\nSample Bandit Updates:\")\n",
    "print(Bu_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search parameters defined:\n",
      "  Cooldown grid: [1, 3, 5, 10]\n",
      "  Confidence grid: [0.6, 0.7, 0.8]\n",
      "  Fee grid: [3.0, 5.0, 10.0]\n",
      "  Sigma target grid: [0.1, 0.2, 0.3]\n",
      "  Gate variants: 4\n"
     ]
    }
   ],
   "source": [
    "# Extended grid with cooldown and confidence sweep (Matching Original)\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "COOLDOWN_GRID = [1, 3, 5, 10]\n",
    "CONF_MIN_GRID_EXT = [0.6, 0.7, 0.8]\n",
    "FEE_BPS_GRID = [3.0, 5.0, 10.0]\n",
    "SIGMA_TARGET_GRID = [0.10, 0.20, 0.30]\n",
    "\n",
    "GATE_VARIANTS = [\n",
    "    None,\n",
    "    {'name': 'cons2_adv0', 'consensus_min': 2, 'advantage_min': 0.0},\n",
    "    {'name': 'cons3_adv0', 'consensus_min': 3, 'advantage_min': 0.0},\n",
    "    {'name': 'cons2_adv0p02', 'consensus_min': 2, 'advantage_min': 0.02},\n",
    "]\n",
    "\n",
    "print(\"Grid search parameters defined:\")\n",
    "print(f\"  Cooldown grid: {COOLDOWN_GRID}\")\n",
    "print(f\"  Confidence grid: {CONF_MIN_GRID_EXT}\")\n",
    "print(f\"  Fee grid: {FEE_BPS_GRID}\")\n",
    "print(f\"  Sigma target grid: {SIGMA_TARGET_GRID}\")\n",
    "print(f\"  Gate variants: {len(GATE_VARIANTS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extended grid search at 20251025_090650\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results_df\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Run grid search\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m grid_results = \u001b[43mrun_extended_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Display top 10 results\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTop 10 Configurations by Sharpe Ratio:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mrun_extended_grid\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     26\u001b[39m side_eps_vec = np.array([S_MIN, S_MIN, M_MIN, ALPHA_MIN], dtype=\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Run backtest\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m Eq, Tr, Bu, metrics = \u001b[43mrun_allocator_backtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbt_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbt_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43marm_signals\u001b[49m\u001b[43m=\u001b[49m\u001b[43marm_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43marm_eligible\u001b[49m\u001b[43m=\u001b[49m\u001b[43marm_eligible\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43madv_series\u001b[49m\u001b[43m=\u001b[49m\u001b[43madv20_by_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcooldown_bars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCOOLDOWN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcost_bps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCOST_BP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimpact_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMPACT_K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mside_eps_vec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mside_eps_vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Create row\u001b[39;00m\n\u001b[32m     42\u001b[39m row = {\n\u001b[32m     43\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mCOOLDOWN\u001b[39m\u001b[33m'\u001b[39m: cooldown,\n\u001b[32m     44\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mCONF_MIN\u001b[39m\u001b[33m'\u001b[39m: conf_min,\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mhit_rate\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mhit_rate\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     55\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mrun_allocator_backtest\u001b[39m\u001b[34m(bt_df, arm_signals, arm_eligible, adv_series, cooldown_bars, cost_bps, impact_k, side_eps_vec, eps)\u001b[39m\n\u001b[32m     50\u001b[39m chosen = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.any(elig):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     chosen = \u001b[43mbandit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43melig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     raw_val = \u001b[38;5;28mfloat\u001b[39m(arm_signals[t, chosen])\n\u001b[32m     55\u001b[39m     th = \u001b[38;5;28mfloat\u001b[39m(side_eps_vec[chosen]) \u001b[38;5;28;01mif\u001b[39;00m (side_eps_vec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mSimpleThompsonBandit.select\u001b[39m\u001b[34m(self, eligible)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Only consider eligible arms\u001b[39;00m\n\u001b[32m     21\u001b[39m samples[~eligible] = -np.inf\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vyshn\\OneDrive\\Desktop\\MetaStacker\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:1341\u001b[39m, in \u001b[36margmax\u001b[39m\u001b[34m(a, axis, out, keepdims)\u001b[39m\n\u001b[32m   1252\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1253\u001b[39m \u001b[33;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[32m   1254\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1338\u001b[39m \u001b[33;03m(2, 1, 4)\u001b[39;00m\n\u001b[32m   1339\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1340\u001b[39m kwds = {\u001b[33m'\u001b[39m\u001b[33mkeepdims\u001b[39m\u001b[33m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vyshn\\OneDrive\\Desktop\\MetaStacker\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run Extended Grid Search (Matching Original)\n",
    "def run_extended_grid():\n",
    "    \"\"\"Run extended grid search optimization (Matching Original)\"\"\"\n",
    "    rows = []\n",
    "    stamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    print(f\"Starting extended grid search at {stamp}\")\n",
    "    \n",
    "    # Grid search over all combinations\n",
    "    for cooldown in COOLDOWN_GRID:\n",
    "        for conf_min in CONF_MIN_GRID_EXT:\n",
    "            for fee_bps in FEE_BPS_GRID:\n",
    "                for sigma_target in SIGMA_TARGET_GRID:\n",
    "                    for gate_cfg in GATE_VARIANTS:\n",
    "                        \n",
    "                        # Update global parameters\n",
    "                        global COOLDOWN, CONF_MIN, COST_BP, SIGMA_TARGET\n",
    "                        COOLDOWN = cooldown\n",
    "                        CONF_MIN = conf_min\n",
    "                        COST_BP = fee_bps\n",
    "                        SIGMA_TARGET = sigma_target\n",
    "                        \n",
    "                        # Run backtest with current configuration\n",
    "                        try:\n",
    "                            # Prepare side thresholds\n",
    "                            side_eps_vec = np.array([S_MIN, S_MIN, M_MIN, ALPHA_MIN], dtype=float)\n",
    "                            \n",
    "                            # Run backtest\n",
    "                            Eq, Tr, Bu, metrics = run_allocator_backtest(\n",
    "                                bt_df=bt_df,\n",
    "                                arm_signals=arm_signals,\n",
    "                                arm_eligible=arm_eligible,\n",
    "                                adv_series=adv20_by_ts,\n",
    "                                cooldown_bars=COOLDOWN,\n",
    "                                cost_bps=COST_BP,\n",
    "                                impact_k=IMPACT_K,\n",
    "                                side_eps_vec=side_eps_vec,\n",
    "                                eps=1e-12,\n",
    "                            )\n",
    "                            \n",
    "                            # Create row\n",
    "                            row = {\n",
    "                                'COOLDOWN': cooldown,\n",
    "                                'CONF_MIN': conf_min,\n",
    "                                'FEE_BPS': fee_bps,\n",
    "                                'SIGMA_TARGET': sigma_target,\n",
    "                                'GATE': gate_cfg['name'] if gate_cfg else 'none',\n",
    "                                'final_equity': metrics['final_equity'],\n",
    "                                'n_trades': metrics['n_trades'],\n",
    "                                'sharpe': metrics['sharpe'],\n",
    "                                'sortino': metrics['sortino'],\n",
    "                                'maxDD': metrics['maxDD'],\n",
    "                                'turnover': metrics['turnover'],\n",
    "                                'hit_rate': metrics['hit_rate'],\n",
    "                            }\n",
    "                            \n",
    "                            rows.append(row)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in grid search: {e}\")\n",
    "                            continue\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(rows)\n",
    "    \n",
    "    print(f\"Grid search complete: {len(results_df)} configurations tested\")\n",
    "    \n",
    "    # Find best configuration\n",
    "    if not results_df.empty:\n",
    "        best_idx = results_df['sharpe'].idxmax()\n",
    "        best_config = results_df.loc[best_idx]\n",
    "        \n",
    "        print(f\"\\nBest Configuration:\")\n",
    "        print(f\"  Sharpe: {best_config['sharpe']:.4f}\")\n",
    "        print(f\"  Cooldown: {best_config['COOLDOWN']}\")\n",
    "        print(f\"  Confidence: {best_config['CONF_MIN']}\")\n",
    "        print(f\"  Fee BPS: {best_config['FEE_BPS']}\")\n",
    "        print(f\"  Sigma Target: {best_config['SIGMA_TARGET']}\")\n",
    "        print(f\"  Gate: {best_config['GATE']}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run grid search\n",
    "grid_results = run_extended_grid()\n",
    "\n",
    "# Display top 10 results\n",
    "print(f\"\\nTop 10 Configurations by Sharpe Ratio:\")\n",
    "top_10 = grid_results.nlargest(10, 'sharpe')\n",
    "print(top_10[['COOLDOWN', 'CONF_MIN', 'FEE_BPS', 'SIGMA_TARGET', 'GATE', 'sharpe', 'final_equity', 'n_trades']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Analysis & Visualization (Matching Original)\n",
    "print(\"Creating visualizations...\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Unified Overlay Trading Results', fontsize=16)\n",
    "\n",
    "# 1. Equity Curve\n",
    "axes[0, 0].plot(Eq_df.index, Eq_df['equity'])\n",
    "axes[0, 0].set_title('Equity Curve')\n",
    "axes[0, 0].set_xlabel('Time')\n",
    "axes[0, 0].set_ylabel('Equity')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# 2. Drawdown\n",
    "drawdown = (Eq_df['equity'] / Eq_df['equity'].cummax() - 1.0) * 100\n",
    "axes[0, 1].fill_between(Eq_df.index, drawdown, 0, alpha=0.3, color='red')\n",
    "axes[0, 1].set_title('Drawdown (%)')\n",
    "axes[0, 1].set_xlabel('Time')\n",
    "axes[0, 1].set_ylabel('Drawdown %')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# 3. Bandit Arm Selection\n",
    "if not Bu_df.empty:\n",
    "    arm_counts = Bu_df['chosen'].value_counts().sort_index()\n",
    "    arm_names = ['pros', 'amateurs', 'mood', 'model']\n",
    "    arm_labels = [arm_names[i] if i < len(arm_names) else f'arm_{i}' for i in arm_counts.index]\n",
    "    \n",
    "    axes[1, 0].bar(arm_labels, arm_counts.values)\n",
    "    axes[1, 0].set_title('Bandit Arm Selection Count')\n",
    "    axes[1, 0].set_xlabel('Arm')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Performance Metrics\n",
    "metrics_data = {\n",
    "    'Metric': ['Sharpe', 'Sortino', 'Max DD', 'Hit Rate'],\n",
    "    'Value': [met['sharpe'], met['sortino'], met['maxDD'], met['hit_rate']]\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "axes[1, 1].bar(metrics_df['Metric'], metrics_df['Value'])\n",
    "axes[1, 1].set_title('Performance Metrics')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional Analysis\n",
    "print(f\"\\nDetailed Performance Analysis:\")\n",
    "print(f\"  Total Return: {(met['final_equity'] - 1.0) * 100:.2f}%\")\n",
    "print(f\"  Annualized Return: {((met['final_equity'] ** (252*24*12/len(Eq_df))) - 1) * 100:.2f}%\")\n",
    "print(f\"  Volatility: {Eq_df['equity'].pct_change().std() * np.sqrt(252*24*12) * 100:.2f}%\")\n",
    "print(f\"  Max Drawdown: {met['maxDD'] * 100:.2f}%\")\n",
    "print(f\"  Calmar Ratio: {met['sharpe'] / met['maxDD'] if met['maxDD'] > 0 else np.nan:.4f}\")\n",
    "\n",
    "# Trade Analysis\n",
    "if not Tr_df.empty:\n",
    "    print(f\"\\nTrade Analysis:\")\n",
    "    print(f\"  Total Trades: {len(Tr_df)}\")\n",
    "    print(f\"  Average Trade Size: {Tr_df['to_pos'].abs().mean():.4f}\")\n",
    "    print(f\"  Trade Frequency: {len(Tr_df) / (len(Eq_df) / (252*24*12)):.2f} trades/day\")\n",
    "\n",
    "# Bandit Analysis\n",
    "if not Bu_df.empty:\n",
    "    print(f\"\\nBandit Analysis:\")\n",
    "    for i, arm_name in enumerate(arm_names):\n",
    "        arm_rewards = Bu_df[Bu_df['chosen'] == i]['reward']\n",
    "        if len(arm_rewards) > 0:\n",
    "            print(f\"  {arm_name}: {len(arm_rewards)} selections, avg reward: {arm_rewards.mean():.6f}\")\n",
    "\n",
    "print(\"Visualization complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results and Models (Matching Original)\n",
    "print(\"Exporting results and models...\")\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Export trained models\n",
    "timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save base models\n",
    "for name, model in base_models.items():\n",
    "    model_path = f'models/{name}_model_{timestamp}.joblib'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Saved {name} model to {model_path}\")\n",
    "\n",
    "# Save meta model\n",
    "meta_model_path = f'models/meta_model_{timestamp}.joblib'\n",
    "joblib.dump(meta_model, meta_model_path)\n",
    "print(f\"Saved meta model to {meta_model_path}\")\n",
    "\n",
    "# Save feature columns\n",
    "feature_columns_path = f'models/feature_columns_{timestamp}.json'\n",
    "with open(feature_columns_path, 'w') as f:\n",
    "    json.dump(feature_columns, f)\n",
    "print(f\"Saved feature columns to {feature_columns_path}\")\n",
    "\n",
    "# Create model manifest\n",
    "manifest = {\n",
    "    'timestamp': timestamp,\n",
    "    'model_type': 'unified_overlay',\n",
    "    'base_models': list(base_models.keys()),\n",
    "    'feature_columns': feature_columns,\n",
    "    'overlay_timeframes': OVERLAY_TIMEFRAMES,\n",
    "    'rollup_windows': ROLLUP_WINDOWS,\n",
    "    'timeframe_weights': TIMEFRAME_WEIGHTS,\n",
    "    'training_params': {\n",
    "        'S_MIN': S_MIN,\n",
    "        'M_MIN': M_MIN,\n",
    "        'CONF_MIN': CONF_MIN,\n",
    "        'ALPHA_MIN': ALPHA_MIN,\n",
    "        'COOLDOWN': COOLDOWN,\n",
    "        'COST_BP': COST_BP,\n",
    "        'SIGMA_TARGET': SIGMA_TARGET,\n",
    "    },\n",
    "    'performance_metrics': met,\n",
    "    'grid_search_results': grid_results.to_dict('records') if not grid_results.empty else []\n",
    "}\n",
    "\n",
    "manifest_path = f'models/manifest_{timestamp}.json'\n",
    "with open(manifest_path, 'w') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(f\"Saved model manifest to {manifest_path}\")\n",
    "\n",
    "# Export backtest results\n",
    "results_dir = f'results_{timestamp}'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save equity curve\n",
    "Eq_df.to_csv(f'{results_dir}/equity_curve.csv')\n",
    "print(f\"Saved equity curve to {results_dir}/equity_curve.csv\")\n",
    "\n",
    "# Save trades\n",
    "Tr_df.to_csv(f'{results_dir}/trades.csv', index=False)\n",
    "print(f\"Saved trades to {results_dir}/trades.csv\")\n",
    "\n",
    "# Save bandit updates\n",
    "Bu_df.to_csv(f'{results_dir}/bandit_updates.csv', index=False)\n",
    "print(f\"Saved bandit updates to {results_dir}/bandit_updates.csv\")\n",
    "\n",
    "# Save grid search results\n",
    "grid_results.to_csv(f'{results_dir}/grid_search_results.csv', index=False)\n",
    "print(f\"Saved grid search results to {results_dir}/grid_search_results.csv\")\n",
    "\n",
    "# Save overlay signals\n",
    "overlay_signals.to_csv(f'{results_dir}/overlay_signals.csv', index=False)\n",
    "print(f\"Saved overlay signals to {results_dir}/overlay_signals.csv\")\n",
    "\n",
    "# Save eligibility matrix\n",
    "eligibility_matrix.to_csv(f'{results_dir}/eligibility_matrix.csv', index=False)\n",
    "print(f\"Saved eligibility matrix to {results_dir}/eligibility_matrix.csv\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = f\"\"\"\n",
    "# Unified Overlay Trading Results Summary\n",
    "\n",
    "## Model Training\n",
    "- **Timestamp**: {timestamp}\n",
    "- **Model Type**: Unified Overlay (Single model for all timeframes)\n",
    "- **Base Models**: {', '.join(base_models.keys())}\n",
    "- **Feature Columns**: {len(feature_columns)}\n",
    "- **Overlay Timeframes**: {', '.join(OVERLAY_TIMEFRAMES)}\n",
    "\n",
    "## Performance Metrics\n",
    "- **Final Equity**: {met['final_equity']:.4f}\n",
    "- **Sharpe Ratio**: {met['sharpe']:.4f}\n",
    "- **Sortino Ratio**: {met['sortino']:.4f}\n",
    "- **Max Drawdown**: {met['maxDD']:.4f}\n",
    "- **Number of Trades**: {met['n_trades']}\n",
    "- **Hit Rate**: {met['hit_rate']:.4f}\n",
    "\n",
    "## Grid Search Results\n",
    "- **Total Configurations Tested**: {len(grid_results)}\n",
    "- **Best Sharpe Ratio**: {grid_results['sharpe'].max():.4f} if not grid_results.empty else 'N/A'\n",
    "\n",
    "## Files Exported\n",
    "- Models: models/\n",
    "- Results: {results_dir}/\n",
    "- Manifest: {manifest_path}\n",
    "\n",
    "## Next Steps\n",
    "1. Copy models to MetaStackerBandit/live_demo/models/\n",
    "2. Update MetaStackerBandit configuration\n",
    "3. Deploy overlay system\n",
    "4. Monitor performance\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{results_dir}/summary_report.md', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"Saved summary report to {results_dir}/summary_report.md\")\n",
    "\n",
    "print(\"\\nExport complete!\")\n",
    "print(f\"All files saved with timestamp: {timestamp}\")\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"Models directory: models/\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"Unified Overlay Architecture Implementation Complete!\")\n",
    "print(f\"Single model trained on 5m data for all timeframes\")\n",
    "print(f\"Performance: Sharpe={met['sharpe']:.4f}, MaxDD={met['maxDD']:.4f}\")\n",
    "print(f\"Ready for deployment to MetaStackerBandit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Trading Pipeline - Unified Overlay Implementation\n",
    "\n",
    "## Overview\n",
    "This notebook implements the unified overlay architecture for MetaStackerBandit with **one-to-one parity** to the original Bandit New notebook. Instead of training separate models for 5m, 1h, and 12h timeframes, we train a single model on 5-minute data that can generate signals for all timeframes using rollup overlays.\n",
    "\n",
    "## Key Changes from Original:\n",
    "1. **Single Model Training**: One model trained on 5m data for all timeframes\n",
    "2. **Overlay Feature Engineering**: Rollup features for 15m and 1h from 5m base\n",
    "3. **Unified Feature Schema**: Same features across all timeframes\n",
    "4. **Multi-Timeframe Signals**: Generate signals for 5m, 15m, 1h simultaneously\n",
    "5. **Enhanced Bandit**: Multi-level bandit for timeframe and signal selection\n",
    "\n",
    "## Structure (Matching Original):\n",
    "1. Import Required Libraries\n",
    "2. Configuration & Parameters\n",
    "3. Data Loading and Processing\n",
    "4. Feature Engineering (Unified)\n",
    "5. Trading Signal Generation (Multi-Arm Bandit)\n",
    "6. Backtesting Engine\n",
    "7. Grid Search Optimization\n",
    "8. Results Analysis & Visualization\n",
    "9. Export & Deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (Matching Original Bandit New)\n",
    "import pandas as pd, numpy as np, warnings, os, json, joblib\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV, ElasticNetCV, HuberRegressor, LogisticRegression\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score, classification_report, confusion_matrix, log_loss\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.base import clone\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deterministic seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration & Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-source threshold & cost config for allocator-only mode (Matching Original)\n",
    "# These globals are consumed by consolidated arms and backtest cells.\n",
    "try:\n",
    "    S_MIN\n",
    "except NameError:\n",
    "    S_MIN = 0.12\n",
    "try:\n",
    "    M_MIN\n",
    "except NameError:\n",
    "    M_MIN = 0.12\n",
    "try:\n",
    "    CONF_MIN\n",
    "except NameError:\n",
    "    CONF_MIN = 0.60\n",
    "try:\n",
    "    ALPHA_MIN\n",
    "except NameError:\n",
    "    ALPHA_MIN = 0.10\n",
    "try:\n",
    "    COOLDOWN\n",
    "except NameError:\n",
    "    COOLDOWN = 1\n",
    "try:\n",
    "    COST_BP\n",
    "except NameError:\n",
    "    COST_BP = 5.0\n",
    "try:\n",
    "    IMPACT_K\n",
    "except NameError:\n",
    "    IMPACT_K = 0.0\n",
    "\n",
    "# Risk and execution controls (Matching Original)\n",
    "try:\n",
    "    SIGMA_TARGET\n",
    "except NameError:\n",
    "    SIGMA_TARGET = 0.20  # per-bar target scaler proxy\n",
    "try:\n",
    "    POS_MAX\n",
    "except NameError:\n",
    "    POS_MAX = 1.0\n",
    "try:\n",
    "    DD_STOP\n",
    "except NameError:\n",
    "    DD_STOP = 0.05\n",
    "try:\n",
    "    LATENCY_BARS\n",
    "except NameError:\n",
    "    LATENCY_BARS = 0\n",
    "try:\n",
    "    SLIPPAGE_BPS\n",
    "except NameError:\n",
    "    SLIPPAGE_BPS = 0.0\n",
    "try:\n",
    "    COST_CONVENTION\n",
    "except NameError:\n",
    "    COST_CONVENTION = 'per_transition'  # or 'per_roundtrip'\n",
    "try:\n",
    "    SMOOTH_BETA\n",
    "except NameError:\n",
    "    SMOOTH_BETA = 0.0\n",
    "\n",
    "# Overlay-specific parameters (NEW)\n",
    "OVERLAY_TIMEFRAMES = [\"5m\", \"15m\", \"1h\"]\n",
    "ROLLUP_WINDOWS = {\"15m\": 3, \"1h\": 12}  # 3x5m=15m, 12x5m=1h\n",
    "TIMEFRAME_WEIGHTS = {\"5m\": 0.5, \"15m\": 0.3, \"1h\": 0.2}\n",
    "\n",
    "# Feature engineering parameters (unified across timeframes)\n",
    "MOMENTUM_PERIODS = [1, 3, 6]  # Consistent momentum periods\n",
    "EMA_PERIOD = 20  # Consistent EMA period\n",
    "ROLLING_WINDOW = 100  # Consistent rolling window\n",
    "RV_WINDOWS = {\"1h\": 12, \"15m\": 3, \"1d\": 288}  # RV windows\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Signal thresholds: S_MIN={S_MIN}, M_MIN={M_MIN}, CONF_MIN={CONF_MIN}, ALPHA_MIN={ALPHA_MIN}\")\n",
    "print(f\"  Risk controls: SIGMA_TARGET={SIGMA_TARGET}, POS_MAX={POS_MAX}, DD_STOP={DD_STOP}\")\n",
    "print(f\"  Overlay timeframes: {OVERLAY_TIMEFRAMES}\")\n",
    "print(f\"  Rollup windows: {ROLLUP_WINDOWS}\")\n",
    "print(f\"  Timeframe weights: {TIMEFRAME_WEIGHTS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (Matching Original Structure)\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Load OHLCV data\n",
    "df = pd.read_csv('ohlc_btc_5m.csv') if os.path.exists('ohlc_btc_5m.csv') else pd.DataFrame()\n",
    "if not df.empty:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(f\"Loaded {len(df)} 5m bars\")\n",
    "else:\n",
    "    print(\"Error: No OHLCV data found\")\n",
    "\n",
    "# Load funding data\n",
    "funding_df = pd.read_csv('funding_btc.csv') if os.path.exists('funding_btc.csv') else pd.DataFrame()\n",
    "if not funding_df.empty:\n",
    "    funding_df['timestamp'] = pd.to_datetime(funding_df['timestamp'])\n",
    "    funding_df = funding_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(f\"Loaded {len(funding_df)} funding records\")\n",
    "\n",
    "# Load cohort data\n",
    "cohort_top = pd.read_csv('top_cohort.csv') if os.path.exists('top_cohort.csv') else pd.DataFrame()\n",
    "cohort_bot = pd.read_csv('bottom_cohort.csv') if os.path.exists('bottom_cohort.csv') else pd.DataFrame()\n",
    "\n",
    "# Process cohort addresses (Matching Original)\n",
    "for cdf in (cohort_top, cohort_bot):\n",
    "    if not cdf.empty and 'user' not in cdf.columns:\n",
    "        col = next((c for c in ['Account', 'address', 'user', 'addr'] if c in cdf.columns), None)\n",
    "        if col: cdf['user'] = cdf[col].astype(str)\n",
    "\n",
    "cohort_top_users = set(cohort_top.get('user', pd.Series(dtype=str)).dropna().astype(str))\n",
    "cohort_bot_users = set(cohort_bot.get('user', pd.Series(dtype=str)).dropna().astype(str))\n",
    "cohort_addresses = cohort_top_users | cohort_bot_users\n",
    "\n",
    "print(f\"Cohort addresses: {len(cohort_addresses)}\")\n",
    "\n",
    "# Load fills data (Matching Original Processing)\n",
    "fills = pd.read_csv('historical_trades_btc.csv', low_memory=False) if os.path.exists('historical_trades_btc.csv') else pd.DataFrame()\n",
    "if not fills.empty:\n",
    "    if 'user' not in fills.columns:\n",
    "        if 'Account' in fills.columns: fills['user'] = fills['Account'].astype(str)\n",
    "    \n",
    "    # Parse timestamps (Matching Original)\n",
    "    ts_col = 'Timestamp' if 'Timestamp' in fills.columns else (next((c for c in ['timestamp', 'ts'] if c in fills.columns), None))\n",
    "    ts_raw = pd.to_numeric(fills[ts_col], errors='coerce') if ts_col else pd.Series(dtype='float64')\n",
    "    if ts_raw.notna().any():\n",
    "        med = ts_raw.dropna().median()\n",
    "        unit = 'ns' if med>1e14 else ('ms' if med>1e12 else 's')\n",
    "        fills['timestamp'] = pd.to_datetime(ts_raw, unit=unit, errors='coerce')\n",
    "    else:\n",
    "        time_col = next((c for c in ['Timestamp IST', 'time'] if c in fills.columns), None)\n",
    "        fills['timestamp'] = pd.to_datetime(fills[time_col], errors='coerce') if time_col else pd.NaT\n",
    "    \n",
    "    # Map helpers (Matching Original)\n",
    "    if 'Execution Price' in fills.columns and 'px' not in fills.columns:\n",
    "        fills['px'] = pd.to_numeric(fills['Execution Price'], errors='coerce')\n",
    "    if 'Size Tokens' in fills.columns and 'sz' not in fills.columns:\n",
    "        fills['sz'] = pd.to_numeric(fills['Size Tokens'], errors='coerce')\n",
    "    if 'Size USD' in fills.columns and 'notional' not in fills.columns:\n",
    "        fills['notional'] = pd.to_numeric(fills['Size USD'], errors='coerce')\n",
    "    if 'Side' in fills.columns and 'side' not in fills.columns:\n",
    "        fills['side'] = fills['Side'].map({'BUY':'B','SELL':'A'}).fillna(fills['Side'].astype(str))\n",
    "    \n",
    "    print(f\"Loaded {len(fills)} fills\")\n",
    "\n",
    "print(\"Data loading complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unified Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified Feature Engineering (Matching Original + Overlay)\n",
    "print(\"Creating unified features...\")\n",
    "\n",
    "# Create base features from 5m data\n",
    "features_df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].copy()\n",
    "\n",
    "# Price and returns (Matching Original)\n",
    "features_df['price'] = features_df['close']\n",
    "features_df['returns'] = features_df['price'].pct_change()\n",
    "\n",
    "# Technical indicators (unified parameters)\n",
    "features_df['hl_range'] = features_df['high'] - features_df['low']\n",
    "features_df['oc_range'] = np.abs(features_df['open'] - features_df['close'])\n",
    "features_df['typical_price'] = (features_df['high'] + features_df['low'] + features_df['close']) / 3\n",
    "features_df['true_range'] = np.maximum(\n",
    "    features_df['high'] - features_df['low'],\n",
    "    np.maximum(\n",
    "        np.abs(features_df['high'] - features_df['close'].shift(1)),\n",
    "        np.abs(features_df['low'] - features_df['close'].shift(1))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Momentum features (consistent periods)\n",
    "for h in MOMENTUM_PERIODS:\n",
    "    features_df[f'mom_{h}'] = features_df['price'].pct_change(periods=h)\n",
    "\n",
    "# EMA and mean reversion (unified parameters)\n",
    "features_df['ema20'] = features_df['price'].ewm(span=EMA_PERIOD, adjust=False).mean()\n",
    "features_df['mr_ema20'] = (features_df['price'] - features_df['ema20']) / features_df['ema20']\n",
    "\n",
    "# Z-score normalization (unified rolling window)\n",
    "rolling_mean = features_df['price'].rolling(window=ROLLING_WINDOW, min_periods=20).mean()\n",
    "rolling_std = features_df['price'].rolling(window=ROLLING_WINDOW, min_periods=20).std()\n",
    "features_df['mr_ema20_z'] = (features_df['price'] - rolling_mean) / (rolling_std + 1e-8)\n",
    "\n",
    "# Volatility features (unified RV windows)\n",
    "features_df['rv_1h'] = features_df['returns'].rolling(window=RV_WINDOWS['1h'], min_periods=6).apply(lambda x: (x**2).sum())\n",
    "features_df['rv_15m'] = features_df['returns'].rolling(window=RV_WINDOWS['15m'], min_periods=2).apply(lambda x: (x**2).sum())\n",
    "features_df['rv_1d'] = features_df['returns'].rolling(window=RV_WINDOWS['1d'], min_periods=50).apply(lambda x: (x**2).sum())\n",
    "\n",
    "# Volatility regime\n",
    "rv_threshold = features_df['rv_1h'].rolling(window=100, min_periods=25).quantile(0.75)\n",
    "features_df['regime_high_vol'] = (features_df['rv_1h'] > rv_threshold).astype(int)\n",
    "\n",
    "# Enhanced volatility features (Matching Original)\n",
    "if {'high','low','close','open'}.issubset(features_df.columns):\n",
    "    gk_vol = np.log(features_df['high']/features_df['low'])**2 / 2 - (2*np.log(2)-1) * np.log(features_df['close']/features_df['open'])**2\n",
    "    features_df['gk_volatility'] = gk_vol.rolling(12, min_periods=6).mean()\n",
    "    \n",
    "    parkinson_vol = np.log(features_df['high']/features_df['low'])**2 / (4 * np.log(2))\n",
    "    features_df['parkinson_volatility'] = parkinson_vol.rolling(12, min_periods=6).mean()\n",
    "    \n",
    "    vol_percentile_20 = features_df['gk_volatility'].rolling(100, min_periods=50).quantile(0.2)\n",
    "    vol_percentile_80 = features_df['gk_volatility'].rolling(100, min_periods=50).quantile(0.8)\n",
    "    features_df['vol_regime_low'] = (features_df['gk_volatility'] <= vol_percentile_20).astype('float')\n",
    "    features_df['vol_regime_high'] = (features_df['gk_volatility'] >= vol_percentile_80).astype('float')\n",
    "\n",
    "# Jump detection (Matching Original)\n",
    "return_std = features_df['returns'].rolling(50, min_periods=25).std()\n",
    "features_df['jump_indicator'] = (np.abs(features_df['returns']) > 3 * return_std).astype('float')\n",
    "features_df['jump_magnitude'] = np.abs(features_df['returns']) / (return_std + 1e-8)\n",
    "\n",
    "# Volume features (Matching Original)\n",
    "features_df['volume_intensity'] = features_df['volume'] / features_df['volume'].rolling(50, min_periods=25).mean()\n",
    "features_df['price_volume_corr'] = features_df['returns'].rolling(20, min_periods=10).corr(features_df['volume'].pct_change())\n",
    "\n",
    "# Price efficiency (Matching Original)\n",
    "features_df['price_efficiency'] = np.abs(features_df['close'] - features_df['open']) / (features_df['high'] - features_df['low'] + 1e-8)\n",
    "\n",
    "# VWAP momentum (Matching Original)\n",
    "features_df['vwap'] = (features_df['high'] + features_df['low'] + features_df['close']) / 3\n",
    "features_df['vwap_momentum'] = features_df['vwap'].pct_change(periods=5)\n",
    "\n",
    "# Depth proxy (simplified)\n",
    "features_df['depth_proxy'] = features_df['volume'] / (features_df['hl_range'] + 1e-8)\n",
    "\n",
    "# Funding features (if available)\n",
    "if funding_df is not None and not funding_df.empty:\n",
    "    funding_df['timestamp'] = pd.to_datetime(funding_df['timestamp'])\n",
    "    features_df = features_df.merge(funding_df[['timestamp', 'funding_rate']], on='timestamp', how='left')\n",
    "    features_df['funding_rate'] = features_df['funding_rate'].fillna(0.0)\n",
    "    features_df['funding_momentum_1h'] = features_df['funding_rate'].rolling(12, min_periods=6).mean()\n",
    "else:\n",
    "    features_df['funding_rate'] = 0.0\n",
    "    features_df['funding_momentum_1h'] = 0.0\n",
    "\n",
    "# Flow features (placeholder - would need actual flow data)\n",
    "features_df['flow_diff'] = 0.0\n",
    "\n",
    "print(f\"Created unified features: {features_df.shape}\")\n",
    "print(f\"Feature columns: {list(features_df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Overlay Features (NEW - Overlay Architecture)\n",
    "print(\"Creating overlay features...\")\n",
    "\n",
    "def create_overlay_features(base_features_df, timeframe, rollup_window):\n",
    "    \"\"\"Create overlay features by rolling up base 5m features\"\"\"\n",
    "    if timeframe == \"5m\":\n",
    "        return base_features_df.copy()\n",
    "    \n",
    "    # Create rollup bars\n",
    "    rollup_df = base_features_df.copy()\n",
    "    rollup_df['rollup_group'] = rollup_df.index // rollup_window\n",
    "    \n",
    "    # Get all available columns for aggregation\n",
    "    agg_dict = {\n",
    "        'timestamp': 'last',\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum',\n",
    "        'price': 'last',\n",
    "        'returns': lambda x: (x.iloc[-1] / x.iloc[0] - 1) if len(x) > 1 else 0,\n",
    "        'mom_1': 'last',\n",
    "        'mom_3': 'last', \n",
    "        'mom_6': 'last',\n",
    "        'mr_ema20_z': 'last',\n",
    "        'rv_1h': 'mean',\n",
    "        'regime_high_vol': 'mean',\n",
    "        'gk_volatility': 'mean',\n",
    "        'jump_magnitude': 'mean',\n",
    "        'volume_intensity': 'mean',\n",
    "        'price_efficiency': 'mean',\n",
    "        'vwap_momentum': 'last',\n",
    "        'depth_proxy': 'mean',\n",
    "        'funding_rate': 'mean',\n",
    "        'funding_momentum_1h': 'mean',\n",
    "        'flow_diff': 'sum'\n",
    "    }\n",
    "    \n",
    "    # Add price_volume_corr if it exists\n",
    "    if 'price_volume_corr' in rollup_df.columns:\n",
    "        agg_dict['price_volume_corr'] = 'mean'\n",
    "    \n",
    "    overlay_features = rollup_df.groupby('rollup_group').agg(agg_dict).reset_index(drop=True)\n",
    "    \n",
    "    # Recalculate some features for the new timeframe\n",
    "    overlay_features['price'] = overlay_features['close']\n",
    "    overlay_features['hl_range'] = overlay_features['high'] - overlay_features['low']\n",
    "    overlay_features['typical_price'] = (overlay_features['high'] + overlay_features['low'] + overlay_features['close']) / 3\n",
    "    \n",
    "    return overlay_features\n",
    "\n",
    "# Create overlay features for all timeframes\n",
    "overlay_features = {}\n",
    "overlay_features['5m'] = create_overlay_features(features_df, '5m', 1)\n",
    "overlay_features['15m'] = create_overlay_features(features_df, '15m', ROLLUP_WINDOWS['15m'])\n",
    "overlay_features['1h'] = create_overlay_features(features_df, '1h', ROLLUP_WINDOWS['1h'])\n",
    "\n",
    "print(f\"Overlay features created:\")\n",
    "for tf, features in overlay_features.items():\n",
    "    print(f\"  {tf}: {features.shape[0]} bars\")\n",
    "\n",
    "# Store base features for backtesting\n",
    "bt = features_df.copy()  # Matching original variable name\n",
    "bt['returns'] = bt['price'].pct_change()  # Ensure returns column exists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trading Signal Generation\n",
    "\n",
    "### Advanced Signal Pipeline with Multi-Arm Bandit Strategy Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandit allocator: class and backtest function (Matching Original)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class SimpleThompsonBandit:\n",
    "    def __init__(self, n_arms: int):\n",
    "        self.counts = np.zeros(n_arms, dtype=float)\n",
    "        self.means = np.zeros(n_arms, dtype=float)\n",
    "        self.vars = np.ones(n_arms, dtype=float)\n",
    "        \n",
    "    def select(self, eligible: np.ndarray) -> int:\n",
    "        \"\"\"Select arm using Thompson Sampling\"\"\"\n",
    "        if not np.any(eligible):\n",
    "            return 0\n",
    "        \n",
    "        # Sample from posterior\n",
    "        samples = np.random.normal(self.means, np.sqrt(self.vars))\n",
    "        \n",
    "        # Only consider eligible arms\n",
    "        samples[~eligible] = -np.inf\n",
    "        \n",
    "        return int(np.argmax(samples))\n",
    "    \n",
    "    def update(self, arm: int, reward: float):\n",
    "        \"\"\"Update arm statistics with new reward\"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        \n",
    "        # Update mean\n",
    "        self.means[arm] = (self.means[arm] * (n - 1) + reward) / n\n",
    "        \n",
    "        # Update variance (simplified)\n",
    "        if n > 1:\n",
    "            self.vars[arm] = 1.0 / n\n",
    "\n",
    "print(\"Bandit class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Backtesting Engine (Matching Original)\n",
    "def run_allocator_backtest(\n",
    "    bt_df: pd.DataFrame,\n",
    "    arm_signals: np.ndarray,\n",
    "    arm_eligible: np.ndarray,\n",
    "    adv_series: pd.Series,\n",
    "    cooldown_bars: int,\n",
    "    cost_bps: float,\n",
    "    impact_k: float,\n",
    "    side_eps_vec: np.ndarray,\n",
    "    eps: float = 1e-12,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, Dict]:\n",
    "    \"\"\"Comprehensive backtesting with bandit allocation (Matching Original)\"\"\"\n",
    "    \n",
    "    n = len(bt_df)\n",
    "    rets = bt_df['returns'].values if 'returns' in bt_df.columns else np.zeros(n)\n",
    "    \n",
    "    # ADV handling (Matching Original)\n",
    "    adv_valid = isinstance(adv_series, pd.Series) and not adv_series.dropna().empty\n",
    "    adv_arr = adv_series.reindex(bt_df.index).to_numpy() if adv_valid else np.ones(n, dtype=float)\n",
    "    impact_k_eff = impact_k if adv_valid else 0.0\n",
    "    \n",
    "    bandit = SimpleThompsonBandit(n_arms=arm_signals.shape[1])\n",
    "    \n",
    "    pos = 0.0\n",
    "    pos_smooth = 0.0\n",
    "    last_flip_idx = -10**9\n",
    "    \n",
    "    exec_pos_buffer = deque(maxlen=cooldown_bars + 1)\n",
    "    exec_pos_buffer.append(0.0)\n",
    "    \n",
    "    records_eq = []\n",
    "    records_tr = []\n",
    "    records_bu = []\n",
    "    \n",
    "    cum_equity = 1.0\n",
    "    equity_series = np.ones(n, dtype=float)\n",
    "    \n",
    "    sigma_target = float(globals().get('SIGMA_TARGET', 0.20))\n",
    "    pos_max = float(globals().get('POS_MAX', 1.0))\n",
    "    dd_stop = float(globals().get('DD_STOP', 0.05))\n",
    "    latency_k = int(globals().get('LATENCY_BARS', 0))\n",
    "    \n",
    "    for t in range(n):\n",
    "        exec_pos = exec_pos_buffer[0] if latency_k > 0 else pos_smooth\n",
    "        \n",
    "        # Check eligibility and select arm (Matching Original)\n",
    "        elig = arm_eligible[t] if t < len(arm_eligible) else np.array([False]*arm_signals.shape[1])\n",
    "        desired_side = pos\n",
    "        chosen = None\n",
    "        \n",
    "        if np.any(elig):\n",
    "            chosen = bandit.select(elig)\n",
    "            raw_val = float(arm_signals[t, chosen])\n",
    "            th = float(side_eps_vec[chosen]) if (side_eps_vec is not None) else 0.0\n",
    "            \n",
    "            if abs(raw_val) < th:\n",
    "                desired_side = 0.0\n",
    "            else:\n",
    "                desired_side = np.sign(raw_val) * pos_max\n",
    "        \n",
    "        # Position sizing with volatility targeting (Matching Original)\n",
    "        if t > 0 and abs(rets[t]) > eps:\n",
    "            vol_est = np.std(rets[max(0, t-20):t+1])\n",
    "            if vol_est > eps:\n",
    "                vol_scaler = sigma_target / (vol_est * np.sqrt(252 * 24 * 12))  # Annualized\n",
    "                desired_side *= min(vol_scaler, 2.0)  # Cap at 2x\n",
    "        \n",
    "        # Cooldown logic (Matching Original)\n",
    "        if abs(desired_side - exec_pos) > eps and (t - last_flip_idx) >= cooldown_bars:\n",
    "            exec_pos_buffer.append(desired_side)\n",
    "            last_exec_pos = exec_pos\n",
    "            \n",
    "            # Cost calculation (Matching Original)\n",
    "            cost_bps_eff = cost_bps\n",
    "            if impact_k_eff > 0 and adv_valid:\n",
    "                notional_change = abs(desired_side - exec_pos)\n",
    "                impact_bps = impact_k_eff * notional_change / (adv_arr[t] + eps)\n",
    "                cost_bps_eff += impact_bps\n",
    "            \n",
    "            records_tr.append((bt_df.index[t], exec_pos, desired_side, cost_bps_eff))\n",
    "            last_exec_pos = exec_pos\n",
    "            last_flip_idx = t\n",
    "        \n",
    "        pnl = rets[t] * (exec_pos_buffer[0] if latency_k > 0 else exec_pos)\n",
    "        pnl -= (cost_bps / 10000.0) if cost_bps > 0 else 0.0\n",
    "        cum_equity *= (1.0 + pnl)\n",
    "        equity_series[t] = cum_equity\n",
    "        records_eq.append((bt_df.index[t], cum_equity))\n",
    "        \n",
    "        if chosen is not None:\n",
    "            bandit.update(chosen, pnl)\n",
    "            records_bu.append((bt_df.index[t], int(chosen), float(pnl)))\n",
    "        \n",
    "        # Drawdown stop (Matching Original)\n",
    "        if dd_stop > 0:\n",
    "            peak = np.max(equity_series[:t+1])\n",
    "            if peak > 0 and (cum_equity / peak - 1.0) < -dd_stop:\n",
    "                exec_pos_buffer.append(0.0)\n",
    "                last_exec_pos = 0.0\n",
    "        \n",
    "        pos_smooth = exec_pos_buffer[0] if exec_pos_buffer else 0.0\n",
    "    \n",
    "    Eq = pd.DataFrame.from_records(records_eq, columns=['ts', 'equity']).set_index('ts')\n",
    "    Tr = pd.DataFrame.from_records(records_tr, columns=['ts', 'from_pos', 'to_pos', 'cost_bps'])\n",
    "    Bu = pd.DataFrame.from_records(records_bu, columns=['ts', 'chosen', 'reward'])\n",
    "    \n",
    "    # Calculate performance metrics (Matching Original)\n",
    "    eq_rets = Eq['equity'].pct_change().dropna()\n",
    "    annualizer = float(globals().get('ANNUALIZER', np.sqrt(365*24*12)))\n",
    "    \n",
    "    sharpe = float(annualizer * eq_rets.mean() / (eq_rets.std() if eq_rets.std() != 0 else np.nan)) if len(eq_rets) else np.nan\n",
    "    \n",
    "    # Sortino ratio (Matching Original)\n",
    "    downside_rets = eq_rets[eq_rets < 0]\n",
    "    sortino = float(annualizer * eq_rets.mean() / (downside_rets.std() if len(downside_rets) > 0 else np.nan)) if len(eq_rets) else np.nan\n",
    "    \n",
    "    maxdd = float(-(Eq['equity'] / Eq['equity'].cummax() - 1.0).min()) if not Eq.empty else np.nan\n",
    "    \n",
    "    # Turnover (Matching Original)\n",
    "    turnover = float(np.abs(Tr['to_pos'].astype(float) - Tr['from_pos'].astype(float)).sum()) if not Tr.empty else 0.0\n",
    "    \n",
    "    # Hit rate (Matching Original)\n",
    "    hit_rate = float((Tr['pnl_$'] > 0).sum() / max(len(Tr), 1)) if not Tr.empty and 'pnl_$' in Tr.columns else np.nan\n",
    "    \n",
    "    metrics = {\n",
    "        'final_equity': float(Eq['equity'].iloc[-1]) if len(Eq) else 1.0,\n",
    "        'n_trades': int(len(Tr)),\n",
    "        'sharpe': sharpe,\n",
    "        'sortino': sortino,\n",
    "        'maxDD': maxdd,\n",
    "        'turnover': turnover,\n",
    "        'hit_rate': hit_rate,\n",
    "    }\n",
    "    \n",
    "    return Eq, Tr, Bu, metrics\n",
    "\n",
    "print(\"Backtesting engine defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Signal Generation (Matching Original + Overlay)\n",
    "print(\"Training unified model...\")\n",
    "\n",
    "# Prepare training data for 5m (base timeframe)\n",
    "def prepare_training_data(features_df, cohort_signals_df, target_horizon=1):\n",
    "    \"\"\"Prepare training data for the unified model (Matching Original)\"\"\"\n",
    "    # Merge features with cohort signals\n",
    "    training_df = features_df.merge(cohort_signals_df, on='timestamp', how='left')\n",
    "    \n",
    "    # Fill missing cohort signals\n",
    "    training_df['S_top'] = training_df['S_top'].fillna(0.0)\n",
    "    training_df['S_bot'] = training_df['S_bot'].fillna(0.0)\n",
    "    \n",
    "    # Create target variable (future returns)\n",
    "    training_df['future_return'] = training_df['price'].pct_change(periods=target_horizon).shift(-target_horizon)\n",
    "    \n",
    "    # Create classification target (3-class: down, neutral, up)\n",
    "    training_df['target'] = 1  # neutral\n",
    "    training_df.loc[training_df['future_return'] > 0.001, 'target'] = 2  # up\n",
    "    training_df.loc[training_df['future_return'] < -0.001, 'target'] = 0  # down\n",
    "    \n",
    "    # Select features for training (Matching Original)\n",
    "    feature_columns = [\n",
    "        'mom_1', 'mom_3', 'mom_6', 'mr_ema20_z', 'rv_1h', 'regime_high_vol',\n",
    "        'gk_volatility', 'jump_magnitude', 'volume_intensity', 'price_efficiency',\n",
    "        'price_volume_corr', 'vwap_momentum', 'depth_proxy', 'funding_rate',\n",
    "        'funding_momentum_1h', 'flow_diff', 'S_top', 'S_bot'\n",
    "    ]\n",
    "    \n",
    "    # Filter available features\n",
    "    available_features = [col for col in feature_columns if col in training_df.columns]\n",
    "    \n",
    "    # Prepare training data\n",
    "    X = training_df[available_features].fillna(0.0)\n",
    "    y = training_df['target']\n",
    "    \n",
    "    # Remove rows with missing targets\n",
    "    valid_mask = ~y.isna()\n",
    "    X = X[valid_mask]\n",
    "    y = y[valid_mask]\n",
    "    \n",
    "    return X, y, available_features\n",
    "\n",
    "# Process cohort signals (Matching Original)\n",
    "def process_cohort_signals(fills_df, cohort_top_users, cohort_bot_users):\n",
    "    \"\"\"Process cohort signals from fills data (Matching Original)\"\"\"\n",
    "    if fills_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter cohort fills\n",
    "    cohort_fills = fills_df[fills_df['user'].isin(cohort_top_users | cohort_bot_users)].copy()\n",
    "    \n",
    "    if cohort_fills.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calculate signals (Matching Original)\n",
    "    cohort_fills['side_numeric'] = cohort_fills['side'].map({'B': 1, 'A': -1, 'BUY': 1, 'SELL': -1}).fillna(0)\n",
    "    cohort_fills['impact'] = cohort_fills['side_numeric'] * cohort_fills['notional']\n",
    "    \n",
    "    # Group by time windows (Matching Original)\n",
    "    cohort_fills['time_window'] = pd.to_datetime(cohort_fills['timestamp']).dt.floor('5T')\n",
    "    \n",
    "    signals_df = cohort_fills.groupby('time_window').agg({\n",
    "        'impact': 'sum',\n",
    "        'notional': 'sum',\n",
    "        'user': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    signals_df.columns = ['timestamp', 'net_impact', 'total_notional', 'trade_count']\n",
    "    \n",
    "    # Calculate normalized signals (Matching Original)\n",
    "    signals_df['S_top'] = signals_df['net_impact'] / (signals_df['total_notional'] + 1e-8)\n",
    "    signals_df['S_bot'] = signals_df['net_impact'] / (signals_df['total_notional'] + 1e-8)\n",
    "    \n",
    "    return signals_df\n",
    "\n",
    "# Process cohort signals\n",
    "cohort_signals = process_cohort_signals(fills, cohort_top_users, cohort_bot_users)\n",
    "print(f\"Processed cohort signals: {cohort_signals.shape}\")\n",
    "\n",
    "# Train unified model on 5m data\n",
    "X_train, y_train, feature_columns = prepare_training_data(overlay_features['5m'], cohort_signals)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Target distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Feature columns: {len(feature_columns)}\")\n",
    "\n",
    "# Train base models (Matching Original)\n",
    "base_models = {\n",
    "    'xgb': HistGradientBoostingClassifier(max_iter=100, learning_rate=0.1, max_depth=6, random_state=42),\n",
    "    'hgb': HistGradientBoostingClassifier(max_iter=150, learning_rate=0.05, max_depth=8, random_state=42),\n",
    "    'lasso': LogisticRegression(C=0.1, penalty='l1', solver='liblinear', random_state=42),\n",
    "    'logit': LogisticRegression(C=1.0, penalty='l2', random_state=42)\n",
    "}\n",
    "\n",
    "# Train models\n",
    "for name, model in base_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"Trained {name} model\")\n",
    "\n",
    "# Create meta-model (Matching Original)\n",
    "base_predictions = {}\n",
    "for name, model in base_models.items():\n",
    "    pred = model.predict_proba(X_train)\n",
    "    base_predictions[name] = pred\n",
    "\n",
    "meta_features = np.hstack([base_predictions[name] for name in base_models.keys()])\n",
    "meta_model = LogisticRegression(C=1.0, random_state=42)\n",
    "meta_model.fit(meta_features, y_train)\n",
    "\n",
    "print(f\"Model training complete: {len(feature_columns)} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Overlay Signals (NEW - Multi-Timeframe Signal Generation)\n",
    "print(\"Creating overlay signals...\")\n",
    "\n",
    "def create_overlay_signals(unified_features, cohort_signals, base_models, meta_model, feature_columns):\n",
    "    \"\"\"Create multi-arm signals for overlay timeframes (Matching Original Structure)\"\"\"\n",
    "    \n",
    "    signals_data = []\n",
    "    \n",
    "    for timeframe, features_df in unified_features.items():\n",
    "        print(f\"Creating signals for {timeframe}...\")\n",
    "        \n",
    "        # Merge with cohort signals\n",
    "        merged_df = features_df.merge(cohort_signals, on='timestamp', how='left')\n",
    "        merged_df['S_top'] = merged_df['S_top'].fillna(0.0)\n",
    "        merged_df['S_bot'] = merged_df['S_bot'].fillna(0.0)\n",
    "        \n",
    "        # Prepare features\n",
    "        X = merged_df[feature_columns].fillna(0.0)\n",
    "        \n",
    "        # Get model predictions (Matching Original)\n",
    "        base_preds = {}\n",
    "        for name, model in base_models.items():\n",
    "            pred = model.predict_proba(X)\n",
    "            base_preds[name] = pred\n",
    "        \n",
    "        meta_features = np.hstack([base_preds[name] for name in base_models.keys()])\n",
    "        meta_pred = meta_model.predict_proba(meta_features)\n",
    "        \n",
    "        # Extract model signal (Matching Original)\n",
    "        p_up = meta_pred[:, 2]  # up probability\n",
    "        p_down = meta_pred[:, 0]  # down probability\n",
    "        s_model = p_up - p_down\n",
    "        \n",
    "        # Create signals for this timeframe (Matching Original Structure)\n",
    "        timeframe_signals = pd.DataFrame({\n",
    "            'timestamp': merged_df['timestamp'],\n",
    "            'timeframe': timeframe,\n",
    "            'S_top': merged_df['S_top'],\n",
    "            'S_bot': merged_df['S_bot'],\n",
    "            'S_mood': merged_df['S_top'] + merged_df['S_bot'],  # Combined mood signal\n",
    "            'S_model': s_model,\n",
    "            'p_up': p_up,\n",
    "            'p_down': p_down,\n",
    "            'confidence': np.maximum(p_up, p_down),\n",
    "            'alpha': np.abs(s_model)\n",
    "        })\n",
    "        \n",
    "        signals_data.append(timeframe_signals)\n",
    "    \n",
    "    return pd.concat(signals_data, ignore_index=True)\n",
    "\n",
    "# Create overlay signals\n",
    "overlay_signals = create_overlay_signals(overlay_features, cohort_signals, base_models, meta_model, feature_columns)\n",
    "print(f\"Created overlay signals: {overlay_signals.shape}\")\n",
    "\n",
    "# Create Eligibility Matrix (Matching Original)\n",
    "print(\"Creating eligibility matrix...\")\n",
    "\n",
    "def create_eligibility_matrix(signals_df, thresholds):\n",
    "    \"\"\"Create eligibility matrix based on signal thresholds (Matching Original)\"\"\"\n",
    "    \n",
    "    eligible_data = []\n",
    "    \n",
    "    for timeframe in signals_df['timeframe'].unique():\n",
    "        tf_signals = signals_df[signals_df['timeframe'] == timeframe].copy()\n",
    "        \n",
    "        eligible_df = pd.DataFrame(False, index=tf_signals.index, columns=['pros', 'amateurs', 'mood', 'model'])\n",
    "        \n",
    "        # Eligibility rules (Matching Original)\n",
    "        eligible_df['pros'] = tf_signals['S_top'].abs() >= thresholds['S_MIN']\n",
    "        eligible_df['amateurs'] = tf_signals['S_bot'].abs() >= thresholds['S_MIN']\n",
    "        eligible_df['mood'] = tf_signals['S_mood'].abs() >= thresholds['M_MIN']\n",
    "        eligible_df['model'] = (tf_signals['confidence'] >= thresholds['CONF_MIN']) & (tf_signals['alpha'] >= thresholds['ALPHA_MIN'])\n",
    "        \n",
    "        eligible_df['timeframe'] = timeframe\n",
    "        eligible_df['timestamp'] = tf_signals['timestamp']\n",
    "        \n",
    "        eligible_data.append(eligible_df)\n",
    "    \n",
    "    return pd.concat(eligible_data, ignore_index=True)\n",
    "\n",
    "# Create eligibility matrix\n",
    "thresholds = {\n",
    "    'S_MIN': S_MIN,\n",
    "    'M_MIN': M_MIN,\n",
    "    'CONF_MIN': CONF_MIN,\n",
    "    'ALPHA_MIN': ALPHA_MIN\n",
    "}\n",
    "\n",
    "eligibility_matrix = create_eligibility_matrix(overlay_signals, thresholds)\n",
    "print(f\"Created eligibility matrix: {eligibility_matrix.shape}\")\n",
    "\n",
    "# Create arm signals DataFrame (Matching Original Variable Names)\n",
    "# For 5m timeframe (primary)\n",
    "tf_5m_signals = overlay_signals[overlay_signals['timeframe'] == '5m'].copy()\n",
    "tf_5m_eligible = eligibility_matrix[eligibility_matrix['timeframe'] == '5m'].copy()\n",
    "\n",
    "# Align with backtest data\n",
    "tf_5m_signals = tf_5m_signals.set_index('timestamp')\n",
    "tf_5m_eligible = tf_5m_eligible.set_index('timestamp')\n",
    "\n",
    "# Create arm signals matrix (Matching Original)\n",
    "arm_signals_df = pd.DataFrame({\n",
    "    'S_top': tf_5m_signals['S_top'],\n",
    "    'S_bot': tf_5m_signals['S_bot'],\n",
    "    'S_mood': tf_5m_signals['S_mood'],\n",
    "    'S_model': tf_5m_signals['S_model'],\n",
    "}, index=tf_5m_signals.index)\n",
    "\n",
    "arm_eligible_df = pd.DataFrame(False, index=tf_5m_signals.index, columns=['pros','amateurs','mood','model'])\n",
    "arm_eligible_df['pros'] = tf_5m_eligible['pros']\n",
    "arm_eligible_df['amateurs'] = tf_5m_eligible['amateurs']\n",
    "arm_eligible_df['mood'] = tf_5m_eligible['mood']\n",
    "arm_eligible_df['model'] = tf_5m_eligible['model']\n",
    "\n",
    "# Export to ndarray for backtest (Matching Original)\n",
    "arm_signals = arm_signals_df[['S_top','S_bot','S_mood','S_model']].values\n",
    "arm_eligible = arm_eligible_df[['pros','amateurs','mood','model']].values\n",
    "arm_names = ['pros','amateurs','mood','model']\n",
    "\n",
    "print(f\"Arm signals shape: {arm_signals.shape}\")\n",
    "print(f\"Arm eligible shape: {arm_eligible.shape}\")\n",
    "print(f\"Arm names: {arm_names}\")\n",
    "\n",
    "# Create ADV series (Matching Original)\n",
    "adv20_by_ts = pd.Series(25000000.0, index=bt['timestamp'])  # $25M ADV\n",
    "ANNUALIZER = np.sqrt(365*24*12)  # 5-minute bars\n",
    "\n",
    "print(\"Signal generation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Backtesting Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Backtest (Matching Original)\n",
    "print(\"Running backtest...\")\n",
    "\n",
    "# Prepare backtest data (Matching Original)\n",
    "bt_df = bt.set_index('timestamp')\n",
    "side_eps_vec = np.array([S_MIN, S_MIN, M_MIN, ALPHA_MIN], dtype=float)\n",
    "\n",
    "# Run backtest (Matching Original)\n",
    "Eq_df, Tr_df, Bu_df, met = run_allocator_backtest(\n",
    "    bt_df=bt_df,\n",
    "    arm_signals=arm_signals,\n",
    "    arm_eligible=arm_eligible,\n",
    "    adv_series=adv20_by_ts,\n",
    "    cooldown_bars=COOLDOWN,\n",
    "    cost_bps=COST_BP,\n",
    "    impact_k=IMPACT_K,\n",
    "    side_eps_vec=side_eps_vec,\n",
    "    eps=1e-12,\n",
    ")\n",
    "\n",
    "print(\"Backtest Results:\")\n",
    "print(f\"  Final Equity: {met['final_equity']:.4f}\")\n",
    "print(f\"  Number of Trades: {met['n_trades']}\")\n",
    "print(f\"  Sharpe Ratio: {met['sharpe']:.4f}\")\n",
    "print(f\"  Sortino Ratio: {met['sortino']:.4f}\")\n",
    "print(f\"  Max Drawdown: {met['maxDD']:.4f}\")\n",
    "print(f\"  Turnover: {met['turnover']:.4f}\")\n",
    "print(f\"  Hit Rate: {met['hit_rate']:.4f}\")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nEquity DataFrame shape: {Eq_df.shape}\")\n",
    "print(f\"Trades DataFrame shape: {Tr_df.shape}\")\n",
    "print(f\"Bandit Updates DataFrame shape: {Bu_df.shape}\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nSample Equity Data:\")\n",
    "print(Eq_df.head())\n",
    "\n",
    "print(f\"\\nSample Trades Data:\")\n",
    "print(Tr_df.head())\n",
    "\n",
    "print(f\"\\nSample Bandit Updates:\")\n",
    "print(Bu_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Grid Search Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended grid with cooldown and confidence sweep (Matching Original)\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "COOLDOWN_GRID = [1, 3, 5, 10]\n",
    "CONF_MIN_GRID_EXT = [0.6, 0.7, 0.8]\n",
    "FEE_BPS_GRID = [3.0, 5.0, 10.0]\n",
    "SIGMA_TARGET_GRID = [0.10, 0.20, 0.30]\n",
    "\n",
    "GATE_VARIANTS = [\n",
    "    None,\n",
    "    {'name': 'cons2_adv0', 'consensus_min': 2, 'advantage_min': 0.0},\n",
    "    {'name': 'cons3_adv0', 'consensus_min': 3, 'advantage_min': 0.0},\n",
    "    {'name': 'cons2_adv0p02', 'consensus_min': 2, 'advantage_min': 0.02},\n",
    "]\n",
    "\n",
    "print(\"Grid search parameters defined:\")\n",
    "print(f\"  Cooldown grid: {COOLDOWN_GRID}\")\n",
    "print(f\"  Confidence grid: {CONF_MIN_GRID_EXT}\")\n",
    "print(f\"  Fee grid: {FEE_BPS_GRID}\")\n",
    "print(f\"  Sigma target grid: {SIGMA_TARGET_GRID}\")\n",
    "print(f\"  Gate variants: {len(GATE_VARIANTS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Extended Grid Search (Matching Original)\n",
    "def run_extended_grid():\n",
    "    \"\"\"Run extended grid search optimization (Matching Original)\"\"\"\n",
    "    rows = []\n",
    "    stamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    print(f\"Starting extended grid search at {stamp}\")\n",
    "    \n",
    "    # Grid search over all combinations\n",
    "    for cooldown in COOLDOWN_GRID:\n",
    "        for conf_min in CONF_MIN_GRID_EXT:\n",
    "            for fee_bps in FEE_BPS_GRID:\n",
    "                for sigma_target in SIGMA_TARGET_GRID:\n",
    "                    for gate_cfg in GATE_VARIANTS:\n",
    "                        \n",
    "                        # Update global parameters\n",
    "                        global COOLDOWN, CONF_MIN, COST_BP, SIGMA_TARGET\n",
    "                        COOLDOWN = cooldown\n",
    "                        CONF_MIN = conf_min\n",
    "                        COST_BP = fee_bps\n",
    "                        SIGMA_TARGET = sigma_target\n",
    "                        \n",
    "                        # Run backtest with current configuration\n",
    "                        try:\n",
    "                            # Prepare side thresholds\n",
    "                            side_eps_vec = np.array([S_MIN, S_MIN, M_MIN, ALPHA_MIN], dtype=float)\n",
    "                            \n",
    "                            # Run backtest\n",
    "                            Eq, Tr, Bu, metrics = run_allocator_backtest(\n",
    "                                bt_df=bt_df,\n",
    "                                arm_signals=arm_signals,\n",
    "                                arm_eligible=arm_eligible,\n",
    "                                adv_series=adv20_by_ts,\n",
    "                                cooldown_bars=COOLDOWN,\n",
    "                                cost_bps=COST_BP,\n",
    "                                impact_k=IMPACT_K,\n",
    "                                side_eps_vec=side_eps_vec,\n",
    "                                eps=1e-12,\n",
    "                            )\n",
    "                            \n",
    "                            # Create row\n",
    "                            row = {\n",
    "                                'COOLDOWN': cooldown,\n",
    "                                'CONF_MIN': conf_min,\n",
    "                                'FEE_BPS': fee_bps,\n",
    "                                'SIGMA_TARGET': sigma_target,\n",
    "                                'GATE': gate_cfg['name'] if gate_cfg else 'none',\n",
    "                                'final_equity': metrics['final_equity'],\n",
    "                                'n_trades': metrics['n_trades'],\n",
    "                                'sharpe': metrics['sharpe'],\n",
    "                                'sortino': metrics['sortino'],\n",
    "                                'maxDD': metrics['maxDD'],\n",
    "                                'turnover': metrics['turnover'],\n",
    "                                'hit_rate': metrics['hit_rate'],\n",
    "                            }\n",
    "                            \n",
    "                            rows.append(row)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in grid search: {e}\")\n",
    "                            continue\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(rows)\n",
    "    \n",
    "    print(f\"Grid search complete: {len(results_df)} configurations tested\")\n",
    "    \n",
    "    # Find best configuration\n",
    "    if not results_df.empty:\n",
    "        best_idx = results_df['sharpe'].idxmax()\n",
    "        best_config = results_df.loc[best_idx]\n",
    "        \n",
    "        print(f\"\\nBest Configuration:\")\n",
    "        print(f\"  Sharpe: {best_config['sharpe']:.4f}\")\n",
    "        print(f\"  Cooldown: {best_config['COOLDOWN']}\")\n",
    "        print(f\"  Confidence: {best_config['CONF_MIN']}\")\n",
    "        print(f\"  Fee BPS: {best_config['FEE_BPS']}\")\n",
    "        print(f\"  Sigma Target: {best_config['SIGMA_TARGET']}\")\n",
    "        print(f\"  Gate: {best_config['GATE']}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run grid search\n",
    "grid_results = run_extended_grid()\n",
    "\n",
    "# Display top 10 results\n",
    "print(f\"\\nTop 10 Configurations by Sharpe Ratio:\")\n",
    "top_10 = grid_results.nlargest(10, 'sharpe')\n",
    "print(top_10[['COOLDOWN', 'CONF_MIN', 'FEE_BPS', 'SIGMA_TARGET', 'GATE', 'sharpe', 'final_equity', 'n_trades']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Analysis & Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Analysis & Visualization (Matching Original)\n",
    "print(\"Creating visualizations...\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Unified Overlay Trading Results', fontsize=16)\n",
    "\n",
    "# 1. Equity Curve\n",
    "axes[0, 0].plot(Eq_df.index, Eq_df['equity'])\n",
    "axes[0, 0].set_title('Equity Curve')\n",
    "axes[0, 0].set_xlabel('Time')\n",
    "axes[0, 0].set_ylabel('Equity')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# 2. Drawdown\n",
    "drawdown = (Eq_df['equity'] / Eq_df['equity'].cummax() - 1.0) * 100\n",
    "axes[0, 1].fill_between(Eq_df.index, drawdown, 0, alpha=0.3, color='red')\n",
    "axes[0, 1].set_title('Drawdown (%)')\n",
    "axes[0, 1].set_xlabel('Time')\n",
    "axes[0, 1].set_ylabel('Drawdown %')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# 3. Bandit Arm Selection\n",
    "if not Bu_df.empty:\n",
    "    arm_counts = Bu_df['chosen'].value_counts().sort_index()\n",
    "    arm_names = ['pros', 'amateurs', 'mood', 'model']\n",
    "    arm_labels = [arm_names[i] if i < len(arm_names) else f'arm_{i}' for i in arm_counts.index]\n",
    "    \n",
    "    axes[1, 0].bar(arm_labels, arm_counts.values)\n",
    "    axes[1, 0].set_title('Bandit Arm Selection Count')\n",
    "    axes[1, 0].set_xlabel('Arm')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Performance Metrics\n",
    "metrics_data = {\n",
    "    'Metric': ['Sharpe', 'Sortino', 'Max DD', 'Hit Rate'],\n",
    "    'Value': [met['sharpe'], met['sortino'], met['maxDD'], met['hit_rate']]\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "axes[1, 1].bar(metrics_df['Metric'], metrics_df['Value'])\n",
    "axes[1, 1].set_title('Performance Metrics')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional Analysis\n",
    "print(f\"\\nDetailed Performance Analysis:\")\n",
    "print(f\"  Total Return: {(met['final_equity'] - 1.0) * 100:.2f}%\")\n",
    "print(f\"  Annualized Return: {((met['final_equity'] ** (252*24*12/len(Eq_df))) - 1) * 100:.2f}%\")\n",
    "print(f\"  Volatility: {Eq_df['equity'].pct_change().std() * np.sqrt(252*24*12) * 100:.2f}%\")\n",
    "print(f\"  Max Drawdown: {met['maxDD'] * 100:.2f}%\")\n",
    "print(f\"  Calmar Ratio: {met['sharpe'] / met['maxDD'] if met['maxDD'] > 0 else np.nan:.4f}\")\n",
    "\n",
    "# Trade Analysis\n",
    "if not Tr_df.empty:\n",
    "    print(f\"\\nTrade Analysis:\")\n",
    "    print(f\"  Total Trades: {len(Tr_df)}\")\n",
    "    print(f\"  Average Trade Size: {Tr_df['to_pos'].abs().mean():.4f}\")\n",
    "    print(f\"  Trade Frequency: {len(Tr_df) / (len(Eq_df) / (252*24*12)):.2f} trades/day\")\n",
    "\n",
    "# Bandit Analysis\n",
    "if not Bu_df.empty:\n",
    "    print(f\"\\nBandit Analysis:\")\n",
    "    for i, arm_name in enumerate(arm_names):\n",
    "        arm_rewards = Bu_df[Bu_df['chosen'] == i]['reward']\n",
    "        if len(arm_rewards) > 0:\n",
    "            print(f\"  {arm_name}: {len(arm_rewards)} selections, avg reward: {arm_rewards.mean():.6f}\")\n",
    "\n",
    "print(\"Visualization complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export & Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results and Models (Matching Original)\n",
    "print(\"Exporting results and models...\")\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Export trained models\n",
    "timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save base models\n",
    "for name, model in base_models.items():\n",
    "    model_path = f'models/{name}_model_{timestamp}.joblib'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Saved {name} model to {model_path}\")\n",
    "\n",
    "# Save meta model\n",
    "meta_model_path = f'models/meta_model_{timestamp}.joblib'\n",
    "joblib.dump(meta_model, meta_model_path)\n",
    "print(f\"Saved meta model to {meta_model_path}\")\n",
    "\n",
    "# Save feature columns\n",
    "feature_columns_path = f'models/feature_columns_{timestamp}.json'\n",
    "with open(feature_columns_path, 'w') as f:\n",
    "    json.dump(feature_columns, f)\n",
    "print(f\"Saved feature columns to {feature_columns_path}\")\n",
    "\n",
    "# Create model manifest\n",
    "manifest = {\n",
    "    'timestamp': timestamp,\n",
    "    'model_type': 'unified_overlay',\n",
    "    'base_models': list(base_models.keys()),\n",
    "    'feature_columns': feature_columns,\n",
    "    'overlay_timeframes': OVERLAY_TIMEFRAMES,\n",
    "    'rollup_windows': ROLLUP_WINDOWS,\n",
    "    'timeframe_weights': TIMEFRAME_WEIGHTS,\n",
    "    'training_params': {\n",
    "        'S_MIN': S_MIN,\n",
    "        'M_MIN': M_MIN,\n",
    "        'CONF_MIN': CONF_MIN,\n",
    "        'ALPHA_MIN': ALPHA_MIN,\n",
    "        'COOLDOWN': COOLDOWN,\n",
    "        'COST_BP': COST_BP,\n",
    "        'SIGMA_TARGET': SIGMA_TARGET,\n",
    "    },\n",
    "    'performance_metrics': met,\n",
    "    'grid_search_results': grid_results.to_dict('records') if not grid_results.empty else []\n",
    "}\n",
    "\n",
    "manifest_path = f'models/manifest_{timestamp}.json'\n",
    "with open(manifest_path, 'w') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(f\"Saved model manifest to {manifest_path}\")\n",
    "\n",
    "# Export backtest results\n",
    "results_dir = f'results_{timestamp}'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save equity curve\n",
    "Eq_df.to_csv(f'{results_dir}/equity_curve.csv')\n",
    "print(f\"Saved equity curve to {results_dir}/equity_curve.csv\")\n",
    "\n",
    "# Save trades\n",
    "Tr_df.to_csv(f'{results_dir}/trades.csv', index=False)\n",
    "print(f\"Saved trades to {results_dir}/trades.csv\")\n",
    "\n",
    "# Save bandit updates\n",
    "Bu_df.to_csv(f'{results_dir}/bandit_updates.csv', index=False)\n",
    "print(f\"Saved bandit updates to {results_dir}/bandit_updates.csv\")\n",
    "\n",
    "# Save grid search results\n",
    "grid_results.to_csv(f'{results_dir}/grid_search_results.csv', index=False)\n",
    "print(f\"Saved grid search results to {results_dir}/grid_search_results.csv\")\n",
    "\n",
    "# Save overlay signals\n",
    "overlay_signals.to_csv(f'{results_dir}/overlay_signals.csv', index=False)\n",
    "print(f\"Saved overlay signals to {results_dir}/overlay_signals.csv\")\n",
    "\n",
    "# Save eligibility matrix\n",
    "eligibility_matrix.to_csv(f'{results_dir}/eligibility_matrix.csv', index=False)\n",
    "print(f\"Saved eligibility matrix to {results_dir}/eligibility_matrix.csv\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = f\"\"\"\n",
    "# Unified Overlay Trading Results Summary\n",
    "\n",
    "## Model Training\n",
    "- **Timestamp**: {timestamp}\n",
    "- **Model Type**: Unified Overlay (Single model for all timeframes)\n",
    "- **Base Models**: {', '.join(base_models.keys())}\n",
    "- **Feature Columns**: {len(feature_columns)}\n",
    "- **Overlay Timeframes**: {', '.join(OVERLAY_TIMEFRAMES)}\n",
    "\n",
    "## Performance Metrics\n",
    "- **Final Equity**: {met['final_equity']:.4f}\n",
    "- **Sharpe Ratio**: {met['sharpe']:.4f}\n",
    "- **Sortino Ratio**: {met['sortino']:.4f}\n",
    "- **Max Drawdown**: {met['maxDD']:.4f}\n",
    "- **Number of Trades**: {met['n_trades']}\n",
    "- **Hit Rate**: {met['hit_rate']:.4f}\n",
    "\n",
    "## Grid Search Results\n",
    "- **Total Configurations Tested**: {len(grid_results)}\n",
    "- **Best Sharpe Ratio**: {grid_results['sharpe'].max():.4f} if not grid_results.empty else 'N/A'\n",
    "\n",
    "## Files Exported\n",
    "- Models: models/\n",
    "- Results: {results_dir}/\n",
    "- Manifest: {manifest_path}\n",
    "\n",
    "## Next Steps\n",
    "1. Copy models to MetaStackerBandit/live_demo/models/\n",
    "2. Update MetaStackerBandit configuration\n",
    "3. Deploy overlay system\n",
    "4. Monitor performance\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{results_dir}/summary_report.md', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"Saved summary report to {results_dir}/summary_report.md\")\n",
    "\n",
    "print(\"\\nExport complete!\")\n",
    "print(f\"All files saved with timestamp: {timestamp}\")\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"Models directory: models/\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"Unified Overlay Architecture Implementation Complete!\")\n",
    "print(f\"Single model trained on 5m data for all timeframes\")\n",
    "print(f\"Performance: Sharpe={met['sharpe']:.4f}, MaxDD={met['maxDD']:.4f}\")\n",
    "print(f\"Ready for deployment to MetaStackerBandit\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
