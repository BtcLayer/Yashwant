{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012ff529",
   "metadata": {},
   "source": [
    "# ML Trading Pipeline - Classification-Based Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e15673",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5e78dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd, numpy as np, warnings, os, json, joblib\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV, ElasticNetCV, HuberRegressor, LogisticRegression\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score, classification_report, confusion_matrix, log_loss\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.base import clone\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deterministic seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e60e75d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-source threshold & cost config for allocator-only mode\n",
    "# These globals are consumed by consolidated arms and backtest cells.\n",
    "try:\n",
    "    S_MIN\n",
    "except NameError:\n",
    "    S_MIN = 0.12\n",
    "try:\n",
    "    M_MIN\n",
    "except NameError:\n",
    "    M_MIN = 0.12\n",
    "try:\n",
    "    CONF_MIN\n",
    "except NameError:\n",
    "    CONF_MIN = 0.60\n",
    "try:\n",
    "    ALPHA_MIN\n",
    "except NameError:\n",
    "    ALPHA_MIN = 0.10\n",
    "try:\n",
    "    COOLDOWN\n",
    "except NameError:\n",
    "    COOLDOWN = 1\n",
    "try:\n",
    "    COST_BP\n",
    "except NameError:\n",
    "    COST_BP = 5.0\n",
    "try:\n",
    "    IMPACT_K\n",
    "except NameError:\n",
    "    IMPACT_K = 0.0\n",
    "\n",
    "# Risk and execution controls\n",
    "try:\n",
    "    SIGMA_TARGET\n",
    "except NameError:\n",
    "    SIGMA_TARGET = 0.20  # per-bar target scaler proxy\n",
    "try:\n",
    "    POS_MAX\n",
    "except NameError:\n",
    "    POS_MAX = 1.0\n",
    "try:\n",
    "    DD_STOP\n",
    "except NameError:\n",
    "    DD_STOP = 0.05\n",
    "try:\n",
    "    LATENCY_BARS\n",
    "except NameError:\n",
    "    LATENCY_BARS = 0\n",
    "try:\n",
    "    SLIPPAGE_BPS\n",
    "except NameError:\n",
    "    SLIPPAGE_BPS = 0.0\n",
    "try:\n",
    "    COST_CONVENTION\n",
    "except NameError:\n",
    "    COST_CONVENTION = 'per_transition'  # or 'per_roundtrip'\n",
    "try:\n",
    "    SMOOTH_BETA\n",
    "except NameError:\n",
    "    SMOOTH_BETA = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff21a5d",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9567f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal robust merge of OHLCV + funding + cohort-filtered fills (keep all columns)\n",
    "import os\n",
    "\n",
    "# Cohorts\n",
    "cohort_top = pd.read_csv('top_cohort.csv') if os.path.exists('top_cohort.csv') else pd.DataFrame()\n",
    "cohort_bot = pd.read_csv('bottom_cohort.csv') if os.path.exists('bottom_cohort.csv') else pd.DataFrame()\n",
    "for cdf in (cohort_top, cohort_bot):\n",
    "    if not cdf.empty and 'user' not in cdf.columns:\n",
    "        col = next((c for c in ['Account', 'address', 'user', 'addr'] if c in cdf.columns), None)\n",
    "        if col: cdf['user'] = cdf[col].astype(str)\n",
    "cohort_top_users = set(cohort_top.get('user', pd.Series(dtype=str)).dropna().astype(str))\n",
    "cohort_bot_users = set(cohort_bot.get('user', pd.Series(dtype=str)).dropna().astype(str))\n",
    "cohort_addresses = cohort_top_users | cohort_bot_users\n",
    "\n",
    "# Fills -> keep all columns, add normalized helpers\n",
    "fills = pd.read_csv('historical_trades_btc.csv', low_memory=False) if os.path.exists('historical_trades_btc.csv') else pd.DataFrame()\n",
    "if not fills.empty:\n",
    "    if 'user' not in fills.columns:\n",
    "        if 'Account' in fills.columns: fills['user'] = fills['Account'].astype(str)\n",
    "    # Timestamp parsing (multiple possibilities)\n",
    "    ts_col = 'Timestamp' if 'Timestamp' in fills.columns else (next((c for c in ['timestamp', 'ts'] if c in fills.columns), None))\n",
    "    ts_raw = pd.to_numeric(fills[ts_col], errors='coerce') if ts_col else pd.Series(dtype='float64')\n",
    "    if ts_raw.notna().any():\n",
    "        med = ts_raw.dropna().median(); unit = 'ns' if med>1e14 else ('ms' if med>1e12 else 's')\n",
    "        fills['timestamp'] = pd.to_datetime(ts_raw, unit=unit, errors='coerce')\n",
    "    else:\n",
    "        time_col = next((c for c in ['Timestamp IST', 'time'] if c in fills.columns), None)\n",
    "        fills['timestamp'] = pd.to_datetime(fills[time_col], errors='coerce') if time_col else pd.NaT\n",
    "    # Map helpers (retain originals)\n",
    "    if 'Execution Price' in fills.columns and 'px' not in fills.columns:\n",
    "        fills['px'] = pd.to_numeric(fills['Execution Price'], errors='coerce')\n",
    "    if 'Size Tokens' in fills.columns and 'sz' not in fills.columns:\n",
    "        fills['sz'] = pd.to_numeric(fills['Size Tokens'], errors='coerce')\n",
    "    if 'Size USD' in fills.columns and 'notional' not in fills.columns:\n",
    "        fills['notional'] = pd.to_numeric(fills['Size USD'], errors='coerce')\n",
    "    if 'Fee' in fills.columns and 'fee' not in fills.columns:\n",
    "        fills['fee'] = pd.to_numeric(fills['Fee'], errors='coerce')\n",
    "    if 'Side' in fills.columns and 'side' not in fills.columns:\n",
    "        fills['side'] = fills['Side'].map({'BUY':'B','SELL':'A'}).fillna(fills['Side'].astype(str))\n",
    "    # Cohort flags\n",
    "    if 'user' in fills.columns:\n",
    "        u = fills['user'].astype(str)\n",
    "        fills['is_top_cohort'] = u.isin(cohort_top_users).astype(int)\n",
    "        fills['is_bottom_cohort'] = u.isin(cohort_bot_users).astype(int)\n",
    "    # Filter by cohorts if available\n",
    "    if cohort_addresses and 'user' in fills.columns:\n",
    "        fills = fills[fills['user'].astype(str).isin(cohort_addresses)]\n",
    "    fills_aligned = fills.dropna(subset=['timestamp']).sort_values('timestamp')\n",
    "else:\n",
    "    fills_aligned = pd.DataFrame(columns=['timestamp'])\n",
    "\n",
    "# Funding (keep premium if present)\n",
    "funding = pd.read_csv('funding_btc.csv', low_memory=False) if os.path.exists('funding_btc.csv') else pd.DataFrame()\n",
    "if not funding.empty:\n",
    "    if 'timestamp' not in funding.columns:\n",
    "        cand = next((c for c in ['ts','Timestamp'] if c in funding.columns), None)\n",
    "        if cand: funding = funding.rename(columns={cand:'timestamp'})\n",
    "    if 'funding_rate' not in funding.columns:\n",
    "        cand = next((c for c in ['fundingRate','rate','funding'] if c in funding.columns), None)\n",
    "        if cand: funding = funding.rename(columns={cand:'funding_rate'})\n",
    "    ts_raw = pd.to_numeric(funding['timestamp'], errors='coerce')\n",
    "    med = ts_raw.dropna().median() if ts_raw.notna().any() else 0\n",
    "    unit = 'ns' if med>1e14 else ('ms' if med>1e12 else 's')\n",
    "    funding['timestamp'] = pd.to_datetime(ts_raw, unit=unit, errors='coerce')\n",
    "    if 'funding_rate' in funding.columns:\n",
    "        funding['funding_rate'] = pd.to_numeric(funding['funding_rate'], errors='coerce')\n",
    "    if 'premium' in funding.columns:\n",
    "        funding['premium'] = pd.to_numeric(funding['premium'], errors='coerce')\n",
    "    funding = funding.dropna(subset=['timestamp']).drop_duplicates('timestamp').sort_values('timestamp')\n",
    "\n",
    "# OHLCV (headerless fallback)\n",
    "ohlcv_path = 'ohlc_btc_5m.csv'\n",
    "ohlcv = pd.read_csv(ohlcv_path, low_memory=False, header=0) if os.path.exists(ohlcv_path) else pd.DataFrame()\n",
    "if 'timestamp' not in ohlcv.columns or 'open' not in ohlcv.columns:\n",
    "    ohlcv_raw = pd.read_csv(ohlcv_path, header=None)\n",
    "    ohlcv = ohlcv_raw[[0,2,3,4,5,6]].copy(); ohlcv.columns=['timestamp','open','high','low','close','volume']\n",
    "ts_raw = pd.to_numeric(ohlcv['timestamp'], errors='coerce')\n",
    "med = ts_raw.dropna().median(); unit = 'ns' if med>1e14 else ('ms' if med>1e12 else 's')\n",
    "ohlcv['timestamp'] = pd.to_datetime(ts_raw, unit=unit, errors='coerce')\n",
    "for c in ['open','high','low','close','volume']: ohlcv[c]=pd.to_numeric(ohlcv[c], errors='coerce')\n",
    "ohlcv = ohlcv.dropna(subset=['timestamp','open','high','low','close','volume']).drop_duplicates('timestamp').sort_values('timestamp')\n",
    "\n",
    "# Merge funding columns into OHLCV\n",
    "fund_cols = ['funding_rate','premium']\n",
    "avail_fund_cols = [c for c in fund_cols if c in funding.columns]\n",
    "if avail_fund_cols:\n",
    "    df = pd.merge_asof(ohlcv, funding[['timestamp']+avail_fund_cols].sort_values('timestamp'), on='timestamp', direction='backward')\n",
    "    for c in avail_fund_cols:\n",
    "        df[c] = df[c].fillna(method='ffill').fillna(0.0)\n",
    "else:\n",
    "    df = ohlcv.copy()\n",
    "    for c in fund_cols: df[c]=0.0\n",
    "\n",
    "# Aligned frames for features\n",
    "ohlcv_aligned = df[['timestamp','open','high','low','close','volume']].copy()\n",
    "funding_aligned = df[['timestamp']+avail_fund_cols] if avail_fund_cols else df[['timestamp']+fund_cols]\n",
    "\n",
    "# Optional wide merge with last known fill row (keeps all fill columns)\n",
    "if not fills_aligned.empty:\n",
    "    df_merged = pd.merge_asof(df.sort_values('timestamp'), fills_aligned.sort_values('timestamp'), on='timestamp', direction='backward')\n",
    "else:\n",
    "    df_merged = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d6ea395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic daily cohorts (Top/Bottom 5%) with reliability and cohort signals (robust to column variations)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Resolve bars_df and bar timestamps\n",
    "bars_df = bt if 'bt' in globals() else (ohlcv if 'ohlcv' in globals() else price_bars if 'price_bars' in globals() else df)\n",
    "if 'timestamp' in bars_df.columns:\n",
    "    bar_ts_series = pd.to_datetime(bars_df['timestamp'], errors='coerce')\n",
    "elif hasattr(bars_df.index, 'dtype') and str(bars_df.index.dtype).startswith('datetime64'):\n",
    "    bar_ts_series = pd.to_datetime(bars_df.index)\n",
    "elif 'df' in globals() and isinstance(df, pd.DataFrame) and 'timestamp' in df.columns:\n",
    "    bar_ts_series = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "else:\n",
    "    bar_ts_series = pd.Series(pd.NaT, index=range(len(bars_df)))\n",
    "\n",
    "# Price series and returns aligned to bar_ts_series\n",
    "if 'close' in bars_df.columns and len(bars_df) == len(bar_ts_series):\n",
    "    price_series = pd.Series(bars_df['close'].values, index=bar_ts_series)\n",
    "else:\n",
    "    # fallback to df\n",
    "    if 'df' in globals() and 'close' in df.columns and 'timestamp' in df.columns:\n",
    "        price_series = pd.Series(df['close'].values, index=pd.to_datetime(df['timestamp'], errors='coerce'))\n",
    "    else:\n",
    "        # minimal fallback\n",
    "        price_series = pd.Series(bars_df.iloc[:, 0].values, index=bar_ts_series)\n",
    "\n",
    "price_series = price_series.dropna()\n",
    "bar_ts_series = price_series.index  # ensure consistency\n",
    "returns_5m = price_series.pct_change()\n",
    "r_next = returns_5m.shift(-1)\n",
    "\n",
    "# Build a normalized fills frame with canonical columns: ts, address, sign, notional\n",
    "fraw = fills.copy() if 'fills' in globals() else pd.DataFrame()\n",
    "if isinstance(fraw, pd.DataFrame) and not fraw.empty:\n",
    "    fr = fraw.copy()\n",
    "    # Timestamp -> ts (datetime)\n",
    "    if 'ts' in fr.columns:\n",
    "        ts = pd.to_datetime(fr['ts'], errors='coerce')\n",
    "    else:\n",
    "        cand_ts = next((c for c in ['timestamp','Timestamp','time','date'] if c in fr.columns), None)\n",
    "        if cand_ts is not None:\n",
    "            ts_raw = fr[cand_ts]\n",
    "            if np.issubdtype(getattr(ts_raw, 'dtype', object), np.number):\n",
    "                med = pd.to_numeric(ts_raw, errors='coerce').dropna().median()\n",
    "                unit = 'ns' if (pd.notna(med) and med>1e14) else ('ms' if (pd.notna(med) and med>1e12) else 's')\n",
    "                ts = pd.to_datetime(pd.to_numeric(ts_raw, errors='coerce'), unit=unit, errors='coerce')\n",
    "            else:\n",
    "                ts = pd.to_datetime(ts_raw, errors='coerce')\n",
    "        else:\n",
    "            ts = pd.to_datetime(pd.NaT)\n",
    "    # Address/user id -> address\n",
    "    if 'address' in fr.columns:\n",
    "        addr = fr['address'].astype(str)\n",
    "    else:\n",
    "        cand_addr = next((c for c in ['user','User','Account','addr'] if c in fr.columns), None)\n",
    "        addr = fr[cand_addr].astype(str) if cand_addr is not None else pd.Series([], dtype=str)\n",
    "    # Trade direction -> sign (+1 buy, -1 sell)\n",
    "    if 'sign' in fr.columns:\n",
    "        sgn = pd.to_numeric(fr['sign'], errors='coerce')\n",
    "    else:\n",
    "        if 'side' in fr.columns:\n",
    "            sgn = fr['side'].map({'B':1,'A':-1,'BUY':1,'SELL':-1}).fillna(0).astype(float)\n",
    "        elif 'Side' in fr.columns:\n",
    "            sgn = fr['Side'].map({'BUY':1,'SELL':-1}).fillna(0).astype(float)\n",
    "        else:\n",
    "            sgn = pd.Series(0.0, index=fr.index)\n",
    "    # Notional -> notional (fallback px*sz)\n",
    "    if 'notional' in fr.columns:\n",
    "        notional = pd.to_numeric(fr['notional'], errors='coerce')\n",
    "    else:\n",
    "        if {'px','sz'}.issubset(fr.columns):\n",
    "            notional = pd.to_numeric(fr['px'], errors='coerce') * pd.to_numeric(fr['sz'], errors='coerce')\n",
    "        elif 'Size USD' in fr.columns:\n",
    "            notional = pd.to_numeric(fr['Size USD'], errors='coerce')\n",
    "        else:\n",
    "            notional = pd.Series(0.0, index=fr.index)\n",
    "    f = pd.DataFrame({'ts': ts, 'address': addr, 'sign': sgn, 'notional': notional})\n",
    "    f = f.dropna(subset=['ts'])\n",
    "    # Discard zero/not-a-number rows\n",
    "    f['sign'] = pd.to_numeric(f['sign'], errors='coerce').fillna(0.0)\n",
    "    f['notional'] = pd.to_numeric(f['notional'], errors='coerce').fillna(0.0)\n",
    "    f = f[(f['notional']>0) & (f['sign']!=0)]\n",
    "else:\n",
    "    f = pd.DataFrame(columns=['ts','address','sign','notional'])\n",
    "\n",
    "if not f.empty:\n",
    "    f = f.sort_values('ts')\n",
    "    # Map each fill to its bar and next-bar return using datetime keys\n",
    "    base = pd.DataFrame({'bar_ts': pd.to_datetime(bar_ts_series, errors='coerce')}).dropna().sort_values('bar_ts').drop_duplicates('bar_ts')\n",
    "    f['ts'] = pd.to_datetime(f['ts'], errors='coerce')\n",
    "    f = f.dropna(subset=['ts'])\n",
    "    f = pd.merge_asof(f.sort_values('ts'), base, left_on='ts', right_on='bar_ts', direction='backward', allow_exact_matches=True)\n",
    "    f['date'] = f['ts'].dt.floor('D')\n",
    "    # r_next keyed by bar_ts index\n",
    "    r_next_by_ts = pd.Series(r_next.values, index=price_series.index)\n",
    "    f['r_next'] = f['bar_ts'].map(r_next_by_ts)\n",
    "    f['pnl_proxy'] = f['sign'].astype(float) * f['notional'].astype(float) * f['r_next'].fillna(0.0)\n",
    "    # Daily user metrics\n",
    "    daily_user = f.groupby(['address','date'], as_index=False).agg(\n",
    "        notional_sum=('notional','sum'),\n",
    "        trades_count=('notional','size'),\n",
    "        pnl_sum=('pnl_proxy','sum')\n",
    "    )\n",
    "    daily_user['pnl_bps'] = 1e4 * daily_user['pnl_sum'] / daily_user['notional_sum'].replace(0.0, np.nan)\n",
    "    daily_user['pnl_bps'] = daily_user['pnl_bps'].fillna(0.0)\n",
    "    daily_user = daily_user.sort_values(['address','date'])\n",
    "    # Rolling Sharpe (30d) and activity gates (60d)\n",
    "    def _rolling_features(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = g.sort_values('date').copy()\n",
    "        g['active'] = (g['trades_count'] > 0).astype(int)\n",
    "        g['sh_30d'] = g['pnl_bps'].rolling(window=30, min_periods=10).mean() / (g['pnl_bps'].rolling(30, min_periods=10).std().replace(0.0,np.nan))\n",
    "        g['sh_30d'] = g['sh_30d'].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        g['tr_60d'] = g['trades_count'].rolling(60, min_periods=10).sum()\n",
    "        g['ad_60d'] = g['active'].rolling(60, min_periods=10).sum()\n",
    "        # recency: days since last trade\n",
    "        last_trade = g['date'].where(g['trades_count']>0)\n",
    "        last_trade = last_trade.ffill()\n",
    "        g['days_since'] = (g['date'] - last_trade).dt.days.fillna(999)\n",
    "        return g\n",
    "    daily_user = daily_user.groupby('address', group_keys=False).apply(_rolling_features)\n",
    "    # Daily cohort selection\n",
    "    def _select_day(df_day: pd.DataFrame) -> pd.DataFrame:\n",
    "        eligible = df_day[(df_day['tr_60d']>=200) & (df_day['ad_60d']>=30)]\n",
    "        if eligible.empty:\n",
    "            return pd.DataFrame(columns=['date','address','cohort','rho'])\n",
    "        q_top = eligible['sh_30d'].quantile(0.95)\n",
    "        q_bot = eligible['sh_30d'].quantile(0.05)\n",
    "        top = eligible[eligible['sh_30d']>=q_top].copy()\n",
    "        bot = eligible[eligible['sh_30d']<=q_bot].copy()\n",
    "        if top.empty and bot.empty:\n",
    "            return pd.DataFrame(columns=['date','address','cohort','rho'])\n",
    "        tau = 10.0\n",
    "        lam = np.exp(-1.0/tau)\n",
    "        rows = []\n",
    "        if not top.empty:\n",
    "            w_rec = np.power(lam, top['days_since'].clip(lower=0))\n",
    "            w_stab = top['sh_30d'].clip(lower=0.0, upper=2.0)\n",
    "            rho = (w_rec * w_stab).clip(lower=0.0, upper=2.0)\n",
    "            rows.append(pd.DataFrame({'date': top['date'], 'address': top['address'], 'cohort':'top', 'rho': rho}))\n",
    "        if not bot.empty:\n",
    "            w_rec = np.power(lam, bot['days_since'].clip(lower=0))\n",
    "            w_stab = (-bot['sh_30d']).clip(lower=0.0, upper=2.0)\n",
    "            rho = (w_rec * w_stab).clip(lower=0.0, upper=2.0)\n",
    "            rows.append(pd.DataFrame({'date': bot['date'], 'address': bot['address'], 'cohort':'bot', 'rho': rho}))\n",
    "        return pd.concat(rows, axis=0, ignore_index=True) if rows else pd.DataFrame(columns=['date','address','cohort','rho'])\n",
    "    cohorts = daily_user.groupby('date', group_keys=False).apply(_select_day)\n",
    "    # Map cohorts to fills per day and aggregate flows to 5m\n",
    "    if not cohorts.empty:\n",
    "        fills_day = f[['ts','bar_ts','date','address','sign','notional']].copy()\n",
    "        fills_day = fills_day.merge(cohorts, on=['date','address'], how='inner')\n",
    "        # Normalize rho within day×cohort\n",
    "        denom = fills_day.groupby(['date','cohort'])['rho'].transform('sum').replace(0.0, np.nan)\n",
    "        fills_day['alpha'] = (fills_day['rho'] / denom).fillna(0.0)\n",
    "        fills_day['contrib'] = fills_day['alpha'] * fills_day['sign'].astype(float) * fills_day['notional'].astype(float)\n",
    "        top_flow = fills_day.loc[fills_day['cohort']=='top'].groupby('bar_ts')['contrib'].sum()\n",
    "        bot_flow = fills_day.loc[fills_day['cohort']=='bot'].groupby('bar_ts')['contrib'].sum()\n",
    "        F_top_series = top_flow.reindex(bar_ts_series).fillna(0.0)\n",
    "        F_bot_series = bot_flow.reindex(bar_ts_series).fillna(0.0)\n",
    "        # ADV20 for normalization keyed by bar_ts\n",
    "        def compute_adv20_ts(bars_df_local: pd.DataFrame, bar_ts_local: pd.Series) -> pd.Series:\n",
    "            if {'close','volume'}.issubset(bars_df_local.columns) and len(bars_df_local) == len(bar_ts_local):\n",
    "                adv = (bars_df_local['close'] * bars_df_local['volume']).rolling(20, min_periods=1).mean()\n",
    "                return pd.Series(adv.values, index=bar_ts_local)\n",
    "            if 'df' in globals() and {'close','volume','timestamp'}.issubset(df.columns):\n",
    "                adv = (df['close'] * df['volume']).rolling(20, min_periods=1).mean()\n",
    "                return pd.Series(adv.values, index=pd.to_datetime(df['timestamp'], errors='coerce'))\n",
    "            return pd.Series(1.0, index=bar_ts_local)\n",
    "        adv20_by_ts = compute_adv20_ts(bars_df, bar_ts_series)\n",
    "        def cohort_signals_from_flows(F_top_s: pd.Series, F_bot_s: pd.Series, adv_s: pd.Series, clip_k: float=3.0, decay: float=0.98):\n",
    "            idx = F_top_s.index\n",
    "            adv_s = adv_s.reindex(idx).replace(0.0, np.nan).fillna(method='ffill').fillna(1.0)\n",
    "            nt = (F_top_s / adv_s).fillna(0.0)\n",
    "            nb = (F_bot_s / adv_s).fillna(0.0)\n",
    "            S_top_s = np.tanh(nt.clip(-clip_k, clip_k))\n",
    "            S_bot_s = -np.tanh(nb.clip(-clip_k, clip_k))\n",
    "            S_top_s = S_top_s.ewm(alpha=(1-decay), adjust=False).mean()\n",
    "            S_bot_s = S_bot_s.ewm(alpha=(1-decay), adjust=False).mean()\n",
    "            return S_top_s, S_bot_s\n",
    "        S_top, S_bot = cohort_signals_from_flows(F_top_series, F_bot_series, adv20_by_ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57306494",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "### 3.1 Price-Based Features (A Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "699d0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort flow features (compact): use S_top, S_bot computed from dynamic cohorts (Cell 7) to avoid duplication\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "flow_features = df[['timestamp']].copy()\n",
    "\n",
    "# If dynamic cohort signals exist, reuse them; else fall back to zeroes\n",
    "if 'S_top' in globals() and 'S_bot' in globals():\n",
    "    # Align to df timestamps\n",
    "    ts_idx = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    S_top_aligned = S_top.reindex(ts_idx).values if isinstance(S_top, pd.Series) else np.zeros(len(ts_idx))\n",
    "    S_bot_aligned = S_bot.reindex(ts_idx).values if isinstance(S_bot, pd.Series) else np.zeros(len(ts_idx))\n",
    "    flow_features['S_top'] = S_top_aligned\n",
    "    flow_features['S_bot'] = S_bot_aligned\n",
    "    flow_features['flow_diff'] = flow_features['S_top'] - (-flow_features['S_bot'])\n",
    "else:\n",
    "    # No cohort signals available; keep neutral\n",
    "    flow_features['S_top'] = 0.0\n",
    "    flow_features['S_bot'] = 0.0\n",
    "    flow_features['flow_diff'] = 0.0\n",
    "\n",
    "flow_features = flow_features[['timestamp','S_top','S_bot','flow_diff']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02c7a8",
   "metadata": {},
   "source": [
    "### 3.2 Microstructure Features (B Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e01ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "microstructure_features = df[['timestamp']].copy()\n",
    "if 'volume' in df.columns:\n",
    "    microstructure_features['volume_roc_1'] = df['volume'].pct_change()\n",
    "    volume_ma_20 = df['volume'].rolling(20, min_periods=10).mean()\n",
    "    microstructure_features['volume_intensity'] = df['volume'] / (volume_ma_20 + 1e-8)\n",
    "    volume_std_20 = df['volume'].rolling(20, min_periods=10).std()\n",
    "    microstructure_features['volume_spike'] = ((df['volume'] - volume_ma_20) / (volume_std_20 + 1e-8)).clip(-3, 3)\n",
    "else:\n",
    "    for c in ['volume_roc_1','volume_intensity','volume_spike']:\n",
    "        microstructure_features[c]=np.nan\n",
    "if {'high','low','close'}.issubset(df.columns):\n",
    "    microstructure_features['price_efficiency'] = ((df['close'] - df['low']) / (df['high'] - df['low'] + 1e-8)).replace([np.inf,-np.inf], np.nan)\n",
    "    price_range = df['high'] - df['low']\n",
    "    microstructure_features['price_impact_proxy'] = (price_range / (df['volume'] + 1e-8)).rolling(5, min_periods=3).mean()\n",
    "    returns_volatility = df['close'].pct_change().rolling(20, min_periods=10).std()\n",
    "    microstructure_features['vol_adj_volume'] = df['volume'] / (returns_volatility + 1e-8)\n",
    "else:\n",
    "    microstructure_features['price_efficiency']=np.nan\n",
    "    microstructure_features['price_impact_proxy']=np.nan\n",
    "    microstructure_features['vol_adj_volume']=np.nan\n",
    "if {'volume','close'}.issubset(df.columns):\n",
    "    price_change = df['close'].pct_change()\n",
    "    microstructure_features['vwap_momentum'] = ((price_change * df['volume']).rolling(5, min_periods=3).sum() / (df['volume'].rolling(5, min_periods=3).sum() + 1e-8))\n",
    "    microstructure_features['volume_pressure'] = np.tanh(price_change * np.log1p(df['volume']))\n",
    "else:\n",
    "    microstructure_features['vwap_momentum']=np.nan\n",
    "    microstructure_features['volume_pressure']=np.nan\n",
    "if {'volume','high','low','close'}.issubset(df.columns):\n",
    "    microstructure_features['turnover_proxy'] = np.log1p(df['volume'])\n",
    "    microstructure_features['price_volume_corr'] = df['close'].rolling(10, min_periods=5).corr(df['volume'])\n",
    "    price_volatility = (df['high'] - df['low']) / df['close']\n",
    "    microstructure_features['depth_proxy'] = np.log1p(df['volume']) / (price_volatility + 1e-8)\n",
    "else:\n",
    "    microstructure_features['turnover_proxy']=np.nan\n",
    "    microstructure_features['price_volume_corr']=np.nan\n",
    "    microstructure_features['depth_proxy']=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ff183",
   "metadata": {},
   "source": [
    "### 3.3 Price Action Features (C Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d69ff183",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_bars = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].copy()\n",
    "\n",
    "technical_cols = ['hl_range', 'oc_range', 'typical_price', 'weighted_price', 'true_range',\n",
    "                 'body_size', 'upper_shadow', 'lower_shadow', 'direction', 'price_change', \n",
    "                 'price_change_pct', 'range_pct']\n",
    "\n",
    "for col in technical_cols:\n",
    "    if col in df.columns:\n",
    "        price_bars[col] = df[col]\n",
    "\n",
    "price_bars['price'] = price_bars['close']\n",
    "\n",
    "price_bars = price_bars.dropna(subset=['price'])\n",
    "\n",
    "if len(price_bars) > 0:\n",
    "    price_bars['returns'] = price_bars['price'].pct_change()\n",
    "    \n",
    "    if 'true_range' in price_bars.columns:\n",
    "        price_bars['atr_14'] = price_bars['true_range'].rolling(window=14, min_periods=7).mean()\n",
    "        price_bars['vol_50'] = price_bars['atr_14'] / price_bars['price']\n",
    "    else:\n",
    "        price_bars['vol_50'] = price_bars['returns'].rolling(window=50, min_periods=10).std()\n",
    "    \n",
    "    price_bars['vol_200'] = price_bars['returns'].rolling(window=200, min_periods=20).std()\n",
    "    \n",
    "    for h in [1, 3, 6]:\n",
    "        if 'price_change_pct' in price_bars.columns and h == 1:\n",
    "            price_bars[f'mom_{h}'] = price_bars['price_change_pct']\n",
    "        else:\n",
    "            price_bars[f'mom_{h}'] = price_bars['price'].pct_change(periods=h)\n",
    "    \n",
    "    price_bars['ema20'] = price_bars['price'].ewm(span=20, adjust=False).mean()\n",
    "    price_bars['mr_ema20'] = (price_bars['price'] - price_bars['ema20']) / price_bars['ema20']\n",
    "    \n",
    "    rolling_mean = price_bars['price'].rolling(window=100, min_periods=20).mean()\n",
    "    rolling_std = price_bars['price'].rolling(window=100, min_periods=20).std()\n",
    "    price_bars['mr_ema20_z'] = (price_bars['price'] - rolling_mean) / (rolling_std + 1e-8)\n",
    "    \n",
    "    if 'true_range' in price_bars.columns:\n",
    "        price_bars['rv_1h'] = price_bars['true_range'].rolling(window=12, min_periods=6).sum() / price_bars['price']\n",
    "        price_bars['rv_15m'] = price_bars['true_range'].rolling(window=3, min_periods=2).sum() / price_bars['price']\n",
    "        price_bars['rv_1d'] = price_bars['true_range'].rolling(window=288, min_periods=50).sum() / price_bars['price']\n",
    "    else:\n",
    "        price_bars['rv_1h'] = price_bars['returns'].rolling(window=12, min_periods=6).apply(lambda x: (x**2).sum())\n",
    "        price_bars['rv_15m'] = price_bars['returns'].rolling(window=3, min_periods=2).apply(lambda x: (x**2).sum())\n",
    "        price_bars['rv_1d'] = price_bars['returns'].rolling(window=288, min_periods=50).apply(lambda x: (x**2).sum())\n",
    "    \n",
    "    rv_threshold = price_bars['rv_1h'].rolling(window=100, min_periods=25).quantile(0.75)\n",
    "    price_bars['regime_high_vol'] = (price_bars['rv_1h'] > rv_threshold).astype(int)\n",
    "    \n",
    "    price_bars['price_velocity'] = price_bars['price'].diff().ewm(span=5, adjust=False).mean()\n",
    "    price_bars['price_acceleration'] = price_bars['price_velocity'].diff()\n",
    "    \n",
    "    price_bars['price_normalized'] = (price_bars['price'] - price_bars['price'].rolling(window=1000, min_periods=100).mean()) / (price_bars['price'].rolling(window=1000, min_periods=100).std() + 1e-8)\n",
    "    price_bars['price_velocity_norm'] = np.tanh(price_bars['price_velocity'] / (price_bars['price'].rolling(window=100).std() + 1e-8))\n",
    "    price_bars['price_acceleration_norm'] = np.tanh(price_bars['price_acceleration'] / (price_bars['price_velocity'].rolling(window=100).std() + 1e-8))\n",
    "    \n",
    "    price_action_cols = [\n",
    "        'timestamp', 'mom_1', 'mom_3', 'mr_ema20_z', 'rv_1h', 'regime_high_vol'\n",
    "    ]\n",
    "    \n",
    "    available_cols = [col for col in price_action_cols if col in price_bars.columns]\n",
    "    price_action_features = price_bars[available_cols].copy()\n",
    "    \n",
    "else:\n",
    "    price_action_features = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380a4d1",
   "metadata": {},
   "source": [
    "### 3.4 Volatility Features (D Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c380a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility features from price_action_features if available, else compute minimal fallback\n",
    "if 'price_action_features' in locals() and len(price_action_features) > 0:\n",
    "    volatility_features = price_action_features[['timestamp']].copy()\n",
    "    if 'rv_1h' in price_action_features.columns:\n",
    "        volatility_features['rv_1h'] = price_action_features['rv_1h']\n",
    "    if 'regime_high_vol' in price_action_features.columns:\n",
    "        volatility_features['regime_high_vol'] = price_action_features['regime_high_vol']\n",
    "else:\n",
    "    volatility_features = df[['timestamp']].copy()\n",
    "    returns = df['close'].pct_change()\n",
    "    volatility_features['rv_1h'] = returns.rolling(12, min_periods=6).apply(lambda x: (x**2).sum())\n",
    "    vol_threshold = volatility_features['rv_1h'].rolling(100, min_periods=25).quantile(0.75)\n",
    "    volatility_features['regime_high_vol'] = (volatility_features['rv_1h'] > vol_threshold).astype('float')\n",
    "# no ffill/bfill here; NaNs remain for early bars and will be dropped later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "715bd962",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_vol_features = df[['timestamp']].copy()\n",
    "if {'high','low','close','open'}.issubset(df.columns):\n",
    "    gk_vol = np.log(df['high']/df['low'])**2 / 2 - (2*np.log(2)-1) * np.log(df['close']/df['open'])**2\n",
    "    enhanced_vol_features['gk_volatility'] = gk_vol.rolling(12, min_periods=6).mean()\n",
    "    parkinson_vol = np.log(df['high']/df['low'])**2 / (4 * np.log(2))\n",
    "    enhanced_vol_features['parkinson_volatility'] = parkinson_vol.rolling(12, min_periods=6).mean()\n",
    "    vol_percentile_20 = enhanced_vol_features['gk_volatility'].rolling(100, min_periods=50).quantile(0.2)\n",
    "    vol_percentile_80 = enhanced_vol_features['gk_volatility'].rolling(100, min_periods=50).quantile(0.8)\n",
    "    enhanced_vol_features['vol_regime_low'] = (enhanced_vol_features['gk_volatility'] <= vol_percentile_20).astype('float')\n",
    "    enhanced_vol_features['vol_regime_high'] = (enhanced_vol_features['gk_volatility'] >= vol_percentile_80).astype('float')\n",
    "else:\n",
    "    for c in ['gk_volatility','parkinson_volatility','vol_regime_low','vol_regime_high']:\n",
    "        enhanced_vol_features[c]=np.nan\n",
    "if 'close' in df.columns:\n",
    "    returns = df['close'].pct_change(); return_std = returns.rolling(50, min_periods=25).std()\n",
    "    enhanced_vol_features['jump_indicator'] = (np.abs(returns) > 3 * return_std).astype('float')\n",
    "    enhanced_vol_features['jump_magnitude'] = np.abs(returns) / (return_std + 1e-8)\n",
    "else:\n",
    "    enhanced_vol_features['jump_indicator']=np.nan\n",
    "    enhanced_vol_features['jump_magnitude']=np.nan\n",
    "if 'gk_volatility' in enhanced_vol_features.columns:\n",
    "    vol_ma_short = enhanced_vol_features['gk_volatility'].rolling(5, min_periods=3).mean()\n",
    "    vol_ma_long = enhanced_vol_features['gk_volatility'].rolling(20, min_periods=10).mean()\n",
    "    enhanced_vol_features['vol_momentum'] = (vol_ma_short - vol_ma_long) / (vol_ma_long + 1e-8)\n",
    "    enhanced_vol_features['vol_mean_reversion'] = (enhanced_vol_features['gk_volatility'] - vol_ma_long) / (vol_ma_long + 1e-8)\n",
    "# no ffill/bfill; early NaNs are allowed and will be dropped later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8245a",
   "metadata": {},
   "source": [
    "### 3.5 Funding Features (E Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58b8245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_5m = df[['timestamp']].copy()\n",
    "\n",
    "if len(funding_aligned) > 0:\n",
    "    funding_5m = pd.merge_asof(funding_5m, funding_aligned, on='timestamp', direction='backward')\n",
    "    # use as-is; do not forward/back fill here to keep strict causality\n",
    "    use_real_funding = True\n",
    "else:\n",
    "    funding_5m['funding_rate'] = np.nan\n",
    "    use_real_funding = False\n",
    "\n",
    "if use_real_funding and len(funding_5m) > 0 and 'funding_rate' in funding_5m.columns:\n",
    "    funding_5m['funding_momentum_1h'] = funding_5m['funding_rate'].diff(12)\n",
    "    funding_5m['funding_momentum_4h'] = funding_5m['funding_rate'].diff(48)\n",
    "    \n",
    "    funding_ma = funding_5m['funding_rate'].rolling(window=288, min_periods=50).mean()\n",
    "    funding_5m['funding_mr'] = (funding_5m['funding_rate'] - funding_ma) / (funding_ma.abs() + 1e-8)\n",
    "    \n",
    "    funding_5m['funding_vol'] = funding_5m['funding_rate'].rolling(window=288, min_periods=50).std()\n",
    "    \n",
    "    funding_features = funding_5m[['timestamp', 'funding_rate', 'funding_momentum_1h', \n",
    "                                  'funding_momentum_4h', 'funding_mr', 'funding_vol']].copy()\n",
    "    \n",
    "else:\n",
    "    funding_features = df[['timestamp']].copy()\n",
    "    funding_features['funding_rate'] = np.nan\n",
    "    funding_features['funding_momentum_1h'] = np.nan\n",
    "    funding_features['funding_momentum_4h'] = np.nan\n",
    "    funding_features['funding_mr'] = np.nan\n",
    "    funding_features['funding_vol'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac4316",
   "metadata": {},
   "source": [
    "### 3.8 Smart Trader Cohort Features & Final Dataset Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59ac4316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble a compact feature set (no duplicate columns)\n",
    "\n",
    "def create_target_labels(df_in, col_name, thresholds):\n",
    "    lower, upper = thresholds\n",
    "    x = df_in[col_name]\n",
    "    # 0: down, 1: neutral, 2: up (to match 3-class classifier)\n",
    "    return np.where(x > upper, 2, np.where(x < lower, 0, 1))\n",
    "\n",
    "base_timestamps = ohlcv_aligned[['timestamp', 'close']].copy()\n",
    "\n",
    "# Keep list: single source per feature; avoid duplicates downstream\n",
    "features_keep = [\n",
    "    'mom_1','mom_3','mr_ema20_z',               # momentum/price action\n",
    "    'rv_1h','gk_volatility','jump_magnitude','regime_high_vol',  # vol/jumps\n",
    "    'volume_intensity','price_efficiency','price_volume_corr','vwap_momentum','depth_proxy',  # micro/liquidity\n",
    "    'funding_rate','funding_momentum_1h',       # funding\n",
    "    'flow_diff','S_top','S_bot'                 # flow\n",
    "]\n",
    "\n",
    "# Source priority controls which slice \"wins\" if multiple slices carry the same feature\n",
    "sources = [\n",
    "    locals().get('price_action_features', pd.DataFrame()),\n",
    "    locals().get('volatility_features', pd.DataFrame()),           # prefer this for rv_1h / regime_high_vol\n",
    "    locals().get('enhanced_vol_features', pd.DataFrame()),         # skip duplicates already taken\n",
    "    locals().get('microstructure_features', pd.DataFrame()),\n",
    "    locals().get('funding_features', pd.DataFrame()),\n",
    "    locals().get('interaction_features', pd.DataFrame()),\n",
    "    locals().get('flow_features', pd.DataFrame()),\n",
    "]\n",
    "\n",
    "# Build slices with de-duplication by priority order\n",
    "selected = set()\n",
    "slices = []\n",
    "for df_src in sources:\n",
    "    if isinstance(df_src, pd.DataFrame) and len(df_src) > 0:\n",
    "        cols = [c for c in features_keep if c in df_src.columns and c not in selected]\n",
    "        if cols:\n",
    "            keep = ['timestamp'] + cols\n",
    "            slices.append(df_src[keep].copy())\n",
    "            selected.update(cols)\n",
    "\n",
    "# Merge all unique feature slices\n",
    "final_dataset = base_timestamps.copy()\n",
    "for sl in slices:\n",
    "    final_dataset = final_dataset.merge(sl, on='timestamp', how='inner', suffixes=('', '_dup'))\n",
    "\n",
    "# Clean up any residual suffixes (robustness if upstream added overlaps)\n",
    "# 1) Handle \"_dup\" from merges above\n",
    "_dup_cols = [c for c in final_dataset.columns if c.endswith('_dup')]\n",
    "for col in _dup_cols:\n",
    "    base = col[:-4]\n",
    "    if base not in final_dataset.columns:\n",
    "        final_dataset[base] = final_dataset[col]\n",
    "if _dup_cols:\n",
    "    final_dataset.drop(columns=_dup_cols, inplace=True, errors='ignore')\n",
    "\n",
    "# 2) Handle stray \"_x\"/\"_y\" if prior cells introduced them\n",
    "for suf in ('_x', '_y'):\n",
    "    suf_cols = [c for c in final_dataset.columns if c.endswith(suf)]\n",
    "    for col in suf_cols:\n",
    "        base = col[:-2]\n",
    "        if base not in final_dataset.columns:\n",
    "            final_dataset[base] = final_dataset[col]\n",
    "    if suf_cols:\n",
    "        final_dataset.drop(columns=suf_cols, inplace=True, errors='ignore')\n",
    "\n",
    "# Ensure rv_1h and regime_high_vol exist by fallback if missing\n",
    "if 'rv_1h' not in final_dataset.columns:\n",
    "    if 'volatility_features' in locals() and isinstance(volatility_features, pd.DataFrame) and 'rv_1h' in volatility_features.columns:\n",
    "        final_dataset = final_dataset.merge(volatility_features[['timestamp','rv_1h']], on='timestamp', how='left')\n",
    "    else:\n",
    "        returns = df['close'].pct_change()\n",
    "        rv_1h_fallback = returns.rolling(12, min_periods=6).apply(lambda x: (x**2).sum())\n",
    "        final_dataset = final_dataset.merge(pd.DataFrame({'timestamp': df['timestamp'], 'rv_1h': rv_1h_fallback}), on='timestamp', how='left')\n",
    "if 'regime_high_vol' not in final_dataset.columns:\n",
    "    if 'volatility_features' in locals() and isinstance(volatility_features, pd.DataFrame) and 'regime_high_vol' in volatility_features.columns:\n",
    "        final_dataset = final_dataset.merge(volatility_features[['timestamp','regime_high_vol']], on='timestamp', how='left')\n",
    "    else:\n",
    "        vol_threshold = final_dataset['rv_1h'].rolling(100, min_periods=25).quantile(0.75)\n",
    "        final_dataset['regime_high_vol'] = (final_dataset['rv_1h'] > vol_threshold).astype(float)\n",
    "\n",
    "# Targets (horizon aligned to next bar on 5m data)\n",
    "if 'returns_3min_bps' not in final_dataset.columns:\n",
    "    final_dataset['returns_3min_bps'] = (final_dataset['close'].shift(-1) / final_dataset['close'] - 1) * 10000\n",
    "# Adjust label thresholds for 5m horizon to reduce over-neutralization\n",
    "final_dataset['direction_confidence_3min'] = create_target_labels(final_dataset, 'returns_3min_bps', (-5, 5))\n",
    "final_dataset['profitable_opportunity'] = (final_dataset['returns_3min_bps'].abs() > 5).astype(int)\n",
    "\n",
    "TARGET_COLS = ['direction_confidence_3min', 'returns_3min_bps', 'profitable_opportunity']\n",
    "\n",
    "# Strict causality: do not ffill/bfill; drop any rows with missing features\n",
    "feature_cols = [c for c in final_dataset.columns if c not in ['timestamp','close'] + TARGET_COLS]\n",
    "final_dataset = final_dataset.dropna(subset=feature_cols)\n",
    "\n",
    "# Drop rows with missing targets\n",
    "if final_dataset[TARGET_COLS].isnull().any(axis=None):\n",
    "    final_dataset = final_dataset[final_dataset[TARGET_COLS].notna().all(axis=1)].copy()\n",
    "\n",
    "# Lightweight causality audit of predictors\n",
    "_predictor_cols = [c for c in final_dataset.columns if c not in ['timestamp','close'] + TARGET_COLS]\n",
    "_suspect = [c for c in _predictor_cols if c.endswith('_tplus')]\n",
    "if len(_suspect) > 0:\n",
    "    _suspect  # intentionally no print/log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52524f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete | train_data: (8290, 22) | backtest_data: (2073, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create exactly two datasets from final_dataset: train_data (80%) and backtest_data (20%)\n",
    "# Minimal, deterministic split; no extra variables.\n",
    "\n",
    "if 'final_dataset' not in globals():\n",
    "    print(\"final_dataset not found. Please run cells 5–18 to build it before splitting.\")\n",
    "else:\n",
    "    n = len(final_dataset)\n",
    "    split_idx = max(1, int(n * 0.8))\n",
    "    train_data = final_dataset.iloc[:split_idx].copy()\n",
    "    backtest_data = final_dataset.iloc[split_idx:].copy()\n",
    "    print(f\"Split complete | train_data: {train_data.shape} | backtest_data: {backtest_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8574bc",
   "metadata": {},
   "source": [
    "## 4. Model Training and Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8bb385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMetaClassifier:\n",
    "    \"\"\"Enhanced Meta-Learner for Classification\"\"\"\n",
    "    \n",
    "    def __init__(self, meta_C=1.0, random_state=42, n_folds=5, \n",
    "                 embargo_pct=0.01, purge_pct=0.02, min_train_samples=1000, n_classes=3):\n",
    "        from sklearn.model_selection import TimeSeriesSplit\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "        from sklearn.metrics import accuracy_score, log_loss\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        \n",
    "        self.meta_C = meta_C\n",
    "        self.random_state = random_state\n",
    "        self.n_folds = n_folds\n",
    "        self.embargo_pct = embargo_pct\n",
    "        self.purge_pct = purge_pct\n",
    "        self.min_train_samples = min_train_samples\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.base_models = {}\n",
    "        self.meta_model = None\n",
    "        self.scaler = RobustScaler()\n",
    "        self.is_fitted = False\n",
    "        self.meta_score = 0.0\n",
    "        self.cv_scores = {}\n",
    "        self.fold_scores = {}\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'meta_C': self.meta_C,\n",
    "            'random_state': self.random_state,\n",
    "            'n_folds': self.n_folds,\n",
    "            'embargo_pct': self.embargo_pct,\n",
    "            'purge_pct': self.purge_pct,\n",
    "            'min_train_samples': self.min_train_samples,\n",
    "            'n_classes': self.n_classes\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            if hasattr(self, param):\n",
    "                setattr(self, param, value)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid parameter {param} for estimator {type(self).__name__}\")\n",
    "        return self\n",
    "        \n",
    "    def _get_base_models(self):\n",
    "        from sklearn.preprocessing import QuantileTransformer\n",
    "        from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "        return {\n",
    "            'histgb': HistGradientBoostingClassifier(\n",
    "                max_iter=180,\n",
    "                learning_rate=0.07,\n",
    "                max_depth=7,\n",
    "                min_samples_leaf=20,\n",
    "                l2_regularization=1e-2,\n",
    "                max_bins=255,\n",
    "                validation_fraction=0.1,\n",
    "                random_state=self.random_state\n",
    "            ),\n",
    "            'randomforest': RandomForestClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=16,\n",
    "                min_samples_split=4,\n",
    "                min_samples_leaf=2,\n",
    "                max_features='sqrt',\n",
    "                bootstrap=True,\n",
    "                class_weight='balanced_subsample',\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'extratrees': ExtraTreesClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=16,\n",
    "                min_samples_split=4,\n",
    "                min_samples_leaf=2,\n",
    "                class_weight='balanced',\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'gb_classifier': GradientBoostingClassifier(\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=3,\n",
    "                subsample=0.7,\n",
    "                random_state=self.random_state\n",
    "            ),\n",
    "            'logistic_scaled': Pipeline([\n",
    "                ('quantile', QuantileTransformer(n_quantiles=800, output_distribution='normal')),\n",
    "                ('scaler', RobustScaler()),\n",
    "                ('classifier', LogisticRegression(random_state=self.random_state, max_iter=1500, C=1.0, solver='lbfgs', multi_class='auto', class_weight='balanced'))\n",
    "            ]),\n",
    "            'naive_bayes_scaled': Pipeline([\n",
    "                ('scaler', RobustScaler()),\n",
    "                ('classifier', GaussianNB())\n",
    "            ])\n",
    "        }\n",
    "    \n",
    "    def _create_purged_splits(self, X, y):\n",
    "        n_samples = len(X)\n",
    "        embargo_periods = max(1, int(n_samples * self.embargo_pct))\n",
    "        purge_periods = max(1, int(n_samples * self.purge_pct))\n",
    "        \n",
    "        class_counts = y.value_counts()\n",
    "        \n",
    "        min_required_samples = self.min_train_samples + embargo_periods + purge_periods + 100\n",
    "        if n_samples < min_required_samples:\n",
    "            raise ValueError(f\"INSUFFICIENT DATA: Need at least {min_required_samples} samples for purged CV, got {n_samples}\")\n",
    "        \n",
    "        min_class_count = class_counts.min()\n",
    "        \n",
    "        splits = []\n",
    "        step_size = n_samples // (self.n_folds + 1)\n",
    "        \n",
    "        for i in range(self.n_folds):\n",
    "            train_end = (i + 1) * step_size\n",
    "            train_start = 0\n",
    "            \n",
    "            train_end_purged = max(train_start + self.min_train_samples, train_end - purge_periods)\n",
    "            \n",
    "            val_start = train_end + embargo_periods\n",
    "            val_end = min(val_start + step_size, n_samples)\n",
    "            \n",
    "            train_size = train_end_purged - train_start\n",
    "            val_size = val_end - val_start\n",
    "            \n",
    "            if train_size >= self.min_train_samples and val_size >= 50:\n",
    "                train_idx = list(range(train_start, train_end_purged))\n",
    "                val_idx = list(range(val_start, val_end))\n",
    "                \n",
    "                train_classes = np.unique(y.iloc[train_idx])\n",
    "                val_classes = np.unique(y.iloc[val_idx])\n",
    "                \n",
    "                if len(train_classes) >= 2 and len(val_classes) >= 2:\n",
    "                    if hasattr(X, 'index'):\n",
    "                        try:\n",
    "                            train_times = X.index[train_idx]\n",
    "                            val_times = X.index[val_idx]\n",
    "                            if len(train_times) > 0 and len(val_times) > 0:\n",
    "                                train_max_time = max(train_times)\n",
    "                                val_min_time = min(val_times)\n",
    "                                if train_max_time >= val_min_time:\n",
    "                                    continue\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    \n",
    "                    splits.append((train_idx, val_idx))\n",
    "        \n",
    "        if len(splits) == 0:\n",
    "            raise ValueError(\"No valid classification splits created\")\n",
    "        \n",
    "        if len(splits) < 2:\n",
    "            raise ValueError(\"Only {len(splits)} valid folds created, need at least 2\")\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        unique_classes = np.unique(y)\n",
    "        if len(unique_classes) < 2:\n",
    "            raise ValueError(f\"Need at least 2 classes for classification, found: {unique_classes}\")\n",
    "        \n",
    "        self.base_models = self._get_base_models()\n",
    "        \n",
    "        purged_splits = self._create_purged_splits(X, y)\n",
    "        \n",
    "        oof_preds = np.zeros((len(X), len(self.base_models)))\n",
    "        oof_probs = np.zeros((len(X), len(self.base_models) * self.n_classes))\n",
    "        \n",
    "        model_names = list(self.base_models.keys())\n",
    "        \n",
    "        for model_idx, (name, model) in enumerate(self.base_models.items()):\n",
    "            fold_scores = []\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(purged_splits):\n",
    "                if hasattr(X, 'index') and len(X.index) > 0:\n",
    "                    try:\n",
    "                        train_max_time = X.index[train_idx].max()\n",
    "                        val_min_time = X.index[val_idx].min()\n",
    "                        if train_max_time >= val_min_time:\n",
    "                            continue\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                model_clone = clone(model)\n",
    "                try:\n",
    "                    model_clone.fit(X_train_fold, y_train_fold)\n",
    "                    val_preds = model_clone.predict(X_val_fold)\n",
    "                    val_probs = model_clone.predict_proba(X_val_fold)\n",
    "                    \n",
    "                    oof_preds[val_idx, model_idx] = val_preds\n",
    "                    \n",
    "                    prob_start = model_idx * self.n_classes\n",
    "                    prob_end = prob_start + self.n_classes\n",
    "                    \n",
    "                    if val_probs.shape[1] == self.n_classes:\n",
    "                        oof_probs[val_idx, prob_start:prob_end] = val_probs\n",
    "                    else:\n",
    "                        temp_probs = np.zeros((len(val_idx), self.n_classes))\n",
    "                        classes = model_clone.classes_\n",
    "                        for i, cls in enumerate(classes):\n",
    "                            if cls < self.n_classes:\n",
    "                                temp_probs[:, cls] = val_probs[:, i]\n",
    "                        oof_probs[val_idx, prob_start:prob_end] = temp_probs\n",
    "                    \n",
    "                    fold_accuracy = accuracy_score(y_val_fold, val_preds)\n",
    "                    fold_scores.append(fold_accuracy)\n",
    "                    \n",
    "                except Exception:\n",
    "                    most_frequent_class = y_train_fold.mode().iloc[0]\n",
    "                    oof_preds[val_idx, model_idx] = most_frequent_class\n",
    "                    uniform_prob = 1.0 / self.n_classes\n",
    "                    prob_start = model_idx * self.n_classes\n",
    "                    prob_end = prob_start + self.n_classes\n",
    "                    oof_probs[val_idx, prob_start:prob_end] = uniform_prob\n",
    "                    fold_scores.append(0.0)\n",
    "            \n",
    "            self.cv_scores[name] = np.mean(fold_scores) if fold_scores else 0.0\n",
    "            self.fold_scores[name] = fold_scores\n",
    "        \n",
    "        oof_probs_scaled = self.scaler.fit_transform(oof_probs)\n",
    "        \n",
    "        # Meta logistic: balanced classes and higher max_iter for stable convergence\n",
    "        self.meta_model = LogisticRegression(\n",
    "            C=self.meta_C,\n",
    "            random_state=self.random_state,\n",
    "            max_iter=1500,\n",
    "            solver='lbfgs',\n",
    "            multi_class='auto',\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        self.meta_model.fit(oof_probs_scaled, y)\n",
    "        \n",
    "        meta_pred = self.meta_model.predict(oof_probs_scaled)\n",
    "        self.meta_score = accuracy_score(y, meta_pred)\n",
    "        \n",
    "        for name, model in self.base_models.items():\n",
    "            model.fit(X, y)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.is_fitted:\n",
    "            return np.zeros(len(X))\n",
    "        \n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if not self.is_fitted:\n",
    "            return np.full((len(X), self.n_classes), 1.0/self.n_classes)\n",
    "        \n",
    "        base_probs = np.zeros((len(X), len(self.base_models) * self.n_classes))\n",
    "        \n",
    "        for model_idx, (name, model) in enumerate(self.base_models.items()):\n",
    "            try:\n",
    "                model_probs = model.predict_proba(X)\n",
    "                prob_start = model_idx * self.n_classes\n",
    "                prob_end = prob_start + self.n_classes\n",
    "                \n",
    "                if model_probs.shape[1] == self.n_classes:\n",
    "                    base_probs[:, prob_start:prob_end] = model_probs\n",
    "                else:\n",
    "                    temp_probs = np.zeros((len(X), self.n_classes))\n",
    "                    classes = model.classes_\n",
    "                    for i, cls in enumerate(classes):\n",
    "                        if cls < self.n_classes:\n",
    "                            temp_probs[:, cls] = model_probs[:, i]\n",
    "                    base_probs[:, prob_start:prob_end] = temp_probs\n",
    "                    \n",
    "            except Exception:\n",
    "                prob_start = model_idx * self.n_classes\n",
    "                prob_end = prob_start + self.n_classes\n",
    "                base_probs[:, prob_start:prob_end] = 1.0 / self.n_classes\n",
    "        \n",
    "        base_probs_scaled = self.scaler.transform(base_probs)\n",
    "        return self.meta_model.predict_proba(base_probs_scaled)\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        if not self.is_fitted:\n",
    "            return {'type': 'enhanced_meta_classifier', 'fitted': False}\n",
    "        \n",
    "        feature_importance = {}\n",
    "        model_names = list(self.base_models.keys())\n",
    "        \n",
    "        for class_idx in range(self.n_classes):\n",
    "            feature_importance[f'class_{class_idx}'] = {}\n",
    "            for model_idx, name in enumerate(model_names):\n",
    "                prob_indices = list(range(model_idx * self.n_classes, (model_idx + 1) * self.n_classes))\n",
    "                coefs = self.meta_model.coef_[class_idx] if self.n_classes > 2 else self.meta_model.coef_[0]\n",
    "                feature_importance[f'class_{class_idx}'][name] = np.mean(np.abs(coefs[prob_indices]))\n",
    "        \n",
    "        return {\n",
    "            'type': 'enhanced_meta_classifier',\n",
    "            'fitted': True,\n",
    "            'meta_score': self.meta_score,\n",
    "            'feature_importance': feature_importance,\n",
    "            'meta_C': self.meta_C,\n",
    "            'cv_scores': self.cv_scores,\n",
    "            'fold_scores': self.fold_scores,\n",
    "            'n_folds': self.n_folds,\n",
    "            'embargo_pct': self.embargo_pct,\n",
    "            'purge_pct': self.purge_pct,\n",
    "            'n_classes': self.n_classes\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a324c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "class CustomClassificationCalibrator:\n",
    "    \"\"\"Isotonic-only calibration for the meta-learner classifier (no CV; fits on full calibration window).\"\"\"\n",
    "\n",
    "    def __init__(self, base_estimator):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.calibrators = {}\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit isotonic calibration on the full calibration set probabilities (per class).\"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        # Get uncalibrated probabilities from the already-fitted base estimator\n",
    "        uncal_probas = self.base_estimator.predict_proba(X)\n",
    "        n_classes = uncal_probas.shape[1]\n",
    "        y_arr = np.asarray(y)\n",
    "\n",
    "        for class_idx in range(n_classes):\n",
    "            class_probas = uncal_probas[:, class_idx]\n",
    "            class_labels = (y_arr == class_idx).astype(int)\n",
    "\n",
    "            # Isotonic requires both positive and negative examples; fallback if class absent\n",
    "            pos = class_labels.sum()\n",
    "            neg = len(class_labels) - pos\n",
    "            if pos == 0 or neg == 0:\n",
    "                # Not enough signal to fit isotonic; keep calibrator as None to pass through uncalibrated probs\n",
    "                self.calibrators[class_idx] = None\n",
    "                continue\n",
    "\n",
    "            calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "            calibrator.fit(class_probas, class_labels)\n",
    "            self.calibrators[class_idx] = calibrator\n",
    "\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return isotonic-calibrated probabilities (rows normalized to sum to 1).\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Calibrator must be fitted before making predictions\")\n",
    "\n",
    "        import numpy as np\n",
    "        uncal_probas = self.base_estimator.predict_proba(X)\n",
    "        cal_probas = uncal_probas.copy()\n",
    "\n",
    "        n_classes = uncal_probas.shape[1]\n",
    "        for class_idx in range(n_classes):\n",
    "            calibrator = self.calibrators.get(class_idx, None)\n",
    "            if calibrator is not None:\n",
    "                cal_probas[:, class_idx] = calibrator.transform(uncal_probas[:, class_idx])\n",
    "            # else: leave uncalibrated values as-is\n",
    "\n",
    "        # Normalize rows to sum to 1 to get a proper probability distribution\n",
    "        row_sums = cal_probas.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1.0\n",
    "        cal_probas = cal_probas / row_sums\n",
    "        return cal_probas\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return calibrated class predictions\"\"\"\n",
    "        import numpy as np\n",
    "        probas = self.predict_proba(X)\n",
    "        return np.argmax(probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8be92fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'final_dataset' in locals() and not final_dataset.empty:\n",
    "    train_dataset = final_dataset.copy()\n",
    "    \n",
    "    feature_cols = [col for col in train_dataset.columns \n",
    "                   if col not in TARGET_COLS + ['timestamp', 'close']]\n",
    "\n",
    "    # No global imputation here to avoid leakage; ensure clean by dropping any residual NaNs\n",
    "    if train_dataset[feature_cols].isnull().any(axis=None):\n",
    "        train_dataset = train_dataset.dropna(subset=feature_cols)\n",
    "    \n",
    "    proper_feature_cols = feature_cols.copy()\n",
    "\n",
    "    # Three-way time-ordered split: 64% train (fit), 16% calibration (held-out), 20% test/backtest (held-out)\n",
    "    n = len(train_dataset)\n",
    "    train_end = int(n * 0.64)\n",
    "    calib_end = int(n * 0.80)\n",
    "\n",
    "    train_data = train_dataset.iloc[:train_end].copy()\n",
    "    calib_data = train_dataset.iloc[train_end:calib_end].copy()\n",
    "    test_data = train_dataset.iloc[calib_end:].copy()\n",
    "\n",
    "    # Feature matrices\n",
    "    X_train_model_clean = train_data[feature_cols].copy()\n",
    "    X_train_calib_clean = calib_data[feature_cols].copy()\n",
    "    X_test_model_clean = test_data[feature_cols].copy()\n",
    "\n",
    "    classification_target_available = True\n",
    "\n",
    "else:\n",
    "    proper_feature_cols = []\n",
    "    X_train_model_clean = pd.DataFrame()\n",
    "    X_train_calib_clean = pd.DataFrame()\n",
    "    classification_target_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d94f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_target = 'direction_confidence_3min'\n",
    "\n",
    "if classification_target in train_dataset.columns:\n",
    "    y_train_classification = train_dataset[classification_target].loc[X_train_model_clean.index]\n",
    "    y_calib_classification = train_dataset[classification_target].loc[X_train_calib_clean.index]\n",
    "    # Ensure test labels exist for Cell 25\n",
    "    if 'X_test_model_clean' in globals():\n",
    "        y_test_model_clean = train_dataset[classification_target].loc[X_test_model_clean.index]\n",
    "\n",
    "    class_dist = y_train_classification.value_counts().sort_index()\n",
    "    min_class_size = class_dist.min()\n",
    "\n",
    "    try:\n",
    "        meta_classifier = EnhancedMetaClassifier(\n",
    "            meta_C=1.0,\n",
    "            random_state=42,\n",
    "            n_folds=3,\n",
    "            embargo_pct=0.01,\n",
    "            purge_pct=0.02,\n",
    "            min_train_samples=1000,\n",
    "            n_classes=3\n",
    "        )\n",
    "        meta_classifier.fit(X_train_model_clean, y_train_classification)\n",
    "        # Explicit fitted flag for downstream guards\n",
    "        setattr(meta_classifier, 'fitted_', True)\n",
    "\n",
    "        meta_class_info = meta_classifier.get_model_info()\n",
    "\n",
    "    except Exception:\n",
    "        meta_classifier = None\n",
    "\n",
    "    if meta_classifier is not None:\n",
    "        try:\n",
    "            meta_class_probs_uncal = meta_classifier.predict_proba(X_train_calib_clean)\n",
    "            meta_class_preds_uncal = meta_classifier.predict(X_train_calib_clean)\n",
    "\n",
    "            # Isotonic-only calibrator (no method arg)\n",
    "            meta_class_calibrator = CustomClassificationCalibrator(\n",
    "                base_estimator=meta_classifier\n",
    "            )\n",
    "            meta_class_calibrator.fit(X_train_calib_clean, y_calib_classification)\n",
    "            # Explicit fitted flag for downstream guards\n",
    "            setattr(meta_class_calibrator, 'fitted_', True)\n",
    "\n",
    "            meta_class_probs_cal = meta_class_calibrator.predict_proba(X_train_calib_clean)\n",
    "            meta_class_preds_cal = meta_class_calibrator.predict(X_train_calib_clean)\n",
    "\n",
    "            from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "            meta_acc_uncal = accuracy_score(y_calib_classification, meta_class_preds_uncal)\n",
    "            meta_acc_cal = accuracy_score(y_calib_classification, meta_class_preds_cal)\n",
    "\n",
    "            try:\n",
    "                meta_logloss_uncal = log_loss(y_calib_classification, meta_class_probs_uncal)\n",
    "                meta_logloss_cal = log_loss(y_calib_classification, meta_class_probs_cal)\n",
    "            except Exception:\n",
    "                meta_logloss_uncal = meta_logloss_cal = float('inf')\n",
    "\n",
    "        except Exception:\n",
    "            meta_class_calibrator = None\n",
    "else:\n",
    "    meta_classifier = None\n",
    "    meta_class_calibrator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a44e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight BMA-style classifier (reference): trains multiple base models, blends by OOF accuracy/consistency/recency\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class BMAStackerClassifier:\n",
    "    def __init__(self, n_folds=5, random_state=42, embargo_pct=0.01, purge_pct=0.02, min_train_samples=1000, n_classes=3):\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        self.embargo_pct = embargo_pct\n",
    "        self.purge_pct = purge_pct\n",
    "        self.min_train_samples = min_train_samples\n",
    "        self.n_classes = n_classes\n",
    "        self.base_models = {}\n",
    "        self.weights_ = {}\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _get_base_models(self):\n",
    "        return {\n",
    "            'histgb': HistGradientBoostingClassifier(max_iter=100, learning_rate=0.1, max_depth=6, random_state=self.random_state),\n",
    "            'randomforest': RandomForestClassifier(n_estimators=200, max_depth=12, random_state=self.random_state, n_jobs=-1, class_weight='balanced_subsample'),\n",
    "            'extratrees': ExtraTreesClassifier(n_estimators=200, max_depth=12, random_state=self.random_state, n_jobs=-1, class_weight='balanced'),\n",
    "            'logistic_scaled': Pipeline([\n",
    "                ('quantile', QuantileTransformer(n_quantiles=800, output_distribution='normal')),\n",
    "                ('scaler', RobustScaler()),\n",
    "                ('classifier', LogisticRegression(random_state=self.random_state, max_iter=1000, class_weight='balanced'))\n",
    "            ]),\n",
    "        }\n",
    "\n",
    "    def _create_purged_splits(self, X, y):\n",
    "        n = len(X)\n",
    "        embargo = max(1, int(n * self.embargo_pct))\n",
    "        purge = max(1, int(n * self.purge_pct))\n",
    "        step = max(1, n // (self.n_folds + 1))\n",
    "        splits = []\n",
    "        for i in range(self.n_folds):\n",
    "            train_end = (i + 1) * step\n",
    "            train_end_purged = max(self.min_train_samples, train_end - purge)\n",
    "            val_start = train_end + embargo\n",
    "            val_end = min(val_start + step, n)\n",
    "            if train_end_purged > 0 and val_end - val_start >= 50 and train_end_purged >= self.min_train_samples:\n",
    "                train_idx = list(range(0, train_end_purged))\n",
    "                val_idx = list(range(val_start, val_end))\n",
    "                # Basic temporal check\n",
    "                if hasattr(X, 'index') and len(X.index) > 0:\n",
    "                    try:\n",
    "                        if X.index[train_idx].max() >= X.index[val_idx].min():\n",
    "                            continue\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                # Ensure class diversity\n",
    "                if len(np.unique(y.iloc[train_idx])) >= 2 and len(np.unique(y.iloc[val_idx])) >= 2:\n",
    "                    splits.append((train_idx, val_idx))\n",
    "        if len(splits) < 2:\n",
    "            raise ValueError('Insufficient valid purged splits for BMAStackerClassifier')\n",
    "        return splits\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.base_models = self._get_base_models()\n",
    "        splits = self._create_purged_splits(X, y)\n",
    "        m = len(self.base_models)\n",
    "        oof_preds = np.zeros((len(X), m), dtype=float)\n",
    "        oof_probs = np.zeros((len(X), m * self.n_classes), dtype=float)\n",
    "\n",
    "        names = list(self.base_models.keys())\n",
    "        acc_scores = {n: [] for n in names}\n",
    "\n",
    "        for j, (name, model) in enumerate(self.base_models.items()):\n",
    "            for (tr_idx, va_idx) in splits:\n",
    "                mdl = clone(model)\n",
    "                Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "                ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "                try:\n",
    "                    mdl.fit(Xtr, ytr)\n",
    "                    pv = mdl.predict_proba(Xva)\n",
    "                    yp = mdl.predict(Xva)\n",
    "                    acc_scores[name].append(accuracy_score(yva, yp))\n",
    "                    oof_preds[va_idx, j] = yp\n",
    "                    ps, pe = j * self.n_classes, (j + 1) * self.n_classes\n",
    "                    temp = np.zeros((len(va_idx), self.n_classes))\n",
    "                    if pv.shape[1] == self.n_classes:\n",
    "                        temp = pv\n",
    "                    else:\n",
    "                        for ii, cls in enumerate(getattr(mdl, 'classes_', [])):\n",
    "                            if int(cls) < self.n_classes:\n",
    "                                temp[:, int(cls)] = pv[:, ii]\n",
    "                    oof_probs[va_idx, ps:pe] = temp\n",
    "                except Exception:\n",
    "                    acc_scores[name].append(0.0)\n",
    "                    ps, pe = j * self.n_classes, (j + 1) * self.n_classes\n",
    "                    oof_probs[va_idx, ps:pe] = 1.0 / self.n_classes\n",
    "\n",
    "        # Composite weights: accuracy mean as simple proxy (normalize to sum 1)\n",
    "        raw = np.array([np.mean(acc_scores[n]) if len(acc_scores[n]) else 0.0 for n in names], dtype=float)\n",
    "        raw = np.clip(raw, 0.0, None)\n",
    "        s = raw.sum()\n",
    "        w = (raw / s) if s > 0 else np.ones_like(raw) / max(len(raw), 1)\n",
    "        self.weights_ = {n: float(wi) for n, wi in zip(names, w)}\n",
    "\n",
    "        # Fit final bases on full data\n",
    "        for name, model in self.base_models.items():\n",
    "            try:\n",
    "                model.fit(X, y)\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self.is_fitted:\n",
    "            return np.full((len(X), self.n_classes), 1.0 / self.n_classes)\n",
    "        names = list(self.base_models.keys())\n",
    "        probs = np.zeros((len(X), self.n_classes), dtype=float)\n",
    "        for name in names:\n",
    "            mdl = self.base_models[name]\n",
    "            try:\n",
    "                pv = mdl.predict_proba(X)\n",
    "                # align class columns if needed\n",
    "                aligned = np.zeros_like(probs)\n",
    "                if pv.shape[1] == self.n_classes:\n",
    "                    aligned = pv\n",
    "                else:\n",
    "                    for ii, cls in enumerate(getattr(mdl, 'classes_', [])):\n",
    "                        cls = int(cls)\n",
    "                        if 0 <= cls < self.n_classes:\n",
    "                            aligned[:, cls] = pv[:, ii]\n",
    "                probs += self.weights_.get(name, 0.0) * aligned\n",
    "            except Exception:\n",
    "                probs += (self.weights_.get(name, 0.0) * (1.0 / self.n_classes))\n",
    "        # Row-normalize\n",
    "        row_sums = probs.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1.0\n",
    "        return probs / row_sums\n",
    "\n",
    "# Train optional BMA classifier and its calibrator if training data available\n",
    "bma_classifier = None\n",
    "bma_class_calibrator = None\n",
    "try:\n",
    "    if 'X_train_model_clean' in globals() and 'y_train_classification' in globals():\n",
    "        bma_classifier = BMAStackerClassifier(n_folds=3, random_state=42, embargo_pct=0.01, purge_pct=0.02, min_train_samples=800, n_classes=3)\n",
    "        bma_classifier.fit(X_train_model_clean, y_train_classification)\n",
    "        # Reuse isotonic calibrator wrapper defined earlier\n",
    "        if 'CustomClassificationCalibrator' in globals():\n",
    "            bma_class_calibrator = CustomClassificationCalibrator(base_estimator=bma_classifier)\n",
    "            bma_class_calibrator.fit(X_train_calib_clean, y_calib_classification)\n",
    "            setattr(bma_class_calibrator, 'fitted_', True)\n",
    "except Exception:\n",
    "    bma_classifier = None\n",
    "    bma_class_calibrator = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62510c70",
   "metadata": {},
   "source": [
    "### Optional BMA classifier (reference only)\n",
    "\n",
    "We add a lightweight BMA-style classification ensemble (trained with purged, embargoed splits) as a reference implementation. It will not replace the existing meta-classifier; instead, we expose its calibrated signal as an additional arm `bma_model` for the allocator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1b28e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model artifacts:\n",
      " - paper_trading_outputs\\models\\meta_classifier_20251028_101254_d7a9e9fb3a42.joblib\n",
      " - paper_trading_outputs\\models\\calibrator_20251028_101254_d7a9e9fb3a42.joblib\n",
      " - paper_trading_outputs\\models\\feature_columns_20251028_101254_d7a9e9fb3a42.json\n",
      " - paper_trading_outputs\\models\\training_meta_20251028_101254_d7a9e9fb3a42.json\n",
      "LATEST -> paper_trading_outputs\\models\\LATEST.json\n"
     ]
    }
   ],
   "source": [
    "# Persist trained model, calibrator, and feature schema\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "# Optional: pandas only used for fallbacks; safe to import\n",
    "try:\n",
    "    import pandas as pd  # noqa: F401\n",
    "except Exception:\n",
    "    pd = None\n",
    "\n",
    "\n",
    "def _schema_hash(cols):\n",
    "    j = json.dumps(list(cols), separators=(\",\", \":\"), ensure_ascii=False)\n",
    "    return hashlib.sha256(j.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "\n",
    "def _is_fitted(obj) -> bool:\n",
    "    try:\n",
    "        if getattr(obj, \"fitted_\", False):\n",
    "            return True\n",
    "        if getattr(obj, \"is_fitted\", False):\n",
    "            return True\n",
    "        # Common sklearn indicator\n",
    "        if hasattr(obj, \"classes_\"):\n",
    "            return True\n",
    "        # Our calibrator may have inner calibrators\n",
    "        if hasattr(obj, \"calibrators_\"):\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "def save_artifacts(meta_model, calibrator, feature_cols, target_name=\"direction_confidence_3min\"):\n",
    "    assert meta_model is not None and _is_fitted(meta_model), \"Meta model not fitted\"\n",
    "    assert calibrator is not None and _is_fitted(calibrator), \"Calibrator not fitted\"\n",
    "\n",
    "    out_dir = os.path.join(\"paper_trading_outputs\", \"models\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    stamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    schema_sig = _schema_hash(feature_cols)\n",
    "\n",
    "    meta_path = os.path.join(out_dir, f\"meta_classifier_{stamp}_{schema_sig}.joblib\")\n",
    "    cal_path  = os.path.join(out_dir, f\"calibrator_{stamp}_{schema_sig}.joblib\")\n",
    "    cols_path = os.path.join(out_dir, f\"feature_columns_{stamp}_{schema_sig}.json\")\n",
    "    info_path = os.path.join(out_dir, f\"training_meta_{stamp}_{schema_sig}.json\")\n",
    "    manifest  = os.path.join(out_dir, \"LATEST.json\")\n",
    "\n",
    "    # Dump models\n",
    "    joblib.dump(meta_model, meta_path, compress=3)\n",
    "    joblib.dump(calibrator, cal_path, compress=3)\n",
    "\n",
    "    # Dump feature columns\n",
    "    with open(cols_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"feature_cols\": list(feature_cols), \"schema_hash\": schema_sig}, f)\n",
    "\n",
    "    # Training info (best-effort)\n",
    "    info = {}\n",
    "    try:\n",
    "        if hasattr(meta_model, \"get_model_info\"):\n",
    "            info = meta_model.get_model_info()\n",
    "    except Exception:\n",
    "        info = {}\n",
    "    train_info = {\n",
    "        \"timestamp_utc\": stamp,\n",
    "        \"target\": target_name,\n",
    "        \"n_features\": int(len(feature_cols)),\n",
    "        \"schema_hash\": schema_sig,\n",
    "        \"class_mapping\": {\"down\": 0, \"neutral\": 1, \"up\": 2},\n",
    "        \"meta_score_in_sample\": float(info.get(\"meta_score\", 0.0)) if isinstance(info, dict) else 0.0,\n",
    "        \"cv_scores\": info.get(\"cv_scores\", {}) if isinstance(info, dict) else {},\n",
    "        \"folds\": int(info.get(\"n_folds\", 0)) if isinstance(info, dict) else 0,\n",
    "        \"embargo_pct\": float(info.get(\"embargo_pct\", 0.0)) if isinstance(info, dict) else 0.0,\n",
    "        \"purge_pct\": float(info.get(\"purge_pct\", 0.0)) if isinstance(info, dict) else 0.0,\n",
    "    }\n",
    "    with open(info_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(train_info, f)\n",
    "\n",
    "    # Update LATEST manifest\n",
    "    latest = {\n",
    "        \"meta_classifier\": os.path.basename(meta_path),\n",
    "        \"calibrator\": os.path.basename(cal_path),\n",
    "        \"feature_columns\": os.path.basename(cols_path),\n",
    "        \"training_meta\": os.path.basename(info_path),\n",
    "    }\n",
    "    with open(manifest, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(latest, f)\n",
    "\n",
    "    print(\"Saved model artifacts:\")\n",
    "    print(\" -\", meta_path)\n",
    "    print(\" -\", cal_path)\n",
    "    print(\" -\", cols_path)\n",
    "    print(\" -\", info_path)\n",
    "    print(\"LATEST ->\", manifest)\n",
    "\n",
    "\n",
    "# Resolve feature cols and target name from globals\n",
    "_feature_cols = None\n",
    "for name in (\"proper_feature_cols\",):\n",
    "    if name in globals() and globals()[name] is not None:\n",
    "        _feature_cols = list(globals()[name])\n",
    "        break\n",
    "if _feature_cols is None:\n",
    "    for name in (\"X_train_model_clean\", \"X_train_calib_clean\", \"X_test_model_clean\"):\n",
    "        if name in globals() and getattr(globals()[name], \"columns\", None) is not None:\n",
    "            _feature_cols = list(globals()[name].columns)\n",
    "            break\n",
    "if _feature_cols is None:\n",
    "    raise RuntimeError(\"Could not determine feature columns to save. Ensure training cells ran.\")\n",
    "\n",
    "_target_name = globals().get(\"classification_target\", \"direction_confidence_3min\")\n",
    "\n",
    "# Save once after fitting/calibration\n",
    "if \"meta_classifier\" in globals() and \"meta_class_calibrator\" in globals():\n",
    "    if _is_fitted(meta_classifier) and _is_fitted(meta_class_calibrator):\n",
    "        save_artifacts(meta_classifier, meta_class_calibrator, _feature_cols, target_name=_target_name)\n",
    "    else:\n",
    "        print(\"Artifacts not saved: model or calibrator not fitted.\")\n",
    "else:\n",
    "    print(\"Artifacts not saved: meta_classifier or meta_class_calibrator missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b71bd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc_uncal=0.3333 test_acc_cal=0.3734\n",
      "test_logloss_uncal=1.1892 test_logloss_cal=1.1002\n",
      "proba_shapes uncal=(2073, 3) cal=(2073, 3); rowsum≈1 check cal=True\n"
     ]
    }
   ],
   "source": [
    "# Cell 25 — Test-holdout validation and artifact checks\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Guards\n",
    "assert 'meta_classifier' in globals() and getattr(meta_classifier, 'fitted_', False), \"Model not fitted. Run training cells.\"\n",
    "assert 'meta_class_calibrator' in globals() and getattr(meta_class_calibrator, 'fitted_', False), \"Calibrator not fitted. Run calibration.\"\n",
    "assert 'X_test_model_clean' in globals() and 'y_test_model_clean' in globals(), \"Test split missing.\"\n",
    "\n",
    "X_test = X_test_model_clean\n",
    "y_test = y_test_model_clean\n",
    "\n",
    "# Uncalibrated predictions\n",
    "proba_uncal = meta_classifier.predict_proba(X_test)\n",
    "preds_uncal = np.argmax(proba_uncal, axis=1)\n",
    "\n",
    "# Calibrated predictions (calibrator wraps the model internally)\n",
    "proba_cal = meta_class_calibrator.predict_proba(X_test)\n",
    "preds_cal = meta_class_calibrator.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "acc_uncal = accuracy_score(y_test, preds_uncal)\n",
    "acc_cal = accuracy_score(y_test, preds_cal)\n",
    "\n",
    "# Handle potential single-class edge case for log_loss\n",
    "try:\n",
    "    ll_uncal = log_loss(y_test, proba_uncal, labels=np.unique(y_test))\n",
    "    ll_cal = log_loss(y_test, proba_cal, labels=np.unique(y_test))\n",
    "except ValueError:\n",
    "    ll_uncal, ll_cal = np.nan, np.nan\n",
    "\n",
    "print(f\"test_acc_uncal={acc_uncal:.4f} test_acc_cal={acc_cal:.4f}\")\n",
    "print(f\"test_logloss_uncal={ll_uncal:.4f} test_logloss_cal={ll_cal:.4f}\")\n",
    "print(f\"proba_shapes uncal={proba_uncal.shape} cal={proba_cal.shape}; rowsum≈1 check cal={np.allclose(proba_cal.sum(axis=1)[:5], 1.0, atol=1e-6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e90ac6",
   "metadata": {},
   "source": [
    "## 5. Trading Signal Generation\n",
    "\n",
    "### Advanced Signal Pipeline with Bandit Strategy Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74542768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandit allocator: class and backtest function (tri-state side + flat fallback)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class SimpleThompsonBandit:\n",
    "    def __init__(self, n_arms: int):\n",
    "        self.counts = np.zeros(n_arms, dtype=float)\n",
    "        self.means = np.zeros(n_arms, dtype=float)\n",
    "        self.vars = np.ones(n_arms, dtype=float)\n",
    "\n",
    "    def select(self, eligible_mask: np.ndarray) -> int:\n",
    "        masked_means = self.means.copy()\n",
    "        masked_vars = self.vars.copy()\n",
    "        masked_means[~eligible_mask] = -np.inf\n",
    "        masked_vars[~eligible_mask] = 1.0\n",
    "        samples = np.where(eligible_mask, np.random.normal(masked_means, np.sqrt(masked_vars + 1e-9)), -np.inf)\n",
    "        return int(np.argmax(samples))\n",
    "\n",
    "    def update(self, arm: int, reward: float):\n",
    "        c = self.counts[arm] + 1.0\n",
    "        mu = self.means[arm]\n",
    "        new_mu = mu + (reward - mu) / c\n",
    "        delta = reward - mu\n",
    "        delta2 = reward - new_mu\n",
    "        var = self.vars[arm] + (delta * delta2 - self.vars[arm]) / c\n",
    "        self.counts[arm] = c\n",
    "        self.means[arm] = new_mu\n",
    "        self.vars[arm] = max(var, 1e-6)\n",
    "\n",
    "def _compute_sortino(returns: pd.Series, annualizer: float) -> float:\n",
    "    if returns is None or returns.empty:\n",
    "        return np.nan\n",
    "    downside = returns[returns < 0]\n",
    "    denom = downside.std()\n",
    "    if denom == 0 or np.isnan(denom):\n",
    "        return np.nan\n",
    "    mu = returns.mean()\n",
    "    return float(annualizer * mu / denom)\n",
    "\n",
    "\n",
    "def _compute_turnover(pos_series: pd.Series) -> float:\n",
    "    if pos_series is None or pos_series.empty:\n",
    "        return np.nan\n",
    "    return float(np.abs(pos_series.diff().fillna(0.0)).sum())\n",
    "\n",
    "\n",
    "def _compute_hit_rate(trades_df: pd.DataFrame) -> float:\n",
    "    if trades_df is None or trades_df.empty:\n",
    "        return np.nan\n",
    "    if 'pnl_$' not in trades_df.columns:\n",
    "        return np.nan\n",
    "    wins = (trades_df['pnl_$'] > 0).sum()\n",
    "    return float(wins / max(len(trades_df), 1))\n",
    "\n",
    "\n",
    "def run_allocator_backtest(\n",
    "    bt_df: pd.DataFrame,\n",
    "    arm_signals: np.ndarray,\n",
    "    arm_eligible: np.ndarray,\n",
    "    adv_series: pd.Series,\n",
    "    cooldown_bars: int,\n",
    "    cost_bp: float,\n",
    "    impact_k: float,\n",
    "    side_eps_vec: Optional[np.ndarray] = None,\n",
    "    eps: float = 1e-12\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, dict]:\n",
    "    price_series = bt_df['close'] if 'close' in bt_df.columns else bt_df.iloc[:, 0]\n",
    "    price = price_series.to_numpy(dtype=float)\n",
    "    n = len(price)\n",
    "\n",
    "    rets = np.zeros_like(price, dtype=float)\n",
    "    denom = np.where(price[:-1] == 0, 1.0, price[:-1])\n",
    "    rets[1:] = (price[1:] - price[:-1]) / denom\n",
    "\n",
    "    adv_valid = isinstance(adv_series, pd.Series) and not adv_series.dropna().empty\n",
    "    adv_arr = adv_series.reindex(bt_df.index).to_numpy() if adv_valid else np.ones(n, dtype=float)\n",
    "    impact_k_eff = impact_k if adv_valid else 0.0\n",
    "\n",
    "    bandit = SimpleThompsonBandit(n_arms=arm_signals.shape[1])\n",
    "\n",
    "    pos = 0.0\n",
    "    pos_smooth = 0.0\n",
    "    last_flip_idx = -10**9\n",
    "\n",
    "    records_eq = []\n",
    "    records_tr = []\n",
    "    records_bu = []\n",
    "\n",
    "    cum_equity = 1.0\n",
    "    equity_series = np.ones(n, dtype=float)\n",
    "\n",
    "    sigma_target = float(globals().get('SIGMA_TARGET', 0.20))\n",
    "    pos_max = float(globals().get('POS_MAX', 1.0))\n",
    "    dd_stop = float(globals().get('DD_STOP', 0.05))\n",
    "    latency_k = int(globals().get('LATENCY_BARS', 0))\n",
    "    slippage_bps = float(globals().get('SLIPPAGE_BPS', 0.0))\n",
    "    cost_convention = str(globals().get('COST_CONVENTION', 'per_transition'))\n",
    "    smooth_beta = float(globals().get('SMOOTH_BETA', 0.0))\n",
    "\n",
    "    vol_window = 50\n",
    "    rets_series = pd.Series(rets, index=bt_df.index)\n",
    "    realized_vol = rets_series.rolling(vol_window, min_periods=1).std().fillna(0.0)\n",
    "\n",
    "    warmup_count = rets_series.expanding().count()\n",
    "    warmup_done = (warmup_count >= vol_window).astype(float)\n",
    "\n",
    "    from collections import deque\n",
    "    exec_pos_buffer = deque([0.0] * max(1, latency_k + 1), maxlen=max(1, latency_k + 1))\n",
    "\n",
    "    last_exec_pos = 0.0\n",
    "\n",
    "    for t in range(n):\n",
    "        elig = arm_eligible[t] if t < len(arm_eligible) else np.array([False]*arm_signals.shape[1])\n",
    "        desired_side = pos\n",
    "        chosen = None\n",
    "\n",
    "        if np.any(elig):\n",
    "            chosen = bandit.select(elig)\n",
    "            raw_val = float(arm_signals[t, chosen])\n",
    "            th = float(side_eps_vec[chosen]) if (side_eps_vec is not None) else 0.0\n",
    "            if abs(raw_val) < th:\n",
    "                desired_side = 0.0\n",
    "            else:\n",
    "                desired_side = 1.0 if raw_val > 0 else -1.0\n",
    "        else:\n",
    "            if (t - last_flip_idx) >= cooldown_bars:\n",
    "                desired_side = 0.0\n",
    "\n",
    "        rv = float(realized_vol.iloc[t])\n",
    "        scaler_raw = (sigma_target / max(rv, 1e-8)) if sigma_target > 0 else 1.0\n",
    "        scaler = float(warmup_done.iloc[t]) * scaler_raw + (1.0 - float(warmup_done.iloc[t])) * 1.0\n",
    "        target_pos = np.clip(desired_side * scaler, -pos_max, pos_max)\n",
    "\n",
    "        if smooth_beta > 0.0:\n",
    "            pos_smooth = smooth_beta * target_pos + (1.0 - smooth_beta) * pos_smooth\n",
    "            target_pos = np.clip(pos_smooth, -pos_max, pos_max)\n",
    "\n",
    "        exec_pos_buffer.append(target_pos)\n",
    "        exec_pos = exec_pos_buffer[0] if latency_k > 0 else target_pos\n",
    "\n",
    "        cost_bps = 0.0\n",
    "        if exec_pos != last_exec_pos:\n",
    "            delta = abs(exec_pos - last_exec_pos)\n",
    "            impact = (impact_k_eff * (delta / max(adv_arr[t], eps))) if impact_k_eff > 0 else 0.0\n",
    "            trans_cost = cost_bp + slippage_bps + impact\n",
    "            if cost_convention == 'per_roundtrip':\n",
    "                trans_cost *= 0.5\n",
    "            cost_bps = trans_cost\n",
    "            records_tr.append((bt_df.index[t], float(last_exec_pos), float(exec_pos), float(cost_bps)))\n",
    "            last_exec_pos = exec_pos\n",
    "            last_flip_idx = t\n",
    "\n",
    "        pnl = rets[t] * (exec_pos_buffer[0] if latency_k > 0 else exec_pos)\n",
    "        pnl -= (cost_bps / 10000.0) if cost_bps > 0 else 0.0\n",
    "        cum_equity *= (1.0 + pnl)\n",
    "        equity_series[t] = cum_equity\n",
    "        records_eq.append((bt_df.index[t], cum_equity))\n",
    "\n",
    "        if chosen is not None:\n",
    "            bandit.update(chosen, pnl)\n",
    "            records_bu.append((bt_df.index[t], int(chosen), float(pnl)))\n",
    "\n",
    "        if dd_stop > 0:\n",
    "            peak = np.max(equity_series[:t+1])\n",
    "            if peak > 0 and (cum_equity / peak - 1.0) < -dd_stop:\n",
    "                exec_pos_buffer.append(0.0)\n",
    "                last_exec_pos = 0.0\n",
    "\n",
    "    Eq = pd.DataFrame.from_records(records_eq, columns=['ts', 'equity']).set_index('ts')\n",
    "    Tr = pd.DataFrame.from_records(records_tr, columns=['ts', 'from_pos', 'to_pos', 'cost_bps'])\n",
    "    Bu = pd.DataFrame.from_records(records_bu, columns=['ts', 'chosen', 'reward'])\n",
    "\n",
    "    notional = float(globals().get('NOTIONAL', 100000.0))\n",
    "    if not Tr.empty:\n",
    "        Tr['ts'] = pd.to_datetime(Tr['ts'], errors='coerce')\n",
    "        Tr = Tr.sort_values('ts').reset_index(drop=True)\n",
    "        eq_series2 = Eq['equity'].copy()\n",
    "        eq_series2.index = pd.to_datetime(eq_series2.index, errors='coerce')\n",
    "        eq_series2 = eq_series2.sort_index()\n",
    "\n",
    "        eq_df_reset = eq_series2.reset_index()\n",
    "        eq_df_reset.columns = ['ts','equity']\n",
    "        eq_prev_reset = eq_df_reset[['ts']].copy()\n",
    "        eq_prev_reset['equity_prev'] = eq_df_reset['equity'].shift(1).values\n",
    "\n",
    "        Tr = pd.merge_asof(Tr, eq_df_reset, on='ts', direction='backward')\n",
    "        Tr = pd.merge_asof(Tr, eq_prev_reset, on='ts', direction='backward')\n",
    "        Tr['equity_prev'] = Tr['equity_prev'].fillna(1.0)\n",
    "\n",
    "        Tr['cost_$'] = (Tr['equity_prev'] * notional) * (Tr['cost_bps'] / 10000.0)\n",
    "\n",
    "        next_ts = Tr['ts'].shift(-1)\n",
    "        next_ts.iloc[-1] = eq_df_reset['ts'].iloc[-1]\n",
    "        next_df = pd.DataFrame({'ts': next_ts}).sort_values('ts').reset_index(drop=True)\n",
    "        next_df = pd.merge_asof(next_df, eq_df_reset, on='ts', direction='backward')\n",
    "        Tr['equity_next'] = next_df['equity'].values\n",
    "\n",
    "        Tr['pnl_$'] = (Tr['equity_next'] - Tr['equity']) * notional - Tr['cost_$']\n",
    "        Tr['cum_pnl_$'] = Tr['pnl_$'].cumsum()\n",
    "\n",
    "    eq_rets = Eq['equity'].pct_change().dropna()\n",
    "    annualizer = float(globals().get('ANNUALIZER', 1.0))\n",
    "    sharpe = float(annualizer * eq_rets.mean() / (eq_rets.std() if eq_rets.std() != 0 else np.nan)) if len(eq_rets) else np.nan\n",
    "    sortino = _compute_sortino(eq_rets, annualizer)\n",
    "    maxdd = float(-(Eq['equity'] / Eq['equity'].cummax() - 1.0).min()) if not Eq.empty else np.nan\n",
    "    turnover = _compute_turnover(pd.Series(Tr['to_pos'].astype(float))) if not Tr.empty else 0.0\n",
    "    hit_rate = _compute_hit_rate(Tr) if not Tr.empty else np.nan\n",
    "\n",
    "    assumptions = {\n",
    "        'vol_warmup': 'no_target_until_window',\n",
    "        'adv_fallback': (not adv_valid),\n",
    "        'impact_k_effective': float(impact_k_eff),\n",
    "        'equity_prev_first_trade': 1.0,\n",
    "    }\n",
    "\n",
    "    metrics = {\n",
    "        'final_equity': float(Eq['equity'].iloc[-1]) if len(Eq) else 1.0,\n",
    "        'n_trades': int(len(Tr)),\n",
    "        'sharpe': sharpe,\n",
    "        'sortino': sortino,\n",
    "        'maxDD': maxdd,\n",
    "        'turnover': float(turnover),\n",
    "        'hit_rate': float(hit_rate) if not np.isnan(hit_rate) else np.nan,\n",
    "        'assumptions': assumptions,\n",
    "    }\n",
    "    return Eq, Tr, Bu, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a484c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind backtest data (bt) and features (X_bt) to the test holdout deterministically\n",
    "# Also provide a consistent ADV20 series aligned by timestamp, and enforce calibrated-only usage.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Use the three-way split test holdout as the canonical backtest set\n",
    "assert 'test_data' in globals() and isinstance(test_data, pd.DataFrame) and len(test_data) > 0, \"test_data not available; run the training/split cells first.\"\n",
    "assert 'X_test_model_clean' in globals() and isinstance(X_test_model_clean, pd.DataFrame), \"X_test_model_clean missing; build features and splits first.\"\n",
    "\n",
    "# Backtest price frame with datetime index for stable alignment\n",
    "bt = test_data[['timestamp', 'close']].copy()\n",
    "bt = bt.dropna(subset=['timestamp', 'close'])\n",
    "bt['timestamp'] = pd.to_datetime(bt['timestamp'], errors='coerce')\n",
    "bt = bt.dropna(subset=['timestamp']).set_index('timestamp').sort_index()\n",
    "\n",
    "# Feature matrix for model probabilities on the same rows\n",
    "X_bt = X_test_model_clean.copy()\n",
    "\n",
    "# ADV20 series, prefer cohort-derived adv20_by_ts, else compute from df\n",
    "if 'adv20_by_ts' in globals() and isinstance(adv20_by_ts, pd.Series) and len(adv20_by_ts) > 0:\n",
    "    adv20 = adv20_by_ts.copy()\n",
    "elif 'df' in globals() and {'timestamp','close','volume'}.issubset(df.columns):\n",
    "    tmp = df[['timestamp','close','volume']].copy()\n",
    "    tmp['timestamp'] = pd.to_datetime(tmp['timestamp'], errors='coerce')\n",
    "    tmp = tmp.dropna(subset=['timestamp']).sort_values('timestamp')\n",
    "    tmp['adv20'] = (tmp['close'] * tmp['volume']).rolling(20, min_periods=1).mean()\n",
    "    adv20 = pd.Series(tmp['adv20'].values, index=tmp['timestamp'])\n",
    "else:\n",
    "    adv20 = pd.Series(1.0, index=bt.index)\n",
    "\n",
    "# Provide adv20_by_ts alias for downstream code\n",
    "adv20_by_ts = adv20\n",
    "\n",
    "# Class index convention: 0=down, 2=up\n",
    "up_idx = 2\n",
    "down_idx = 0\n",
    "\n",
    "# Force calibrated-only path downstream\n",
    "USE_CALIBRATED = True\n",
    "assert 'meta_class_calibrator' in globals() and getattr(meta_class_calibrator, 'is_fitted', False), \"Calibrator not fitted; run calibration cells first.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a47c07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated arm signals and eligibility (allocator-only)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Resolve backtest price index from bt set earlier\n",
    "assert 'bt' in globals() and isinstance(bt, pd.DataFrame) and 'close' in bt.columns, \"bt not bound; run the binding cell above.\"\n",
    "px = bt['close'].astype(float)\n",
    "idx_bt = px.index\n",
    "\n",
    "# Cohort signals (default to 0 if missing), aligned to bt index\n",
    "S_top_series = (S_top.reindex(idx_bt).fillna(0.0) if 'S_top' in globals() else pd.Series(0.0, index=idx_bt))\n",
    "S_bot_series = (S_bot.reindex(idx_bt).fillna(0.0) if 'S_bot' in globals() else pd.Series(0.0, index=idx_bt))\n",
    "\n",
    "# Mood signal from price (tanh blend of 3-bar momentum and EMA-based mean reversion)\n",
    "mom = px.pct_change(3)\n",
    "mr = -(px / px.rolling(50, min_periods=10).mean() - 1.0)\n",
    "S_mood_series = np.tanh(0.6*mom.fillna(0.0) + 0.4*mr.fillna(0.0))\n",
    "S_mood_series = pd.Series(S_mood_series, index=idx_bt).fillna(0.0)\n",
    "\n",
    "# Model directional score from calibrated probabilities on X_bt (enforced above)\n",
    "assert 'X_bt' in globals() and len(X_bt) == len(idx_bt), \"X_bt not bound to bt rows.\"\n",
    "assert 'USE_CALIBRATED' in globals() and USE_CALIBRATED, \"Calibrated-only required.\"\n",
    "P_bt = meta_class_calibrator.predict_proba(X_bt)\n",
    "up_idx_local = globals().get('up_idx', 2)\n",
    "down_idx_local = globals().get('down_idx', 0)\n",
    "p_up_bt = P_bt[:, up_idx_local]\n",
    "p_dn_bt = P_bt[:, down_idx_local]\n",
    "S_model_series = pd.Series(p_up_bt - p_dn_bt, index=idx_bt).fillna(0.0)\n",
    "\n",
    "# Optional: BMA classifier calibrated probabilities → additional arm\n",
    "if 'bma_class_calibrator' in globals() and bma_class_calibrator is not None and getattr(bma_class_calibrator, 'is_fitted', False):\n",
    "    try:\n",
    "        P_bma_bt = bma_class_calibrator.predict_proba(X_bt)\n",
    "        p_up_bma = P_bma_bt[:, up_idx_local]\n",
    "        p_dn_bma = P_bma_bt[:, down_idx_local]\n",
    "        S_bma_series = pd.Series(p_up_bma - p_dn_bma, index=idx_bt).fillna(0.0)\n",
    "    except Exception:\n",
    "        S_bma_series = pd.Series(0.0, index=idx_bt)\n",
    "else:\n",
    "    S_bma_series = pd.Series(0.0, index=idx_bt)\n",
    "\n",
    "# Assemble signals DataFrame\n",
    "arm_signals_df = pd.DataFrame({\n",
    "    'S_top': S_top_series,\n",
    "    'S_bot': S_bot_series,\n",
    "    'S_mood': S_mood_series,\n",
    "    'S_model': S_model_series,\n",
    "    'S_bma_model': S_bma_series,\n",
    "}, index=idx_bt)\n",
    "\n",
    "# Eligibility thresholds (with safe defaults)\n",
    "S_MIN = globals().get('S_MIN', 0.25)\n",
    "M_MIN = globals().get('M_MIN', 0.15)\n",
    "CONF_MIN = globals().get('CONF_MIN', 0.60)\n",
    "ALPHA_MIN = globals().get('ALPHA_MIN', 0.10)\n",
    "\n",
    "arm_eligible_df = pd.DataFrame(False, index=idx_bt, columns=['pros','amateurs','mood','model','bma_model'])\n",
    "arm_eligible_df['pros'] = arm_signals_df['S_top'].abs() >= S_MIN\n",
    "arm_eligible_df['amateurs'] = arm_signals_df['S_bot'].abs() >= S_MIN\n",
    "arm_eligible_df['mood'] = arm_signals_df['S_mood'].abs() >= M_MIN\n",
    "\n",
    "# Confidence/alpha for model arms\n",
    "conf_bt = np.maximum(p_up_bt, p_dn_bt)\n",
    "alpha_bt = np.abs(p_up_bt - p_dn_bt)\n",
    "arm_eligible_df['model'] = (conf_bt >= CONF_MIN) & (alpha_bt >= ALPHA_MIN)\n",
    "\n",
    "# For BMA arm, compute from its own probs if available else mirror model eligibility\n",
    "if 'P_bma_bt' in locals():\n",
    "    conf_bma = np.maximum(p_up_bma, p_dn_bma)\n",
    "    alpha_bma = np.abs(p_up_bma - p_dn_bma)\n",
    "    arm_eligible_df['bma_model'] = (conf_bma >= CONF_MIN) & (alpha_bma >= ALPHA_MIN)\n",
    "else:\n",
    "    arm_eligible_df['bma_model'] = arm_eligible_df['model']\n",
    "\n",
    "# Export to ndarray for backtest\n",
    "arm_signals = arm_signals_df[['S_top','S_bot','S_mood','S_model','S_bma_model']].values\n",
    "arm_eligible = arm_eligible_df[['pros','amateurs','mood','model','bma_model']].values\n",
    "arm_names = ['pros','amateurs','mood','model','bma_model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32530ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics helpers + single-run wrapper (flip_map default enabled)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Optional\n",
    "\n",
    "ANNUALIZER = np.sqrt(365*24*12)\n",
    "\n",
    "# Default flips learned from diagnostics (can be overridden per-call)\n",
    "DEFAULT_FLIP_MAP = {\"Mood\": True, \"Model\": True}\n",
    "\n",
    "def sharpe_ratio(returns: pd.Series) -> float:\n",
    "    if returns is None or returns.empty:\n",
    "        return np.nan\n",
    "    mu = returns.mean()\n",
    "    sd = returns.std()\n",
    "    if sd == 0 or np.isnan(sd):\n",
    "        return np.nan\n",
    "    return float(ANNUALIZER * mu / sd)\n",
    "\n",
    "\n",
    "def sortino_ratio(returns: pd.Series) -> float:\n",
    "    if returns is None or returns.empty:\n",
    "        return np.nan\n",
    "    downside = returns[returns < 0]\n",
    "    denom = downside.std()\n",
    "    if denom == 0 or np.isnan(denom):\n",
    "        return np.nan\n",
    "    mu = returns.mean()\n",
    "    return float(ANNUALIZER * mu / denom)\n",
    "\n",
    "\n",
    "def max_drawdown(equity: pd.Series) -> float:\n",
    "    if equity is None or equity.empty:\n",
    "        return np.nan\n",
    "    dd = (equity / equity.cummax() - 1.0).min()\n",
    "    return float(-dd) if pd.notna(dd) else np.nan\n",
    "\n",
    "\n",
    "def turnover_from_trades(trades: pd.DataFrame) -> float:\n",
    "    if trades is None or trades.empty:\n",
    "        return 0.0\n",
    "    return float(np.abs(trades['to_pos'].astype(float) - trades['from_pos'].astype(float)).sum())\n",
    "\n",
    "\n",
    "def hit_rate_from_trades(trades: pd.DataFrame) -> float:\n",
    "    if trades is None or trades.empty or 'pnl_$' not in trades.columns:\n",
    "        return np.nan\n",
    "    return float((trades['pnl_$'] > 0).sum() / max(len(trades), 1))\n",
    "\n",
    "\n",
    "def _make_side_eps_vec(columns: list, cfg: Dict) -> np.ndarray:\n",
    "    \"\"\"Map per-arm thresholds dynamically based on arm names.\"\"\"\n",
    "    S_MIN_val = float(cfg.get('S_MIN', globals().get('S_MIN', 0.25)))\n",
    "    M_MIN_val = float(cfg.get('M_MIN', globals().get('M_MIN', 0.15)))\n",
    "    ALPHA_MIN_val = float(cfg.get('ALPHA_MIN', globals().get('ALPHA_MIN', 0.10)))\n",
    "    vec = []\n",
    "    for name in columns:\n",
    "        lname = name.lower()\n",
    "        if 'top' in lname or 'bot' in lname:\n",
    "            vec.append(S_MIN_val)\n",
    "        elif 'mood' in lname:\n",
    "            vec.append(M_MIN_val)\n",
    "        else:  # model-like arms\n",
    "            vec.append(ALPHA_MIN_val)\n",
    "    return np.array(vec, dtype=float)\n",
    "\n",
    "\n",
    "def single_run_metrics(config: Dict, flip_map: Optional[Dict[str, bool]] = None) -> Dict:\n",
    "    if flip_map is None:\n",
    "        flip_map = DEFAULT_FLIP_MAP\n",
    "\n",
    "    S_MIN_val = config.get('S_MIN', S_MIN)\n",
    "    M_MIN_val = config.get('M_MIN', M_MIN)\n",
    "    CONF_MIN_val = config.get('CONF_MIN', CONF_MIN)\n",
    "    ALPHA_MIN_val = config.get('ALPHA_MIN', ALPHA_MIN)\n",
    "    COOLDOWN_val = config.get('COOLDOWN', COOLDOWN)\n",
    "    COST_BP_val = config.get('COST_BP', COST_BP)\n",
    "    IMPACT_K_val = config.get('IMPACT_K', IMPACT_K)\n",
    "\n",
    "    signals_df = arm_signals_df\n",
    "    if flip_map:\n",
    "        signals_df = apply_polarity_flip(signals_df, flip_map)\n",
    "\n",
    "    side_eps_vec = _make_side_eps_vec(list(signals_df.columns), config)\n",
    "\n",
    "    eq_df, tl_df, _, _metrics = run_allocator_backtest(\n",
    "        bt_df=bt,\n",
    "        arm_signals=signals_df.values,\n",
    "        arm_eligible=arm_eligible,\n",
    "        adv_series=adv20_by_ts,\n",
    "        cooldown_bars=COOLDOWN_val,\n",
    "        cost_bp=COST_BP_val,\n",
    "        impact_k=IMPACT_K_val,\n",
    "        side_eps_vec=side_eps_vec,\n",
    "        eps=1e-12,\n",
    "    )\n",
    "\n",
    "    rets = eq_df['equity'].pct_change().dropna() if eq_df is not None else pd.Series(dtype=float)\n",
    "    out = dict(\n",
    "        final_equity=float(eq_df['equity'].iloc[-1]) if eq_df is not None else np.nan,\n",
    "        n_trades=int(tl_df.shape[0]) if tl_df is not None else 0,\n",
    "        sharpe=float(ANNUALIZER * rets.mean() / (rets.std() if rets.std() != 0 else np.nan)) if len(rets) else np.nan,\n",
    "        sortino=sortino_ratio(rets),\n",
    "        maxDD=max_drawdown(eq_df['equity']) if eq_df is not None else np.nan,\n",
    "        turnover=turnover_from_trades(tl_df),\n",
    "        hit_rate=hit_rate_from_trades(tl_df),\n",
    "    )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499df88",
   "metadata": {},
   "source": [
    "## Extended gating and grid\n",
    "\n",
    "We add:\n",
    "- Consensus/advantage gating on top of per-arm thresholds\n",
    "- Gross vs net attribution (run with/without costs)\n",
    "- Extended grid over COOLDOWN and CONF_MIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9f22d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gating utilities: consensus and advantage\n",
    "from typing import Dict, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_consensus_and_advantage(\n",
    "    signals_df: pd.DataFrame,\n",
    "    weights: Optional[Dict[str, float]] = None\n",
    ") -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Contract:\n",
    "    - Input: signals_df with columns named for each arm; values in [-1, 1] directional scores.\n",
    "    - Output:\n",
    "        consensus: integer in {-3,-2,-1,0,1,2,3,4,5} (sum of signed votes across arms)\n",
    "        advantage: float >= 0 representing margin between top-1 and top-2 weighted absolute signal strengths\n",
    "    Notes:\n",
    "    - Vote is sign(signal) with 0 treated as 0 (abstain)\n",
    "    - Advantage compares |w_i * s_i| magnitudes\n",
    "    - NaNs are treated as 0 for both vote and magnitude\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = {c: 1.0 for c in signals_df.columns}\n",
    "\n",
    "    vote = np.sign(signals_df.clip(-1, 1)).fillna(0.0)\n",
    "    weighted = pd.DataFrame({c: weights.get(c, 1.0) * signals_df[c].abs().fillna(0.0) for c in signals_df.columns})\n",
    "\n",
    "    consensus = vote.sum(axis=1)\n",
    "\n",
    "    vals = weighted.values\n",
    "    if vals.shape[1] == 0:\n",
    "        advantage = pd.Series(0.0, index=signals_df.index)\n",
    "    else:\n",
    "        sorted_mag = np.sort(vals, axis=1)\n",
    "        top1 = sorted_mag[:, -1]\n",
    "        top2 = sorted_mag[:, -2] if weighted.shape[1] >= 2 else np.zeros(len(signals_df))\n",
    "        advantage = pd.Series(top1 - top2, index=signals_df.index)\n",
    "\n",
    "    return consensus.astype(float), advantage.astype(float)\n",
    "\n",
    "\n",
    "def apply_polarity_flip(signals_df: pd.DataFrame, flip_map: Optional[Dict[str, bool]] = None) -> pd.DataFrame:\n",
    "    if not flip_map:\n",
    "        return signals_df\n",
    "    out = signals_df.copy()\n",
    "    for k, v in flip_map.items():\n",
    "        if v:\n",
    "            for c in out.columns:\n",
    "                if k.lower() in c.lower():\n",
    "                    out[c] = -out[c]\n",
    "    return out\n",
    "\n",
    "\n",
    "def apply_gating(\n",
    "    signals_df: pd.DataFrame,\n",
    "    eligible_df: pd.DataFrame,\n",
    "    consensus_min: int = 2,\n",
    "    advantage_min: float = 0.0\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    consensus, advantage = compute_consensus_and_advantage(signals_df)\n",
    "    gate_mask = (consensus.abs() >= int(consensus_min)) & (advantage.fillna(0.0) >= float(advantage_min))\n",
    "    gated_eligible = eligible_df.copy()\n",
    "    for c in eligible_df.columns:\n",
    "        gated_eligible[c] = eligible_df[c] & gate_mask\n",
    "    return signals_df.copy(), gated_eligible\n",
    "\n",
    "\n",
    "def with_gating(signals_df: pd.DataFrame, eligible_df: pd.DataFrame, gate_cfg: Optional[Dict] = None):\n",
    "    if not gate_cfg:\n",
    "        return signals_df, eligible_df\n",
    "    return apply_gating(\n",
    "        signals_df,\n",
    "        eligible_df,\n",
    "        consensus_min=gate_cfg.get('consensus_min', 2),\n",
    "        advantage_min=float(gate_cfg.get('advantage_min', 0.0))\n",
    "    )\n",
    "\n",
    "\n",
    "def _flip_signals(df: pd.DataFrame, fmap: Optional[Dict[str, bool]]) -> pd.DataFrame:\n",
    "    if not fmap:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    for k, v in fmap.items():\n",
    "        if v:\n",
    "            for c in out.columns:\n",
    "                if k.lower() in c.lower():\n",
    "                    out[c] = -out[c]\n",
    "    return out\n",
    "\n",
    "\n",
    "def _make_side_eps_vec(columns: list, cfg: Dict) -> np.ndarray:\n",
    "    S_MIN_val = float(cfg.get('S_MIN', globals().get('S_MIN', 0.25)))\n",
    "    M_MIN_val = float(cfg.get('M_MIN', globals().get('M_MIN', 0.15)))\n",
    "    ALPHA_MIN_val = float(cfg.get('ALPHA_MIN', globals().get('ALPHA_MIN', 0.10)))\n",
    "    vec = []\n",
    "    for name in columns:\n",
    "        lname = name.lower()\n",
    "        if 'top' in lname or 'bot' in lname:\n",
    "            vec.append(S_MIN_val)\n",
    "        elif 'mood' in lname:\n",
    "            vec.append(M_MIN_val)\n",
    "        else:\n",
    "            vec.append(ALPHA_MIN_val)\n",
    "    return np.array(vec, dtype=float)\n",
    "\n",
    "\n",
    "def run_allocator_once(cfg: Dict, gate_cfg: Optional[Dict] = None) -> Dict:\n",
    "    if 'set_config_and_rebuild' in globals():\n",
    "        try:\n",
    "            set_config_and_rebuild(\n",
    "                S_MIN_val=cfg.get('S_MIN', globals().get('S_MIN', 0.25)),\n",
    "                M_MIN_val=cfg.get('M_MIN', globals().get('M_MIN', 0.15)),\n",
    "                CONF_MIN_val=cfg.get('CONF_MIN', globals().get('CONF_MIN', 0.60)),\n",
    "                ALPHA_MIN_val=cfg.get('ALPHA_MIN', globals().get('ALPHA_MIN', 0.10)),\n",
    "                IMPACT_K_val=cfg.get('IMPACT_K', globals().get('IMPACT_K', 0.0)),\n",
    "            )\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "    base_signals_df = globals().get('arm_signals_df', None)\n",
    "    base_eligible_df = globals().get('arm_eligible_df', None)\n",
    "    if base_signals_df is None or base_eligible_df is None:\n",
    "        raise RuntimeError('arm_signals_df or arm_eligible_df not available')\n",
    "\n",
    "    fmap = cfg.get('flip_map', globals().get('DEFAULT_FLIP_MAP', {}))\n",
    "\n",
    "    sig_for_gate = _flip_signals(base_signals_df, fmap)\n",
    "\n",
    "    if gate_cfg:\n",
    "        _, gated_elig_df = with_gating(sig_for_gate, base_eligible_df, gate_cfg)\n",
    "    else:\n",
    "        gated_elig_df = base_eligible_df\n",
    "\n",
    "    COOLDOWN_val = int(cfg.get('COOLDOWN', globals().get('COOLDOWN', 1)))\n",
    "    COST_BP_val = float(cfg.get('COST_BP', globals().get('COST_BP', 0.0)))\n",
    "    IMPACT_K_val = float(cfg.get('IMPACT_K', globals().get('IMPACT_K', 0.0)))\n",
    "\n",
    "    side_eps_vec = _make_side_eps_vec(list(sig_for_gate.columns), cfg)\n",
    "\n",
    "    Eq, Tr, Bu, met = run_allocator_backtest(\n",
    "        bt_df=globals().get('bt'),\n",
    "        arm_signals=sig_for_gate.values,\n",
    "        arm_eligible=gated_elig_df.values,\n",
    "        adv_series=globals().get('adv20_by_ts'),\n",
    "        cooldown_bars=COOLDOWN_val,\n",
    "        cost_bp=COST_BP_val,\n",
    "        impact_k=IMPACT_K_val,\n",
    "        side_eps_vec=side_eps_vec,\n",
    "        eps=1e-12,\n",
    "    )\n",
    "\n",
    "    rets = Eq['equity'].pct_change().dropna() if Eq is not None else pd.Series(dtype=float)\n",
    "    out = dict(\n",
    "        final_equity=float(Eq['equity'].iloc[-1]) if Eq is not None else np.nan,\n",
    "        n_trades=int(Tr.shape[0]) if Tr is not None else 0,\n",
    "        sharpe=float(globals().get('ANNUALIZER', 1.0) * rets.mean() / (rets.std() if rets.std() != 0 else np.nan)) if len(rets) else np.nan,\n",
    "        sortino=_compute_sortino(rets, float(globals().get('ANNUALIZER', 1.0))),\n",
    "        maxDD=float(-(Eq['equity'] / Eq['equity'].cummax() - 1.0).min()) if Eq is not None and not Eq.empty else np.nan,\n",
    "        turnover=float(np.abs(Tr['to_pos'].astype(float) - Tr['from_pos'].astype(float)).sum()) if Tr is not None and not Tr.empty else 0.0,\n",
    "        hit_rate=float((Tr['pnl_$'] > 0).sum() / max(len(Tr), 1)) if Tr is not None and 'pnl_$' in Tr.columns else np.nan,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def run_backtest_net_and_gross(cfg: Dict, gate_cfg: Optional[Dict] = None) -> Dict:\n",
    "    net_res = run_allocator_once(cfg, gate_cfg=gate_cfg)\n",
    "\n",
    "    gross_cfg = dict(cfg)\n",
    "    gross_cfg['COST_BP'] = 0.0\n",
    "    gross_cfg['IMPACT_K'] = 0.0\n",
    "    gross_res = run_allocator_once(gross_cfg, gate_cfg=gate_cfg)\n",
    "\n",
    "    out = {\n",
    "        **{f\"net_{k}\": v for k, v in net_res.items()},\n",
    "        **{f\"gross_{k}\": v for k, v in gross_res.items()},\n",
    "    }\n",
    "    try:\n",
    "        out['cost_drag'] = (out.get('gross_final_equity', np.nan) - out.get('net_final_equity', np.nan))\n",
    "        denom_n = max(int(out.get('net_n_trades', 0)), 1)\n",
    "        denom_g = max(int(out.get('gross_n_trades', 0)), 1)\n",
    "        out['net_per_trade'] = out.get('net_final_equity', np.nan) / denom_n\n",
    "        out['gross_per_trade'] = out.get('gross_final_equity', np.nan) / denom_g\n",
    "    except Exception:\n",
    "        pass\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fad6d7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COOLDOWN</th>\n",
       "      <th>CONF_MIN</th>\n",
       "      <th>GATE</th>\n",
       "      <th>FEE_BPS</th>\n",
       "      <th>SIGMA_TARGET</th>\n",
       "      <th>net_final_equity</th>\n",
       "      <th>net_n_trades</th>\n",
       "      <th>net_sharpe</th>\n",
       "      <th>net_sortino</th>\n",
       "      <th>net_maxDD</th>\n",
       "      <th>...</th>\n",
       "      <th>gross_final_equity</th>\n",
       "      <th>gross_n_trades</th>\n",
       "      <th>gross_sharpe</th>\n",
       "      <th>gross_sortino</th>\n",
       "      <th>gross_maxDD</th>\n",
       "      <th>gross_turnover</th>\n",
       "      <th>gross_hit_rate</th>\n",
       "      <th>cost_drag</th>\n",
       "      <th>net_per_trade</th>\n",
       "      <th>gross_per_trade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>none</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons2_adv0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons3_adv0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons2_adv0p02</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>none</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons2_adv0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons3_adv0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons2_adv0p02</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>none</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons2_adv0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons3_adv0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons2_adv0p02</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>none</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons2_adv0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons3_adv0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons2_adv0p02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>none</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons2_adv0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons3_adv0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>cons2_adv0p02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    COOLDOWN  CONF_MIN           GATE  FEE_BPS  SIGMA_TARGET  \\\n",
       "0          1       0.6           none      3.0           0.1   \n",
       "1          1       0.6     cons2_adv0      3.0           0.1   \n",
       "2          1       0.6     cons3_adv0      3.0           0.1   \n",
       "3          1       0.6  cons2_adv0p02      3.0           0.1   \n",
       "4          1       0.6           none      3.0           0.2   \n",
       "5          1       0.6     cons2_adv0      3.0           0.2   \n",
       "6          1       0.6     cons3_adv0      3.0           0.2   \n",
       "7          1       0.6  cons2_adv0p02      3.0           0.2   \n",
       "8          1       0.6           none      3.0           0.3   \n",
       "9          1       0.6     cons2_adv0      3.0           0.3   \n",
       "10         1       0.6     cons3_adv0      3.0           0.3   \n",
       "11         1       0.6  cons2_adv0p02      3.0           0.3   \n",
       "12         1       0.6           none      5.0           0.1   \n",
       "13         1       0.6     cons2_adv0      5.0           0.1   \n",
       "14         1       0.6     cons3_adv0      5.0           0.1   \n",
       "15         1       0.6  cons2_adv0p02      5.0           0.1   \n",
       "16         1       0.6           none      5.0           0.2   \n",
       "17         1       0.6     cons2_adv0      5.0           0.2   \n",
       "18         1       0.6     cons3_adv0      5.0           0.2   \n",
       "19         1       0.6  cons2_adv0p02      5.0           0.2   \n",
       "\n",
       "    net_final_equity  net_n_trades  net_sharpe  net_sortino  net_maxDD  ...  \\\n",
       "0                1.0             0         NaN          NaN       -0.0  ...   \n",
       "1                1.0             0         NaN          NaN       -0.0  ...   \n",
       "2                1.0             0         NaN          NaN       -0.0  ...   \n",
       "3                1.0             0         NaN          NaN       -0.0  ...   \n",
       "4                1.0             0         NaN          NaN       -0.0  ...   \n",
       "5                1.0             0         NaN          NaN       -0.0  ...   \n",
       "6                1.0             0         NaN          NaN       -0.0  ...   \n",
       "7                1.0             0         NaN          NaN       -0.0  ...   \n",
       "8                1.0             0         NaN          NaN       -0.0  ...   \n",
       "9                1.0             0         NaN          NaN       -0.0  ...   \n",
       "10               1.0             0         NaN          NaN       -0.0  ...   \n",
       "11               1.0             0         NaN          NaN       -0.0  ...   \n",
       "12               1.0             0         NaN          NaN       -0.0  ...   \n",
       "13               1.0             0         NaN          NaN       -0.0  ...   \n",
       "14               1.0             0         NaN          NaN       -0.0  ...   \n",
       "15               1.0             0         NaN          NaN       -0.0  ...   \n",
       "16               1.0             0         NaN          NaN       -0.0  ...   \n",
       "17               1.0             0         NaN          NaN       -0.0  ...   \n",
       "18               1.0             0         NaN          NaN       -0.0  ...   \n",
       "19               1.0             0         NaN          NaN       -0.0  ...   \n",
       "\n",
       "    gross_final_equity  gross_n_trades  gross_sharpe  gross_sortino  \\\n",
       "0                  1.0               0           NaN            NaN   \n",
       "1                  1.0               0           NaN            NaN   \n",
       "2                  1.0               0           NaN            NaN   \n",
       "3                  1.0               0           NaN            NaN   \n",
       "4                  1.0               0           NaN            NaN   \n",
       "5                  1.0               0           NaN            NaN   \n",
       "6                  1.0               0           NaN            NaN   \n",
       "7                  1.0               0           NaN            NaN   \n",
       "8                  1.0               0           NaN            NaN   \n",
       "9                  1.0               0           NaN            NaN   \n",
       "10                 1.0               0           NaN            NaN   \n",
       "11                 1.0               0           NaN            NaN   \n",
       "12                 1.0               0           NaN            NaN   \n",
       "13                 1.0               0           NaN            NaN   \n",
       "14                 1.0               0           NaN            NaN   \n",
       "15                 1.0               0           NaN            NaN   \n",
       "16                 1.0               0           NaN            NaN   \n",
       "17                 1.0               0           NaN            NaN   \n",
       "18                 1.0               0           NaN            NaN   \n",
       "19                 1.0               0           NaN            NaN   \n",
       "\n",
       "    gross_maxDD  gross_turnover  gross_hit_rate  cost_drag  net_per_trade  \\\n",
       "0          -0.0             0.0             NaN        0.0            1.0   \n",
       "1          -0.0             0.0             NaN        0.0            1.0   \n",
       "2          -0.0             0.0             NaN        0.0            1.0   \n",
       "3          -0.0             0.0             NaN        0.0            1.0   \n",
       "4          -0.0             0.0             NaN        0.0            1.0   \n",
       "5          -0.0             0.0             NaN        0.0            1.0   \n",
       "6          -0.0             0.0             NaN        0.0            1.0   \n",
       "7          -0.0             0.0             NaN        0.0            1.0   \n",
       "8          -0.0             0.0             NaN        0.0            1.0   \n",
       "9          -0.0             0.0             NaN        0.0            1.0   \n",
       "10         -0.0             0.0             NaN        0.0            1.0   \n",
       "11         -0.0             0.0             NaN        0.0            1.0   \n",
       "12         -0.0             0.0             NaN        0.0            1.0   \n",
       "13         -0.0             0.0             NaN        0.0            1.0   \n",
       "14         -0.0             0.0             NaN        0.0            1.0   \n",
       "15         -0.0             0.0             NaN        0.0            1.0   \n",
       "16         -0.0             0.0             NaN        0.0            1.0   \n",
       "17         -0.0             0.0             NaN        0.0            1.0   \n",
       "18         -0.0             0.0             NaN        0.0            1.0   \n",
       "19         -0.0             0.0             NaN        0.0            1.0   \n",
       "\n",
       "    gross_per_trade  \n",
       "0               1.0  \n",
       "1               1.0  \n",
       "2               1.0  \n",
       "3               1.0  \n",
       "4               1.0  \n",
       "5               1.0  \n",
       "6               1.0  \n",
       "7               1.0  \n",
       "8               1.0  \n",
       "9               1.0  \n",
       "10              1.0  \n",
       "11              1.0  \n",
       "12              1.0  \n",
       "13              1.0  \n",
       "14              1.0  \n",
       "15              1.0  \n",
       "16              1.0  \n",
       "17              1.0  \n",
       "18              1.0  \n",
       "19              1.0  \n",
       "\n",
       "[20 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extended grid to: paper_trading_outputs/extended_grid_20251028_101758.csv\n"
     ]
    }
   ],
   "source": [
    "# Extended grid with cooldown and confidence sweep, optional gating, and gross/net metrics\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "COOLDOWN_GRID = [1, 3, 5, 10]\n",
    "CONF_MIN_GRID_EXT = [0.6, 0.7, 0.8]\n",
    "FEE_BPS_GRID = [3.0, 5.0, 10.0]\n",
    "SIGMA_TARGET_GRID = [0.10, 0.20, 0.30]\n",
    "\n",
    "GATE_VARIANTS = [\n",
    "    None,\n",
    "    {'name': 'cons2_adv0', 'consensus_min': 2, 'advantage_min': 0.0},\n",
    "    {'name': 'cons3_adv0', 'consensus_min': 3, 'advantage_min': 0.0},\n",
    "    {'name': 'cons2_adv0p02', 'consensus_min': 2, 'advantage_min': 0.02},\n",
    "]\n",
    "\n",
    "\n",
    "def run_extended_grid():\n",
    "    rows = []\n",
    "    stamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    for cooldown, conf_min, fee_bps, sig_t in itertools.product(COOLDOWN_GRID, CONF_MIN_GRID_EXT, FEE_BPS_GRID, SIGMA_TARGET_GRID):\n",
    "        for gate in GATE_VARIANTS:\n",
    "            cfg = {\n",
    "                'S_MIN': S_MIN,\n",
    "                'M_MIN': M_MIN,\n",
    "                'ALPHA_MIN': ALPHA_MIN,\n",
    "                'CONF_MIN': conf_min,\n",
    "                'COOLDOWN': cooldown,\n",
    "                'COST_BP': fee_bps,\n",
    "                'IMPACT_K': IMPACT_K,\n",
    "                'flip_map': DEFAULT_FLIP_MAP,\n",
    "            }\n",
    "            globals()['SIGMA_TARGET'] = sig_t\n",
    "            try:\n",
    "                res = run_backtest_net_and_gross(cfg, gate_cfg=gate if gate else None)\n",
    "                base = {\n",
    "                    'COOLDOWN': cooldown,\n",
    "                    'CONF_MIN': conf_min,\n",
    "                    'GATE': (gate['name'] if gate else 'none'),\n",
    "                    'FEE_BPS': fee_bps,\n",
    "                    'SIGMA_TARGET': sig_t,\n",
    "                }\n",
    "                row = {**base, **res}\n",
    "                rows.append(row)\n",
    "            except Exception as e:\n",
    "                rows.append({\n",
    "                    'COOLDOWN': cooldown,\n",
    "                    'CONF_MIN': conf_min,\n",
    "                    'GATE': (gate['name'] if gate else 'none'),\n",
    "                    'FEE_BPS': fee_bps,\n",
    "                    'SIGMA_TARGET': sig_t,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "\n",
    "    grid_df_ext = pd.DataFrame(rows)\n",
    "    sort_cols = [\n",
    "        'net_final_equity', 'net_sharpe', 'net_sortino', 'net_n_trades',\n",
    "        'gross_final_equity', 'gross_sharpe', 'gross_sortino'\n",
    "    ]\n",
    "    sort_cols = [c for c in sort_cols if c in grid_df_ext.columns]\n",
    "    if sort_cols:\n",
    "        grid_df_ext = grid_df_ext.sort_values(by=sort_cols, ascending=[False] + [False]*(len(sort_cols)-1))\n",
    "\n",
    "    out_dir = 'paper_trading_outputs'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = f\"{out_dir}/extended_grid_{stamp}.csv\"\n",
    "    grid_df_ext.to_csv(out_path, index=False)\n",
    "\n",
    "    try:\n",
    "        display(grid_df_ext.head(20))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        print(f\"Saved extended grid to: {out_path}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    _ = run_extended_grid()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579a1ced",
   "metadata": {},
   "source": [
    "## Apply best config to main run\n",
    "\n",
    "We set CONF_MIN=0.60 and COOLDOWN=1 (no extra gating), rebuild eligibility, and re-run the allocator with default flips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3a92a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best config: CONF_MIN=0.60, COOLDOWN=1, GATE=None\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "BEST_CFG = dict(\n",
    "    S_MIN=S_MIN,\n",
    "    M_MIN=M_MIN,\n",
    "    CONF_MIN=0.60,\n",
    "    ALPHA_MIN=ALPHA_MIN,\n",
    "    COOLDOWN=1,\n",
    "    COST_BP=COST_BP,\n",
    "    IMPACT_K=IMPACT_K,\n",
    "    flip_map=DEFAULT_FLIP_MAP,\n",
    " )\n",
    "\n",
    "if 'set_config_and_rebuild' in globals():\n",
    "    try:\n",
    "        set_config_and_rebuild(\n",
    "            S_MIN_val=BEST_CFG['S_MIN'],\n",
    "            M_MIN_val=BEST_CFG['M_MIN'],\n",
    "            CONF_MIN_val=BEST_CFG['CONF_MIN'],\n",
    "            ALPHA_MIN_val=BEST_CFG['ALPHA_MIN'],\n",
    "            IMPACT_K_val=BEST_CFG['IMPACT_K'],\n",
    "        )\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "# Run allocator net and gross without extra gating\n",
    "res = run_backtest_net_and_gross(BEST_CFG, gate_cfg=None)\n",
    "\n",
    "summary_cols = [\n",
    "    'net_final_equity','net_n_trades','net_sharpe','net_sortino','net_maxDD','net_turnover','net_hit_rate',\n",
    "    'gross_final_equity','gross_n_trades','gross_sharpe','gross_sortino','gross_maxDD','gross_turnover','gross_hit_rate',\n",
    "    'cost_drag','net_per_trade','gross_per_trade'\n",
    " ]\n",
    "\n",
    "out_dir = 'paper_trading_outputs'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "stamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "summary_path = f\"{out_dir}/best_config_summary_{stamp}.csv\"\n",
    "\n",
    "# Prepare signals and eligibility as used in execution\n",
    "base_signals_df = arm_signals_df.copy()\n",
    "signals_for_bt = _flip_signals(base_signals_df, BEST_CFG.get('flip_map', DEFAULT_FLIP_MAP))\n",
    "eligible_df = arm_eligible_df.copy()\n",
    "\n",
    "# Build per-arm thresholds dynamically\n",
    "side_eps_vec = _make_side_eps_vec(list(signals_for_bt.columns), BEST_CFG)\n",
    "\n",
    "Eq_df, Tr_df, Bu_df, met = run_allocator_backtest(\n",
    "    bt_df=bt,\n",
    "    arm_signals=signals_for_bt.values,\n",
    "    arm_eligible=eligible_df.values,\n",
    "    adv_series=adv20_by_ts,\n",
    "    cooldown_bars=int(BEST_CFG.get('COOLDOWN', COOLDOWN)),\n",
    "    cost_bp=float(BEST_CFG.get('COST_BP', COST_BP)),\n",
    "    impact_k=float(BEST_CFG.get('IMPACT_K', IMPACT_K)),\n",
    "    side_eps_vec=side_eps_vec,\n",
    "    eps=1e-12,\n",
    " )\n",
    "\n",
    "# Write summary with assumptions\n",
    "assumptions_str = ';'.join([f\"{k}={v}\" for k, v in (met.get('assumptions', {}) or {}).items()])\n",
    "summary_row = {**BEST_CFG, **{k: res.get(k) for k in summary_cols}, 'assumptions': assumptions_str}\n",
    "pd.DataFrame([summary_row]).to_csv(summary_path, index=False)\n",
    "\n",
    "# Enrich trade log with $ fields using robust asof merges\n",
    "NOTIONAL = 100_000.0\n",
    "Tr_enriched = Tr_df.copy()\n",
    "if Tr_enriched is not None and not Tr_enriched.empty:\n",
    "    Tr_enriched['ts'] = pd.to_datetime(Tr_enriched['ts'], errors='coerce')\n",
    "    Tr_enriched = Tr_enriched.sort_values('ts').reset_index(drop=True)\n",
    "\n",
    "    eq_series = Eq_df['equity'].copy()\n",
    "    eq_series.index = pd.to_datetime(eq_series.index, errors='coerce')\n",
    "    eq_series = eq_series.sort_index()\n",
    "\n",
    "    eq_df_reset = eq_series.reset_index()\n",
    "    eq_df_reset.columns = ['ts','equity']\n",
    "    eq_prev_reset = eq_df_reset[['ts']].copy()\n",
    "    eq_prev_reset['equity_prev'] = eq_df_reset['equity'].shift(1).values\n",
    "\n",
    "    Tr_enriched = pd.merge_asof(Tr_enriched, eq_df_reset, on='ts', direction='backward')\n",
    "    if 'equity' not in Tr_enriched.columns:\n",
    "        Tr_enriched['equity'] = eq_series.reindex(Tr_enriched['ts']).fillna(method='ffill').fillna(1.0).values\n",
    "\n",
    "    Tr_enriched = pd.merge_asof(Tr_enriched, eq_prev_reset, on='ts', direction='backward')\n",
    "    if 'equity_prev' not in Tr_enriched.columns:\n",
    "        Tr_enriched['equity_prev'] = eq_series.reindex(Tr_enriched['ts']).shift(1).fillna(1.0).values\n",
    "\n",
    "    Tr_enriched['equity_prev'] = Tr_enriched['equity_prev'].fillna(1.0)\n",
    "    Tr_enriched['cost_$'] = (Tr_enriched['equity_prev'] * NOTIONAL) * (Tr_enriched['cost_bps'] / 10000.0)\n",
    "\n",
    "    next_ts = Tr_enriched['ts'].shift(-1)\n",
    "    if len(eq_df_reset) > 0:\n",
    "        next_ts.iloc[-1] = eq_df_reset['ts'].iloc[-1]\n",
    "    next_df = pd.DataFrame({'ts': next_ts}).sort_values('ts').reset_index(drop=True)\n",
    "    next_df = pd.merge_asof(next_df, eq_df_reset, on='ts', direction='backward')\n",
    "    Tr_enriched['equity_next'] = next_df['equity'].fillna(eq_series.iloc[-1] if len(eq_series) else 1.0).values\n",
    "\n",
    "    Tr_enriched['pnl_$'] = (Tr_enriched['equity_next'] - Tr_enriched['equity']) * NOTIONAL - Tr_enriched['cost_$']\n",
    "    Tr_enriched['cum_pnl_$'] = Tr_enriched['pnl_$'].cumsum()\n",
    "\n",
    "    Tr_export = Tr_enriched.rename(columns={'equity': 'equity_at_trade'})[\n",
    "        ['ts','from_pos','to_pos','cost_bps','cost_$','pnl_$','cum_pnl_$','equity_at_trade','equity_prev','equity_next']\n",
    "    ]\n",
    "else:\n",
    "    Tr_export = Tr_df\n",
    "\n",
    "Eq_path = f\"{out_dir}/best_equity_{stamp}.csv\"\n",
    "Tr_path = f\"{out_dir}/best_trade_log_{stamp}.csv\"\n",
    "Bu_path = f\"{out_dir}/best_bandit_updates_{stamp}.csv\"\n",
    "Eq_df.to_csv(Eq_path)\n",
    "Tr_export.to_csv(Tr_path, index=False)\n",
    "Bu_df.to_csv(Bu_path, index=False)\n",
    "\n",
    "scaled = Eq_df.copy()\n",
    "scaled['equity_$'] = scaled['equity'] * NOTIONAL\n",
    "scaled['pnl_$'] = (scaled['equity'] - 1.0) * NOTIONAL\n",
    "\n",
    "if Tr_export is not None and not Tr_export.empty:\n",
    "    tr_aug = Tr_export.copy()\n",
    "    tr_aug['equity_prev_$'] = tr_aug['equity_prev'] * NOTIONAL\n",
    "else:\n",
    "    tr_aug = None\n",
    "\n",
    "scaled_path = f\"{out_dir}/best_equity_scaled_{int(NOTIONAL)}_{stamp}.csv\"\n",
    "scaled.to_csv(scaled_path)\n",
    "if tr_aug is not None:\n",
    "    tr_scaled_path = f\"{out_dir}/best_trade_log_scaled_{int(NOTIONAL)}_{stamp}.csv\"\n",
    "    tr_aug.to_csv(tr_scaled_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59be2e0",
   "metadata": {},
   "source": [
    "# BMA vs MetaStacker: split-arm comparison and outputs\n",
    "\n",
    "This section computes and compares calibrated probabilities and simple metrics for two model arms:\n",
    "\n",
    "- MetaStacker (calibrated meta-classifier you already trained)\n",
    "- BMA (Bayesian model averaging over base learners, using validation-derived weights)\n",
    "\n",
    "What you'll get after running Cells 2–5 below:\n",
    "- CSV: per-row comparison of true label, p_meta, p_bma, and hard predictions\n",
    "- JSON: summary metrics (accuracy, log loss, Brier) for both arms\n",
    "- JSON: BMA weights used for the combination\n",
    "\n",
    "Assumptions:\n",
    "- Earlier cells have already prepared your eval set (X_backtest/y_backtest or X_test/y_test)\n",
    "- The notebook already saved model artifacts to `paper_trading_outputs/models/LATEST.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271afa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Feature alignment skipped: Missing features in eval: ['feature_cols', 'schema_hash']\n",
      "Eval ready: X_eval=(2073, 17) y_eval=(2073,) precomputed_ok=False\n",
      "Artifacts loaded: meta=meta_classifier_20251028_101254_d7a9e9fb3a42.joblib calibrator=calibrator_20251028_101254_d7a9e9fb3a42.joblib features=feature_columns_20251028_101254_d7a9e9fb3a42.json\n"
     ]
    }
   ],
   "source": [
    "# Load artifacts and evaluation data\n",
    "import os, json, joblib, numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Resolve models directory and LATEST pointers\n",
    "root_dir = os.getcwd()\n",
    "models_dir = os.path.join(root_dir, \"paper_trading_outputs\", \"models\")\n",
    "latest_fp = os.path.join(models_dir, \"LATEST.json\")\n",
    "assert os.path.exists(latest_fp), f\"LATEST.json not found at {latest_fp}. Run the training cells above first.\"\n",
    "\n",
    "with open(latest_fp, \"r\") as f:\n",
    "    latest = json.load(f)\n",
    "\n",
    "# Load meta classifier and calibrator (already trained/saved earlier)\n",
    "meta_fp = os.path.join(models_dir, latest.get(\"meta_classifier\", \"\"))\n",
    "calib_fp = os.path.join(models_dir, latest.get(\"calibrator\", \"\"))\n",
    "feat_fp = os.path.join(models_dir, latest.get(\"feature_columns\", \"\"))\n",
    "train_meta_fp = os.path.join(models_dir, latest.get(\"training_meta\", \"\"))\n",
    "\n",
    "meta = joblib.load(meta_fp)\n",
    "calibrator = joblib.load(calib_fp) if os.path.exists(calib_fp) else None\n",
    "\n",
    "# Load feature columns (JSON)\n",
    "feature_columns = None\n",
    "if os.path.exists(feat_fp):\n",
    "    try:\n",
    "        if feat_fp.lower().endswith('.json'):\n",
    "            with open(feat_fp, 'r') as f:\n",
    "                feature_columns = json.load(f)\n",
    "        else:\n",
    "            feature_columns = joblib.load(feat_fp)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to load feature columns from {os.path.basename(feat_fp)}: {e}\")\n",
    "\n",
    "# Load training meta (JSON)\n",
    "train_meta = None\n",
    "if os.path.exists(train_meta_fp):\n",
    "    try:\n",
    "        with open(train_meta_fp, \"r\") as f:\n",
    "            train_meta = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to load training_meta JSON: {e}\")\n",
    "\n",
    "# Resolve evaluation split prepared earlier\n",
    "X_eval = None\n",
    "y_eval = None\n",
    "for X_name, y_name in [\n",
    "    (\"X_bt\",\"y_test\"),\n",
    "    (\"X_backtest\",\"y_backtest\"),\n",
    "    (\"X_test_model_clean\",\"y_test_model_clean\"),\n",
    "    (\"X_test\",\"y_test\"),\n",
    "    (\"X_val\",\"y_val\"),\n",
    "]:\n",
    "    if X_name in globals() and y_name in globals():\n",
    "        X_eval = globals()[X_name]\n",
    "        y_eval = globals()[y_name]\n",
    "        break\n",
    "\n",
    "# Fallback: if no X_eval but we already have precomputed probabilities and y, allow later cells to use them\n",
    "precomputed_ok = False\n",
    "if X_eval is None or y_eval is None:\n",
    "    if 'P_bt' in globals() and 'y_test' in globals():\n",
    "        y_eval = globals()['y_test']\n",
    "        precomputed_ok = True\n",
    "\n",
    "if not precomputed_ok:\n",
    "    assert X_eval is not None and y_eval is not None, (\n",
    "        \"Could not find evaluation data (tried X_bt/y_test, X_backtest/y_backtest, \"\n",
    "        \"X_test_model_clean/y_test_model_clean, X_test/y_test, X_val/y_val). \"\n",
    "        \"Please run earlier cells.\"\n",
    "    )\n",
    "\n",
    "# Ensure column order matches training\n",
    "try:\n",
    "    import pandas as pd\n",
    "    if feature_columns is not None and 'pd' in globals() and X_eval is not None and isinstance(X_eval, pd.DataFrame):\n",
    "        missing = [c for c in feature_columns if c not in X_eval.columns]\n",
    "        assert not missing, f\"Missing features in eval: {missing}\"\n",
    "        X_eval = X_eval[feature_columns]\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Feature alignment skipped: {e}\")\n",
    "\n",
    "print(f\"Eval ready: X_eval={getattr(X_eval, 'shape', None)} y_eval={getattr(y_eval, 'shape', None)} precomputed_ok={precomputed_ok}\")\n",
    "print(f\"Artifacts loaded: meta={os.path.basename(meta_fp)} calibrator={os.path.basename(calib_fp) if calib_fp else 'None'} features={os.path.basename(feat_fp) if feat_fp else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "131f0a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] bma_classifier available but failed to predict: float() argument must be a string or a real number, not 'dict'. Will try fallback methods.\n",
      "Base/BMA selection:\n",
      " - base_names: [not applicable]\n",
      " - weights: None\n",
      " - reason: equal\n",
      " - classes_ set: False\n"
     ]
    }
   ],
   "source": [
    "# Extract base/BMA probabilities and compute BMA weights (prefer existing bma_classifier if available)\n",
    "import numpy as np\n",
    "\n",
    "base_probs = None\n",
    "base_names = []\n",
    "weights = None\n",
    "reason = \"equal\"\n",
    "classes_ = None\n",
    "\n",
    "p_bma_full = None  # may be set here if bma_classifier or precomputed arrays exist\n",
    "\n",
    "# Use bma_classifier if present and X_eval available\n",
    "if 'bma_classifier' in globals() and X_eval is not None:\n",
    "    try:\n",
    "        p_bma_full = bma_classifier.predict_proba(X_eval)\n",
    "        classes_ = getattr(bma_classifier, 'classes_', None)\n",
    "        # Optional: if the classifier exposes internal base names/weights\n",
    "        if hasattr(bma_classifier, 'base_names_'):\n",
    "            base_names = list(getattr(bma_classifier, 'base_names_'))\n",
    "        elif hasattr(bma_classifier, 'base_learners_'):\n",
    "            base_names = [type(m).__name__ for m in getattr(bma_classifier, 'base_learners_')]\n",
    "        if hasattr(bma_classifier, 'weights_'):\n",
    "            w = np.asarray(getattr(bma_classifier, 'weights_'), dtype=float)\n",
    "            weights = w / w.sum() if w.sum() != 0 else None\n",
    "            reason = 'model_internal'\n",
    "        print(\"[INFO] Used existing bma_classifier to compute p_bma_full.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] bma_classifier available but failed to predict: {e}. Will try fallback methods.\")\n",
    "\n",
    "# If precomputed probabilities exist in the notebook, use them\n",
    "if p_bma_full is None and 'P_bma_bt' in globals():\n",
    "    p_bma_full = globals()['P_bma_bt']\n",
    "    # classes: try meta or bma calibrator\n",
    "    if classes_ is None:\n",
    "        if 'bma_class_calibrator' in globals() and hasattr(bma_class_calibrator, 'classes_'):\n",
    "            classes_ = getattr(bma_class_calibrator, 'classes_')\n",
    "        elif hasattr(meta, 'classes_'):\n",
    "            classes_ = getattr(meta, 'classes_')\n",
    "    base_names = ['bma_combined']\n",
    "    weights = np.array([1.0])\n",
    "    reason = 'precomputed'\n",
    "    print(\"[INFO] Using precomputed P_bma_bt from earlier cells.\")\n",
    "\n",
    "# If neither bma_classifier nor precomputed arrays, fallback to combining base estimators from meta\n",
    "if p_bma_full is None:\n",
    "    # Try to discover base estimators stored inside the meta model\n",
    "    base_estimators = None\n",
    "    for attr in [\"base_learners\", \"base_models\", \"base_estimators\", \"estimators_\"]:\n",
    "        if hasattr(meta, attr):\n",
    "            base_estimators = getattr(meta, attr)\n",
    "            break\n",
    "    if base_estimators is None and hasattr(meta, \"estimators_\") and isinstance(meta.estimators_, list):\n",
    "        base_estimators = [est for _, est in meta.estimators_]\n",
    "\n",
    "    assert base_estimators is not None and len(base_estimators) > 0, \"Could not locate base estimators on the meta model. Expose them or save separately.\"\n",
    "\n",
    "    # Compute per-base probabilities on the eval set\n",
    "    base_prob_list = []\n",
    "    base_names = []\n",
    "    for i, est in enumerate(base_estimators):\n",
    "        name = getattr(est, \"__class__\", type(est)).__name__\n",
    "        base_names.append(name)\n",
    "        if hasattr(est, \"predict_proba\"):\n",
    "            probs = est.predict_proba(X_eval)\n",
    "        else:\n",
    "            if hasattr(est, \"decision_function\"):\n",
    "                df = est.decision_function(X_eval)\n",
    "                if df.ndim == 1:\n",
    "                    p1 = 1.0/(1.0+np.exp(-df))\n",
    "                    probs = np.vstack([1-p1, p1]).T\n",
    "                else:\n",
    "                    ex = np.exp(df - np.max(df, axis=1, keepdims=True))\n",
    "                    probs = ex/np.sum(ex, axis=1, keepdims=True)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Base estimator {name} has neither predict_proba nor decision_function\")\n",
    "        base_prob_list.append(probs)\n",
    "\n",
    "    # Determine class ordering from meta or any base\n",
    "    for obj in [meta] + base_estimators:\n",
    "        if hasattr(obj, \"classes_\"):\n",
    "            classes_ = getattr(obj, \"classes_\")\n",
    "            break\n",
    "\n",
    "    # Compute BMA weights from training_meta if possible\n",
    "    if train_meta is not None:\n",
    "        try:\n",
    "            logloss_by_name = {}\n",
    "            cv_info = train_meta.get(\"cv\", {})\n",
    "            for nm in base_names:\n",
    "                candidates = [nm, nm.lower(), nm.replace(\"Classifier\",\"\"), nm.replace(\" \", \"\").lower()]\n",
    "                found = None\n",
    "                for key in cv_info.keys():\n",
    "                    if key in (\"base_models\", \"base_learners\", \"models\") and isinstance(cv_info[key], dict):\n",
    "                        for k2, v2 in cv_info[key].items():\n",
    "                            if k2 == nm or k2 in candidates or k2.lower() in candidates:\n",
    "                                if isinstance(v2, dict) and \"logloss\" in v2:\n",
    "                                    found = v2[\"logloss\"]\n",
    "                                    break\n",
    "                    elif key == nm and isinstance(cv_info[key], dict) and \"logloss\" in cv_info[key]:\n",
    "                        found = cv_info[key][\"logloss\"]\n",
    "                if found is not None:\n",
    "                    logloss_by_name[nm] = float(found)\n",
    "            if len(logloss_by_name) >= 1:\n",
    "                ws = []\n",
    "                for nm in base_names:\n",
    "                    ll = logloss_by_name.get(nm, None)\n",
    "                    ws.append(np.exp(-ll) if ll is not None else 1.0)\n",
    "                weights = np.asarray(ws, dtype=float)\n",
    "                weights = weights/weights.sum()\n",
    "                reason = \"validation_logloss\"\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to use training_meta for weights: {e}\")\n",
    "\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(base_prob_list), dtype=float)/len(base_prob_list)\n",
    "\n",
    "    base_probs = np.stack(base_prob_list, axis=0)\n",
    "\n",
    "print(\"Base/BMA selection:\")\n",
    "print(\" - base_names:\", base_names if base_names else '[not applicable]')\n",
    "print(\" - weights:\", ( {nm: float(w) for nm, w in zip(base_names, weights)} if weights is not None and len(base_names)==len(weights) else (weights if weights is not None else None) ))\n",
    "print(\" - reason:\", reason)\n",
    "print(\" - classes_ set:\", classes_ is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36ae81e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed p_meta and p_bma.\n",
      "Shapes: (2073, 3) (2073, 3)\n"
     ]
    }
   ],
   "source": [
    "# Compute p_meta and p_bma on the eval set\n",
    "import numpy as np\n",
    "\n",
    "# MetaStacker calibrated probability: prefer fresh compute on X_eval; fallback to precomputed if no X_eval\n",
    "if 'X_eval' in globals() and X_eval is not None:\n",
    "    try:\n",
    "        p_meta_full = meta.predict_proba(X_eval)\n",
    "        classes_meta = getattr(meta, 'classes_', None)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Meta predict_proba failed: {e}\")\n",
    "elif 'P_bt' in globals() and isinstance(P_bt, np.ndarray):\n",
    "    p_meta_full = P_bt\n",
    "    if 'meta_classifier' in globals() and hasattr(meta_classifier, 'classes_'):\n",
    "        classes_meta = getattr(meta_classifier, 'classes_')\n",
    "    else:\n",
    "        classes_meta = getattr(meta, 'classes_', None)\n",
    "else:\n",
    "    raise RuntimeError(\"No path to get Meta probabilities: need X_eval with meta model or precomputed P_bt.\")\n",
    "\n",
    "# BMA combined probability\n",
    "if 'p_bma_full' in globals() and p_bma_full is not None:\n",
    "    pass  # already computed in previous cell via bma_classifier or precomputed\n",
    "elif 'P_bma_bt' in globals() and isinstance(P_bma_bt, np.ndarray):\n",
    "    p_bma_full = P_bma_bt\n",
    "else:\n",
    "    if 'base_probs' in globals() and base_probs is not None and 'weights' in globals() and weights is not None:\n",
    "        # base_probs shape: (n_models, n_samples, n_classes)\n",
    "        p_bma_full = np.tensordot(weights, base_probs, axes=(0,0))  # -> (n_samples, n_classes)\n",
    "    else:\n",
    "        raise RuntimeError(\"No path to compute BMA probabilities: need bma_classifier, precomputed P_bma_bt, or base_probs+weights.\")\n",
    "\n",
    "# Determine canonical classes for downstream handling\n",
    "classes_ = classes_meta if 'classes_' not in globals() or classes_ is None else classes_\n",
    "if classes_ is None:\n",
    "    if 'bma_class_calibrator' in globals() and hasattr(bma_class_calibrator, 'classes_'):\n",
    "        classes_ = getattr(bma_class_calibrator, 'classes_')\n",
    "    elif 'y_eval' in globals() and y_eval is not None:\n",
    "        vals = np.unique(np.asarray(y_eval))\n",
    "        assert len(vals) >= 2, \"Need at least 2 classes\"\n",
    "        classes_ = vals\n",
    "\n",
    "# For binary tasks, distill to positive-class probability (class == 1 if present)\n",
    "classes_arr = np.array(classes_)\n",
    "if p_meta_full.shape[1] == 2 and p_bma_full.shape[1] == 2:\n",
    "    if 1 in classes_arr:\n",
    "        pos_idx = int(np.where(classes_arr == 1)[0][0])\n",
    "    else:\n",
    "        pos_idx = int(np.argmax(classes_arr))\n",
    "    p_meta = p_meta_full[:, pos_idx]\n",
    "    p_bma = p_bma_full[:, pos_idx]\n",
    "else:\n",
    "    p_meta = p_meta_full\n",
    "    p_bma = p_bma_full\n",
    "\n",
    "print(\"Computed p_meta and p_bma.\")\n",
    "print(\"Shapes:\", getattr(p_meta, 'shape', None), getattr(p_bma, 'shape', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca36cb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - c:\\Users\\vyshn\\OneDrive\\Desktop\\MetaStacker\\BotV2-LSTM\\paper_trading_outputs\\bma_meta_comparison_20251028_102144.csv\n",
      " - c:\\Users\\vyshn\\OneDrive\\Desktop\\MetaStacker\\BotV2-LSTM\\paper_trading_outputs\\bma_meta_comparison_summary_20251028_102144.json\n",
      " - c:\\Users\\vyshn\\OneDrive\\Desktop\\MetaStacker\\BotV2-LSTM\\paper_trading_outputs\\bma_weights_20251028_102144.json\n",
      "Metrics: {\n",
      "  \"meta\": {\n",
      "    \"accuracy\": 0.3333333333333333,\n",
      "    \"log_loss\": 1.189169603978172\n",
      "  },\n",
      "  \"bma\": {\n",
      "    \"accuracy\": 0.35407621804148576,\n",
      "    \"log_loss\": 1.1180699129878833\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Metrics and exports: CSV + JSON\n",
    "import os, json\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, log_loss, brier_score_loss\n",
    "import numpy as np\n",
    "\n",
    "# Prepare y_true in numeric form\n",
    "y_true = np.asarray(y_eval)\n",
    "\n",
    "# Determine binary vs multiclass flows\n",
    "is_multiclass = (p_meta.ndim == 2 and p_meta.shape[1] > 2) or (p_bma.ndim == 2 and p_bma.shape[1] > 2)\n",
    "\n",
    "# Helper metrics\n",
    "metrics = {}\n",
    "if not is_multiclass:\n",
    "    # Binary: probabilities are 1D (positive class)\n",
    "    y_pred_meta = (p_meta >= 0.5).astype(int)\n",
    "    y_pred_bma = (p_bma >= 0.5).astype(int)\n",
    "\n",
    "    # For log_loss, need probs for both classes\n",
    "    meta_prob_2c = np.vstack([1 - p_meta, p_meta]).T\n",
    "    bma_prob_2c = np.vstack([1 - p_bma, p_bma]).T\n",
    "\n",
    "    metrics[\"meta\"] = {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred_meta)),\n",
    "        \"log_loss\": float(log_loss(y_true, meta_prob_2c, labels=[0,1])),\n",
    "        \"brier\": float(brier_score_loss(y_true, p_meta)),\n",
    "    }\n",
    "    metrics[\"bma\"] = {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred_bma)),\n",
    "        \"log_loss\": float(log_loss(y_true, bma_prob_2c, labels=[0,1])),\n",
    "        \"brier\": float(brier_score_loss(y_true, p_bma)),\n",
    "    }\n",
    "else:\n",
    "    # Multiclass\n",
    "    y_pred_meta = np.argmax(p_meta, axis=1)\n",
    "    y_pred_bma = np.argmax(p_bma, axis=1)\n",
    "\n",
    "    metrics[\"meta\"] = {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred_meta)),\n",
    "        \"log_loss\": float(log_loss(y_true, p_meta)),\n",
    "    }\n",
    "    metrics[\"bma\"] = {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred_bma)),\n",
    "        \"log_loss\": float(log_loss(y_true, p_bma)),\n",
    "    }\n",
    "\n",
    "# Export per-row comparison\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_dir = os.path.join(root_dir, \"paper_trading_outputs\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "comp_csv = os.path.join(out_dir, f\"bma_meta_comparison_{ts}.csv\")\n",
    "summary_json = os.path.join(out_dir, f\"bma_meta_comparison_summary_{ts}.json\")\n",
    "weights_json = os.path.join(out_dir, f\"bma_weights_{ts}.json\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\n",
    "        \"y_true\": y_true,\n",
    "        \"p_meta\": p_meta if p_meta.ndim == 1 else p_meta.tolist(),\n",
    "        \"p_bma\": p_bma if p_bma.ndim == 1 else p_bma.tolist(),\n",
    "        \"y_pred_meta\": y_pred_meta,\n",
    "        \"y_pred_bma\": y_pred_bma,\n",
    "    })\n",
    "    # Flatten arrays into JSON-friendly strings for multiclass\n",
    "    if is_multiclass:\n",
    "        df[\"p_meta\"] = df[\"p_meta\"].apply(lambda x: json.dumps(x))\n",
    "        df[\"p_bma\"] = df[\"p_bma\"].apply(lambda x: json.dumps(x))\n",
    "    df.to_csv(comp_csv, index=False)\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Failed to write CSV via pandas ({e}), writing minimal CSV manually.\")\n",
    "    with open(comp_csv, \"w\") as f:\n",
    "        header = [\"y_true\",\"p_meta\",\"p_bma\",\"y_pred_meta\",\"y_pred_bma\"]\n",
    "        f.write(\",\".join(header) + \"\\n\")\n",
    "        for i in range(len(y_true)):\n",
    "            pm = p_meta[i] if p_meta.ndim == 1 else json.dumps(p_meta[i].tolist())\n",
    "            pb = p_bma[i] if p_bma.ndim == 1 else json.dumps(p_bma[i].tolist())\n",
    "            f.write(f\"{y_true[i]},{pm},{pb},{y_pred_meta[i]},{y_pred_bma[i]}\\n\")\n",
    "\n",
    "# Export summary metrics and weights\n",
    "with open(summary_json, \"w\") as f:\n",
    "    json.dump({\"metrics\": metrics, \"is_multiclass\": is_multiclass}, f, indent=2)\n",
    "\n",
    "# Weights may be unavailable when using precomputed P_bma_bt\n",
    "weights_payload = {}\n",
    "if 'weights' in globals() and weights is not None and 'base_names' in globals() and base_names and len(base_names) == len(weights):\n",
    "    weights_payload = {nm: float(w) for nm, w in zip(base_names, weights)}\n",
    "else:\n",
    "    source = 'precomputed' if ('P_bma_bt' in globals()) else 'unknown'\n",
    "    weights_payload = {\"source\": source, \"weights\": None}\n",
    "\n",
    "with open(weights_json, \"w\") as f:\n",
    "    json.dump(weights_payload, f, indent=2)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", comp_csv)\n",
    "print(\" -\", summary_json)\n",
    "print(\" -\", weights_json)\n",
    "print(\"Metrics:\", json.dumps(metrics, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BotV2-LSTM",
   "language": "python",
   "name": "botv2-lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
